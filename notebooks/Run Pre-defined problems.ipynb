{"cells":[{"metadata":{},"cell_type":"markdown","source":["# Run Pre-defined Problems\n","\n","This notebook is run on Google Colab. You need to manually upload the data to kaggle and pass the correct path to the pre-processing function."]},{"metadata":{"id":"pD36RrxixyL0","executionInfo":{"status":"ok","timestamp":1598583472250,"user_tz":-480,"elapsed":13696,"user":{"displayName":"Jay Yip","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8HFC9Hw_HDfnVt7H8d-HtDo7Lh8B5itD8Z8FmoiM=s64","userId":"13305867008370392063"}},"outputId":"f266dd9b-07cb-469f-cdae-deb8d7b54062","trusted":true},"cell_type":"code","source":["!pip install tensorflow-gpu\n","!pip install bert-multitask-learning\n","!rm -r transformers\n","!git clone https://github.com/huggingface/transformers.git && cd transformers && pip install ."],"execution_count":null,"outputs":[]},{"metadata":{"id":"qNDPcXgwxoRR","executionInfo":{"status":"ok","timestamp":1598583476483,"user_tz":-480,"elapsed":17919,"user":{"displayName":"Jay Yip","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8HFC9Hw_HDfnVt7H8d-HtDo7Lh8B5itD8Z8FmoiM=s64","userId":"13305867008370392063"}},"outputId":"10bfee6c-ede8-4b92-905c-89a1804b39c4","trusted":true},"cell_type":"code","source":["import tensorflow as tf\n","import transformers\n","from bert_multitask_learning import train_bert_multitask, train_eval_input_fn, BertMultiTask, DynamicBatchSizeParams\n","from bert_multitask_learning.predefined_problems import get_weibo_ner_fn, get_weibo_cws_fn"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["tf.config.list_physical_devices('GPU')"],"execution_count":null,"outputs":[]},{"metadata":{"id":"Dajzr_nYxoRV","executionInfo":{"status":"ok","timestamp":1598583476483,"user_tz":-480,"elapsed":17911,"user":{"displayName":"Jay Yip","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8HFC9Hw_HDfnVt7H8d-HtDo7Lh8B5itD8Z8FmoiM=s64","userId":"13305867008370392063"}},"trusted":true},"cell_type":"code","source":["problem_type_dict = {\n","    'weibo_cws': 'seq_tag',\n","    'weibo_ner': 'seq_tag'\n","}\n","\n","# data \n","processing_fn_dict = {\n","    'weibo_ner': get_weibo_ner_fn(file_path='../input/weibo-ner/weiboNER*'),\n","    'weibo_cws': get_weibo_cws_fn(file_path='../input/weibo-ner/weiboNER*')\n","}"],"execution_count":null,"outputs":[]},{"metadata":{"id":"kA5GLmYxxoRX"},"cell_type":"markdown","source":["## Train Models\n","If you don't want to control every thing, you can just call `train_bert_multitask` function. Please note that starting from 0.4.2, transformer models (the body model) are implemented using [huggingface transformers](https://github.com/huggingface/transformers) and because of that, now we can basically use all transformer models by specifying following params(below is the default value):\n","\n","    params.transformer_model_name = 'bert-base-chinese'\n","    params.transformer_tokenizer_name = 'bert-base-chinese'\n","    params.transformer_config_name = 'bert-base-chinese'\n","    params.transformer_model_loading = 'TFAutoModel'\n","    params.transformer_config_loading = 'BertConfig'\n","    params.transformer_tokenizer_loading = 'AutoTokenizer'\n","\n","  And for decoder:\n","\n","    params.transformer_decoder_model_name = None\n","    params.transformer_decoder_config_name = None\n","    params.transformer_decoder_tokenizer_name = None\n","    params.transformer_decoder_model_loading = 'TFAutoModel'\n","    params.transformer_decoder_config_loading = 'BertConfig'\n","    params.transformer_decoder_tokenizer_loading = 'AutoTokenizer'\n","\n","However, there are still some drawbacks when initializing Keras model within estimator. To overcome this, we need to manually download the weights and pass the model path to `params.transformer_model_name`."]},{"metadata":{"id":"2Dweteny2uk2","executionInfo":{"status":"ok","timestamp":1598583497431,"user_tz":-480,"elapsed":38850,"user":{"displayName":"Jay Yip","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8HFC9Hw_HDfnVt7H8d-HtDo7Lh8B5itD8Z8FmoiM=s64","userId":"13305867008370392063"}},"outputId":"e885305e-a1bf-437d-c993-ed642f832468","trusted":true},"cell_type":"code","source":["transformers.TFAutoModel.from_pretrained('bert-base-chinese').save_weights('./models/bert-base-chinese/pretrained_model')"],"execution_count":null,"outputs":[]},{"metadata":{"id":"KiYKa3NhxoRY","executionInfo":{"status":"ok","timestamp":1598584410854,"user_tz":-480,"elapsed":952263,"user":{"displayName":"Jay Yip","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8HFC9Hw_HDfnVt7H8d-HtDo7Lh8B5itD8Z8FmoiM=s64","userId":"13305867008370392063"}},"outputId":"eebc231f-cc51-4e74-dac0-f9163cc4622f","trusted":true},"cell_type":"code","source":["# here we use the default model which is bert-base-chinese\n","params = DynamicBatchSizeParams()\n","# AutoConfig cannot load from dict...\n","params.transformer_config_loading = 'BertConfig'\n","params.transformer_model_name = './models/bert-base-chinese/pretrained_model'\n","train_bert_multitask(problem='weibo_ner&weibo_cws', params=params, problem_type_dict=problem_type_dict, processing_fn_dict=processing_fn_dict, num_gpus=1, num_epochs=50)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"HD9AxZrpxoRa"},"cell_type":"markdown","source":["If you want to take more control of the training process, you can use lower level api"]},{"metadata":{"id":"MckZxWKjxoRb","executionInfo":{"status":"ok","timestamp":1598584411744,"user_tz":-480,"elapsed":953143,"user":{"displayName":"Jay Yip","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8HFC9Hw_HDfnVt7H8d-HtDo7Lh8B5itD8Z8FmoiM=s64","userId":"13305867008370392063"}},"outputId":"e563a0f0-66d5-4e03-a900-e4f50cef936f","trusted":true},"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.estimator import Estimator\n","\n","problem = 'weibo_ner&weibo_cws'\n","num_gpus = 1\n","bert_multitask_params = DynamicBatchSizeParams()\n","bert_multitask_params.transformer_model_name = './models/bert-base-chinese/pretrained_model'\n","\n","for new_problem, new_problem_processing_fn in processing_fn_dict.items():\n","    print('Adding new problem {0}, problem type: {1}'.format(\n","        new_problem, problem_type_dict[new_problem]))\n","    bert_multitask_params.add_problem(\n","        problem_name=new_problem, problem_type=problem_type_dict[new_problem], processing_fn=new_problem_processing_fn)\n","\n","# assign problem to params\n","bert_multitask_params.train_epoch = 1\n","bert_multitask_params.assign_problem(problem, gpu=1)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"qpiKdg1fxoRd","executionInfo":{"status":"ok","timestamp":1598584411744,"user_tz":-480,"elapsed":953136,"user":{"displayName":"Jay Yip","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8HFC9Hw_HDfnVt7H8d-HtDo7Lh8B5itD8Z8FmoiM=s64","userId":"13305867008370392063"}},"outputId":"855ddf88-5154-4785-919e-802446382556","trusted":true},"cell_type":"code","source":["# get model fn and create mirror strategy for distributed training\n","model = BertMultiTask(params=bert_multitask_params)\n","model_fn = model.get_model_fn()\n","\n","dist_trategy = tf.distribute.MirroredStrategy()\n","\n","run_config = tf.estimator.RunConfig(\n","    train_distribute=dist_trategy,\n","    eval_distribute=dist_trategy,\n","    log_step_count_steps=params.log_every_n_steps)\n","\n","estimator = Estimator(\n","    model_fn,\n","    model_dir=params.ckpt_dir,\n","    params=params,\n","    config=run_config)\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"ZNlUQk2-xoRg","executionInfo":{"status":"ok","timestamp":1598584411745,"user_tz":-480,"elapsed":953129,"user":{"displayName":"Jay Yip","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8HFC9Hw_HDfnVt7H8d-HtDo7Lh8B5itD8Z8FmoiM=s64","userId":"13305867008370392063"}},"outputId":"d6c5cddb-2bd3-4892-df13-32b445647cbe","trusted":true},"cell_type":"code","source":["# train\n","def train_input_fn(): return train_eval_input_fn(bert_multitask_params)\n","estimator.train(\n","    train_input_fn, max_steps=200)\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"uUQ_418MxoRi"},"cell_type":"markdown","source":["## Evaluate and Predict\n","\n","~~For NER and CWS, we need different evaluation logic.~~ Evaluation has bug and not fixed now."]},{"metadata":{"id":"s8ScABR3xoRi","executionInfo":{"status":"ok","timestamp":1598584411746,"user_tz":-480,"elapsed":953123,"user":{"displayName":"Jay Yip","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8HFC9Hw_HDfnVt7H8d-HtDo7Lh8B5itD8Z8FmoiM=s64","userId":"13305867008370392063"}},"trusted":true},"cell_type":"code","source":["from bert_multitask_learning import predict_bert_multitask"],"execution_count":null,"outputs":[]},{"metadata":{"id":"eaW8KwkDxoRp","executionInfo":{"status":"ok","timestamp":1598584421853,"user_tz":-480,"elapsed":963223,"user":{"displayName":"Jay Yip","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8HFC9Hw_HDfnVt7H8d-HtDo7Lh8B5itD8Z8FmoiM=s64","userId":"13305867008370392063"}},"outputId":"f61e8751-906e-498c-8629-6714e4554d65","trusted":true},"cell_type":"code","source":["# predict\n","import numpy as np\n","from bert_multitask_learning.utils import get_or_make_label_encoder\n","predict_params = DynamicBatchSizeParams()\n","# get prediction generator\n","pred_prob = predict_bert_multitask(\n","    inputs=['中国和美国在打贸易战']*10, \n","    problem='weibo_cws&weibo_ner', \n","    params=predict_params,\n","    processing_fn_dict=processing_fn_dict,\n","    problem_type_dict=problem_type_dict)\n","# get label encoder\n","ner_label_encoder = get_or_make_label_encoder(params=predict_params, problem='weibo_ner', mode='predict', label_list=[])\n","cws_label_encoder = get_or_make_label_encoder(params=predict_params, problem='weibo_cws', mode='predict', label_list=[])\n","\n","for prob in pred_prob:\n","    ner_pred = np.argmax(prob['weibo_ner'], axis = -1)\n","    print(ner_label_encoder.inverse_transform(ner_pred.tolist()))"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}