{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask Learning Model\n",
    "Multitask Learning Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from typing import Dict, Tuple\n",
    "from inspect import signature\n",
    "\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "from bert_multitask_learning.modeling import MultiModalBertModel\n",
    "from bert_multitask_learning.params import BaseParams\n",
    "from bert_multitask_learning.top import (Classification, MultiLabelClassification, PreTrain,\n",
    "                  Seq2Seq, SequenceLabel, MaskLM)\n",
    "from bert_multitask_learning.utils import get_embedding_table_from_model, get_transformer_main_model\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.compat.v1.name_scope(name):\n",
    "        mean = tf.reduce_mean(input_tensor=var)\n",
    "        tf.compat.v1.summary.scalar('mean', mean)\n",
    "        with tf.compat.v1.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(\n",
    "                input_tensor=tf.square(var - mean)))\n",
    "        tf.compat.v1.summary.scalar('stddev', stddev)\n",
    "        tf.compat.v1.summary.scalar('max', tf.reduce_max(input_tensor=var))\n",
    "        tf.compat.v1.summary.scalar('min', tf.reduce_min(input_tensor=var))\n",
    "        tf.compat.v1.summary.histogram('histogram', var)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def filter_loss(loss, features, problem):\n",
    "\n",
    "    if tf.reduce_mean(input_tensor=features['%s_loss_multiplier' % problem]) == 0:\n",
    "        return_loss = 0.0\n",
    "    else:\n",
    "        return_loss = loss\n",
    "\n",
    "    return return_loss\n",
    "\n",
    "\n",
    "class BertMultiTaskBody(tf.keras.Model):\n",
    "    \"\"\"Model to extract bert features and dispatch corresponding rows to each problem_chunk.\n",
    "\n",
    "    for each problem chunk, we extract corresponding features\n",
    "    and hidden features for that problem. The reason behind this\n",
    "    is to save computation for downstream processing.\n",
    "    For example, we have a batch of two instances and they're from\n",
    "    problem a and b respectively:\n",
    "    Input:\n",
    "    [{'input_ids': [1,2,3], 'a_loss_multiplier': 1, 'b_loss_multiplier': 0},\n",
    "     {'input_ids': [4,5,6], 'a_loss_multiplier': 0, 'b_loss_multiplier': 1}]\n",
    "    Output:\n",
    "    {\n",
    "      'a': {'input_ids': [1,2,3], 'a_loss_multiplier': 1, 'b_loss_multiplier': 0}\n",
    "      'b': {'input_ids': [4,5,6], 'a_loss_multiplier': 0, 'b_loss_multiplier': 1}\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: BaseParams, name='BertMultiTaskBody'):\n",
    "        super(BertMultiTaskBody, self).__init__(name=name)\n",
    "        self.params = params\n",
    "        self.bert = MultiModalBertModel(params=self.params)\n",
    "        if self.params.custom_pooled_hidden_size:\n",
    "            self.custom_pooled_layer = tf.keras.layers.Dense(\n",
    "                self.params.custom_pooled_hidden_size, activation=tf.keras.activations.selu)\n",
    "        else:\n",
    "            self.custom_pooled_layer = None\n",
    "\n",
    "    @tf.function\n",
    "    def get_features_for_problem(self, features, hidden_feature, problem, mode):\n",
    "        # get features with ind == 1\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            feature_this_round = features\n",
    "            hidden_feature_this_round = hidden_feature\n",
    "        else:\n",
    "            multiplier_name = '%s_loss_multiplier' % problem\n",
    "\n",
    "            record_ind = tf.where(tf.cast(\n",
    "                tf.squeeze(features[multiplier_name]), tf.bool))\n",
    "\n",
    "            hidden_feature_this_round = {}\n",
    "            for hidden_feature_name in hidden_feature:\n",
    "                if hidden_feature_name != 'embed_table':\n",
    "                    hidden_feature_this_round[hidden_feature_name] = tf.squeeze(tf.gather(\n",
    "                        hidden_feature[hidden_feature_name], record_ind, axis=0\n",
    "                    ), axis=1)\n",
    "                    hidden_feature_this_round[hidden_feature_name].set_shape(\n",
    "                        hidden_feature[hidden_feature_name].shape.as_list())\n",
    "                else:\n",
    "                    hidden_feature_this_round[hidden_feature_name] = hidden_feature[hidden_feature_name]\n",
    "\n",
    "            feature_this_round = {}\n",
    "            for features_name in features:\n",
    "                feature_this_round[features_name] = tf.gather_nd(\n",
    "                    features[features_name],\n",
    "                    record_ind)\n",
    "\n",
    "        return feature_this_round, hidden_feature_this_round\n",
    "\n",
    "    def call(self, inputs: Dict[str, tf.Tensor],\n",
    "             mode: str) -> Tuple[Dict[str, Dict[str, tf.Tensor]], Dict[str, Dict[str, tf.Tensor]]]:\n",
    "        _ = self.bert(inputs, mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "        # extract bert hidden features\n",
    "        inputs['model_input_mask'] = self.bert.get_input_mask()\n",
    "        inputs['model_token_type_ids'] = self.bert.get_token_type_ids()\n",
    "\n",
    "        hidden_feature = {}\n",
    "        for logit_type in ['seq', 'pooled', 'all', 'embed', 'embed_table']:\n",
    "            if logit_type == 'seq':\n",
    "                # tensor, [batch_size, seq_length, hidden_size]\n",
    "                hidden_feature[logit_type] = self.bert.get_sequence_output()\n",
    "            elif logit_type == 'pooled':\n",
    "                # tensor, [batch_size, hidden_size]\n",
    "                hidden_feature[logit_type] = self.bert.get_pooled_output()\n",
    "                if self.custom_pooled_layer:\n",
    "                    hidden_feature[logit_type] = self.custom_pooled_layer(\n",
    "                        hidden_feature[logit_type])\n",
    "            elif logit_type == 'all':\n",
    "                # list, num_hidden_layers * [batch_size, seq_length, hidden_size]\n",
    "                hidden_feature[logit_type] = self.bert.get_all_encoder_layers()\n",
    "            elif logit_type == 'embed':\n",
    "                # for res connection\n",
    "                hidden_feature[logit_type] = self.bert.get_embedding_output()\n",
    "            elif logit_type == 'embed_table':\n",
    "                hidden_feature[logit_type] = self.bert.get_embedding_table()\n",
    "\n",
    "        # for each problem chunk, we extract corresponding features\n",
    "        # and hidden features for that problem. The reason behind this\n",
    "        # is to save computation for downstream processing.\n",
    "        # For example, we have a batch of two instances and they're from\n",
    "        # problem a and b respectively:\n",
    "        # Input:\n",
    "        # [{'input_ids': [1,2,3], 'a_loss_multiplier': 1, 'b_loss_multiplier': 0},\n",
    "        #  {'input_ids': [4,5,6], 'a_loss_multiplier': 0, 'b_loss_multiplier': 1}]\n",
    "        # Output:\n",
    "        # {\n",
    "        #   'a': {'input_ids': [1,2,3], 'a_loss_multiplier': 1, 'b_loss_multiplier': 0}\n",
    "        #   'b': {'input_ids': [4,5,6], 'a_loss_multiplier': 0, 'b_loss_multiplier': 1}\n",
    "        # }\n",
    "        features = inputs\n",
    "        return_feature = {}\n",
    "        return_hidden_feature = {}\n",
    "\n",
    "        for problem_dict in self.params.run_problem_list:\n",
    "            for problem in problem_dict:\n",
    "                if self.params.task_transformer:\n",
    "                    # hidden_feature = task_tranformer_hidden_feature[problem]\n",
    "                    raise NotImplementedError\n",
    "\n",
    "                if len(self.params.run_problem_list) > 1:\n",
    "                    feature_this_round, hidden_feature_this_round = self.get_features_for_problem(\n",
    "                        features, hidden_feature, problem, mode)\n",
    "                else:\n",
    "                    feature_this_round, hidden_feature_this_round = features, hidden_feature\n",
    "\n",
    "                if self.params.label_transfer and self.params.grid_transformer:\n",
    "                    raise ValueError(\n",
    "                        'Label Transfer and grid transformer cannot be enabled in the same time.'\n",
    "                    )\n",
    "\n",
    "                if self.params.grid_transformer:\n",
    "                    raise NotImplementedError\n",
    "                return_hidden_feature[problem] = hidden_feature_this_round\n",
    "                return_feature[problem] = feature_this_round\n",
    "        return return_feature, return_hidden_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6211431 0.25003849 0.34417811 0.83333836 0.29095359\n",
      "  0.15957178 0.35497674 0.70136459 0.6841157 ]\n",
      " [0.39214657 0.40482128 0.72668433 0.09576186 0.64503298 0.65570469\n",
      "  0.97768662 0.04\n",
      "INFO:tensorflow:input_ids: [101, 2454, 1346, 3791, 2360, 1501, 1456, 782, 4495, 1963, 1398, 6624, 6822, 671, 4275, 2255, 3717, 8024, 7474, 7474, 4638, 1461, 1429, 8024, 2128, 7474, 4638, 3615, 6605, 8024, 6821, 2218, 3221, 4495\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:weibo_fake_multi_cls_label_ids: [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:image_input: [[0.04128798 0.86211431 0.25003849 0.34417811 0.83333836 0.29095359\n",
      "  0.15957178 0.35497674 0.70136459 0.6841157 ]\n",
      " [0.39214657 0.40482128 0.72668433 0.09576186 0.64503298 0.65570469\n",
      "  0.97768662 0.04\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:['对', '，', '输', '给', '一', '个', '女', '人', '，', '的', '成', '绩', '。', '失', '望']\n",
      "INFO:tensorflow:input_ids: [101, 2190, 8024, 103, 103, 103, 702, 1957, 782, 8024, 4638, 103, 5327, 103, 1927, 3307, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [3, 4, 5, 11, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [6783 5314  671 2768  511    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['今', '天', '下', '午', '起', '来', '看', '到', '外', '面', '的', '太', '阳', '。', '。', '。', '。', '我', '第', '一', '反', '应', '竟', '然', '是', '强', '烈', '的', '想', '回', '家', '泪', '想', '我', '们', '一', '起', '在', '嘉', '鱼',\n",
      "INFO:tensorflow:input_ids: [101, 791, 103, 678, 1286, 6629, 103, 4692, 103, 1912, 7481, 103, 1922, 7345, 511, 103, 511, 511, 103, 5018, 671, 1353, 2418, 103, 4197, 3221, 2487, 4164, 4638, 2682, 1726, 2157, 3801, 2682, 2769, 812\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [2, 6, 8, 11, 15, 18, 23, 37, 38, 58, 60, 74, 75, 79, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1921 3341 1168 4638  511 2769 4994 6629 1762  872 3330 1147 2458 2552\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['今', '年', '拜', '年', '不', '短', '信', '，', '就', '在', '微', '博', '拜', '大', '年', '寻', '龙', '记']\n",
      "INFO:tensorflow:input_ids: [101, 791, 2399, 2876, 2399, 103, 103, 928, 8024, 2218, 1762, 2544, 103, 2876, 1920, 2399, 103, 103, 6381, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [5, 6, 12, 16, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [ 679 4764 1300 2192 7987    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['浑', '身', '酸', '疼', '，', '两', '腿', '无', '力', '，', '眼', '神', '呆', '滞', '，', '怎', '么', '了']\n",
      "INFO:tensorflow:input_ids: [101, 3847, 6716, 7000, 103, 103, 697, 5597, 3187, 1213, 103, 4706, 4868, 1438, 103, 103, 103, 720, 749, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [4, 5, 10, 14, 15, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [4563 8024 8024 4005 8024 2582    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['明', '显', '紧', '张', '状', '态', '没', '出', '来', '，', '失', '误', '多', '。']\n",
      "INFO:tensorflow:input_ids: [101, 103, 3227, 5165, 2476, 4307, 2578, 3766, 1139, 3341, 8024, 1927, 6428, 1914, 511, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3209    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['三', '十', '年', '前', '，', '老', '爹', '带', '我', '来', '这', '里', '开', '会', '，', '今', '天', '打', '了', '个', '颠', '倒', '。', '我', '在', '这', '里']\n",
      "INFO:tensorflow:input_ids: [101, 676, 1282, 2399, 1184, 8024, 5439, 4269, 2372, 2769, 103, 6821, 7027, 2458, 833, 103, 791, 1921, 2802, 749, 702, 7585, 948, 511, 2769, 1762, 6821, 7027, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [10, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3341 8024    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['好', '活', '动', '呀', '，', '给', '力', '的', '商', '家', '，', '必', '须', '支', '持', '，', '希', '望', '以', '后', '多', '多', '办', '活', '动', '跟', '我', '们', '互', '动', '哈', '巧', '慧', '铣']\n",
      "INFO:tensorflow:input_ids: [101, 1962, 3833, 1220, 1435, 8024, 5314, 1213, 4638, 1555, 2157, 8024, 2553, 7557, 103, 103, 8024, 103, 3307, 809, 1400, 1914, 1914, 1215, 3833, 1220, 103, 103, 812, 757, 1220, 1506, 103, 2716, 7203,\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [14, 15, 17, 26, 27, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3118 2898 2361 6656 2769 2341    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['人', '生', '如', '戏', '，', '导', '演', '是', '自', '己', '蜡', '烛']\n",
      "INFO:tensorflow:input_ids: [101, 782, 103, 1963, 2767, 8024, 2193, 4028, 3221, 5632, 2346, 6058, 4169, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [4495    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['听', '说', '小', '米', '开', '卖', '了', '，', '刚', '刚', '预', '约', '了', '台', '。', '我', '爱', '小', '米', '手', '机', '因', '为', '他', '是', '迄', '今', '为', '止', '最', '快', '的', '小', '米', '手', '机', '月', '日', '中', '午',\n",
      "INFO:tensorflow:input_ids: [101, 1420, 6432, 2207, 5101, 2458, 1297, 749, 8024, 1157, 1157, 7564, 5276, 749, 1378, 511, 2769, 4263, 2207, 5101, 2797, 3322, 1728, 711, 800, 3221, 6812, 791, 711, 103, 103, 103, 4638, 103, 5101, 2\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [29, 30, 31, 33, 47, 59, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3632 3297 2571 2207 1164 1765    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['最', '热', '时', '尚', '榜', '女', '人', '不', '坏', '，', '男', '人', '不', '爱', '，', '一', '个', '男', '女', '必', '看', '的', '微', '博', '花', '心']\n",
      "INFO:tensorflow:input_ids: [101, 103, 103, 3198, 2213, 3528, 1957, 782, 679, 1776, 8024, 4511, 782, 679, 4263, 8024, 671, 702, 4511, 1957, 2553, 4692, 4638, 2544, 1300, 5709, 2552, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3297 4178    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['回', '复', '支', '持', '，', '赞', '成', '，', '哈', '哈', '米', '八', '吴', '够', '历', '史', '要', '的', '陈', '小', '奥', '丁', '丁', '我', '爱', '小', '肥', '肥', '一', '族', '大', '头', '仔', '大', '家', '团', '结', '一', '致', '，',\n",
      "INFO:tensorflow:input_ids: [101, 103, 1908, 3118, 103, 8024, 6614, 2768, 8024, 1506, 1506, 103, 1061, 103, 1916, 1325, 103, 6206, 103, 103, 2207, 1952, 672, 672, 2769, 4263, 2207, 5503, 5503, 671, 103, 1920, 1928, 798, 1920, 21\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [1, 4, 11, 13, 16, 18, 19, 30, 37, 48, 51, 53, 57, 59, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1726 2898 5101 1426 1380 4638 7357 3184 5310 6983 1259 8024 1213 1920\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['剑', '网', '乱', '世', '长', '安', '公', '测', '盛', '典', '今', '日', '开', '启', '，', '海', '量', '豪', '礼', '火', '爆', '开', '送', '精', '美', '挂', '件', '、', '听', '雨', '·', '汉', '服', '娃', '娃', '、', '诙', '谐', '双', '骑',\n",
      "INFO:tensorflow:input_ids: [101, 103, 103, 744, 686, 103, 2128, 1062, 3844, 4670, 1073, 791, 3189, 2458, 1423, 8024, 3862, 7030, 6498, 4851, 4125, 4255, 103, 6843, 5125, 5401, 103, 816, 510, 1420, 7433, 185, 3727, 3302, 2015, 2\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [1, 2, 5, 22, 26, 37, 43, 55, 61, 63, 73, 74, 79, 80, 89, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1187 5381 7270 2458 2899 6410 6756 2797 7028 1599 2347 3300 5381 2135\n",
      " 1218    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
      "INFO:tensorflow:['在', '街', '上', '听', '见', '音', '乐', '我', '舞', '动', '起', '来', '很', '丢', '人', '？', '真', '的', '很', '丢', '人', '吗', '？']\n",
      "INFO:tensorflow:input_ids: [101, 103, 6125, 677, 1420, 6224, 103, 727, 2769, 5659, 1220, 6629, 3341, 2523, 103, 782, 8043, 4696, 4638, 103, 696, 782, 1408, 8043, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [1, 6, 14, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1762 7509  696 2523    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['三', '毛', '说', '我', '唯', '一', '锲', '而', '不', '舍', '，', '愿', '意', '以', '自', '己', '的', '生', '命', '去', '努', '力', '的', '，', '只', '不', '过', '是', '保', '守', '我', '个', '人', '的', '心', '怀', '意', '念', '，', '在',\n",
      "INFO:tensorflow:input_ids: [101, 676, 3688, 103, 2769, 1546, 671, 7244, 5445, 679, 5650, 8024, 2703, 2692, 103, 5632, 2346, 4638, 4495, 1462, 1343, 1222, 1213, 4638, 8024, 1372, 679, 6814, 3221, 924, 2127, 2769, 702, 782, 4638,\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [3, 14, 35, 45, 49, 57, 60, 66, 72, 73, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [6432  809 2552 3189  702 2461 3833 4708 3198 4958    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['星', '期', '天', '的', '早', '晨', '七', '点', '学', '车', '，', '驾', '校', '太', '给', '力', '。', '我', '的', '头', '发', '都', '没', '有', '洗']\n",
      "INFO:tensorflow:input_ids: [101, 3215, 3309, 1921, 4638, 3193, 3247, 673, 4157, 2110, 6756, 8024, 7730, 3413, 1922, 5314, 1213, 511, 2769, 4638, 1928, 103, 6963, 3766, 3300, 103, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [21, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1355 3819    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['发', '表', '了', '博', '文', '小', '学', '美', '术', '新', '课', '标', '的', '反', '思', '学', '习', '新', '学', '期', '开', '学', '有', '两', '个', '多', '月', '了', '，', '在', '这', '段', '时', '间', '的', '教', '学', '计', '划', '、',\n",
      "INFO:tensorflow:input_ids: [101, 1355, 6134, 749, 1300, 3152, 2207, 2110, 103, 3318, 3173, 6440, 3403, 4638, 1353, 2590, 2110, 739, 3173, 103, 3309, 2458, 2110, 103, 697, 702, 1914, 3299, 749, 8024, 1762, 6821, 3667, 103, 7313,\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [8, 19, 23, 33, 38, 40, 46, 47, 52, 64, 69, 82, 86, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [5401 2110 3300 3198 6369  510 2339  868 2190 6822 6399 2945 4415    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['哈', '哈', '哈', '哈', '哈', '哈', '镰', '刀', '刮', '腋', '毛', '哈', '哈', '哈', '哈', '哈', '哈', '我', '的', '朋', '友', '是', '个', '呆', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '十', '万', '个',\n",
      "INFO:tensorflow:input_ids: [101, 1506, 1506, 1506, 1506, 103, 103, 103, 1143, 1167, 5573, 3688, 1506, 1506, 1506, 1506, 1506, 1506, 103, 4638, 3301, 1351, 3221, 702, 103, 103, 1506, 1506, 1506, 1506, 1506, 103, 1506, 1506, 103,\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [5, 6, 7, 18, 24, 25, 31, 34, 37, 42, 47, 51, 53, 59, 77, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1506 1506 7266 2769 1438 1506 1506 1506 1506 5010 6821 2094 1506 1506\n",
      " 4343    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
      "INFO:tensorflow:['�', '�', '就', '爱', '这', '个', '牌', '子', '的', '糖', '果']\n",
      "INFO:tensorflow:input_ids: [101, 103, 103, 2218, 4263, 6821, 702, 4277, 2094, 4638, 5131, 3362, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: []\n",
      "INFO:tensorflow:masked_lm_weights: []\n",
      "INFO:tensorflow:['延', '参', '法', '师', '品', '味', '人', '生', '如', '同', '走', '进', '一', '片', '山', '水', '，', '静', '静', '的', '呼', '吸', '，', '安', '静', '的', '欣', '赏', '，', '这', '就', '是', '生', '活', '。']\n",
      "INFO:tensorflow:input_ids: [101, 2454, 1346, 3791, 2360, 1501, 1456, 782, 4495, 1963, 1398, 6624, 103, 671, 4275, 2255, 3717, 8024, 103, 7474, 4638, 1461, 1429, 8024, 2128, 7474, 4638, 3615, 6605, 8024, 103, 2218, 3221, 4495, 1\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [12, 18, 30, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [6822 7474 6821 3833    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['就', '感', '恩', '吧', '越', '来', '越', '没', '战', '斗', '力', '了', '。']\n",
      "INFO:tensorflow:input_ids: [101, 2218, 2697, 2617, 103, 6632, 3341, 6632, 3766, 2773, 3159, 1213, 749, 103, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [4, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1416  511    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.5\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.25\n",
      "INFO:tensorflow:weibo_masklm: 0.25\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.5\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.25\n",
      "INFO:tensorflow:weibo_masklm: 0.25\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from bert_multitask_learning.test_base import TestBase\n",
    "import bert_multitask_learning\n",
    "import shutil\n",
    "import numpy as np\n",
    "test_base = TestBase()\n",
    "test_base.params.assign_problem(\n",
    "    'weibo_fake_ner&weibo_fake_cls|weibo_fake_multi_cls|weibo_masklm')\n",
    "params = test_base.params\n",
    "train_dataset = bert_multitask_learning.train_eval_input_fn(\n",
    "    params=params, mode=bert_multitask_learning.TRAIN)\n",
    "eval_dataset = bert_multitask_learning.train_eval_input_fn(\n",
    "    params=params, mode=bert_multitask_learning.EVAL\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "one_batch_data = next(train_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/voidful/albert_chinese_tiny/resolve/main/tf_model.h5\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n"
     ]
    }
   ],
   "source": [
    "mtl_body = BertMultiTaskBody(params=params)\n",
    "features, hidden_features = mtl_body(one_batch_data, mode=bert_multitask_learning.TRAIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for problem, problem_feature_dict in features.items():\n",
    "    problem_loss_multiplier = problem_feature_dict['{}_loss_multiplier'.format(problem)]\n",
    "    num_rows = problem_loss_multiplier.shape[0]\n",
    "    # all loss multiplier of its corresponding problem should all be one\n",
    "    assert tf.reduce_all(problem_loss_multiplier == 1).numpy()\n",
    "    # batch size of features and hidden_features should be the same\n",
    "    assert num_rows == hidden_features[problem]['pooled'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "class BertMultiTaskTop(tf.keras.Model):\n",
    "    \"\"\"Model to create top layer, aka classification layer, for each problem.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: BaseParams, name='BertMultiTaskTop', input_embeddings: tf.Tensor = None):\n",
    "        super(BertMultiTaskTop, self).__init__(name=name)\n",
    "        self.params = params\n",
    "\n",
    "        problem_type_layer = {\n",
    "            'seq_tag': SequenceLabel,\n",
    "            'cls': Classification,\n",
    "            'seq2seq_tag': Seq2Seq,\n",
    "            'seq2seq_text': Seq2Seq,\n",
    "            'multi_cls': MultiLabelClassification,\n",
    "            'pretrain': PreTrain,\n",
    "            'masklm': MaskLM\n",
    "        }\n",
    "        problem_type_layer.update(self.params.top_layer)\n",
    "        self.top_layer_dict = {}\n",
    "        for problem_dict in self.params.run_problem_list:\n",
    "            for problem in problem_dict:\n",
    "                problem_type = self.params.problem_type[problem]\n",
    "                # some layers has different signatures, assign inputs accordingly\n",
    "                layer_signature_name = signature(\n",
    "                    problem_type_layer[problem_type].__init__).parameters.keys()\n",
    "                inputs_kwargs = {\n",
    "                    'params': self.params,\n",
    "                    'problem_name': problem\n",
    "                }\n",
    "                for signature_name in layer_signature_name:\n",
    "                    if signature_name == 'input_embeddings':\n",
    "                        inputs_kwargs.update(\n",
    "                            {signature_name: input_embeddings})\n",
    "\n",
    "                self.top_layer_dict[problem] = problem_type_layer[problem_type](\n",
    "                    **inputs_kwargs)\n",
    "\n",
    "    def call(self,\n",
    "             inputs: Tuple[Dict[str, Dict[str, tf.Tensor]], Dict[str, Dict[str, tf.Tensor]]],\n",
    "             mode: str) -> Dict[str, tf.Tensor]:\n",
    "        features, hidden_feature = inputs\n",
    "        return_dict = {}\n",
    "\n",
    "        for problem_dict in self.params.run_problem_list:\n",
    "            for problem in problem_dict:\n",
    "                feature_this_round = features[problem]\n",
    "                hidden_feature_this_round = hidden_feature[problem]\n",
    "                problem_type = self.params.problem_type[problem]\n",
    "\n",
    "                # if pretrain, return pretrain logit\n",
    "                if problem_type == 'pretrain':\n",
    "                    pretrain = self.top_layer_dict[problem]\n",
    "                    return_dict[problem] = pretrain(\n",
    "                        (feature_this_round, hidden_feature_this_round), mode)\n",
    "                    return return_dict\n",
    "\n",
    "                if self.params.label_transfer and self.params.grid_transformer:\n",
    "                    raise ValueError(\n",
    "                        'Label Transfer and grid transformer cannot be enabled in the same time.'\n",
    "                    )\n",
    "\n",
    "                with tf.name_scope(problem):\n",
    "                    layer = self.top_layer_dict[problem]\n",
    "\n",
    "                    return_dict[problem] = layer(\n",
    "                        (feature_this_round, hidden_feature_this_round), mode)\n",
    "\n",
    "        if self.params.augument_mask_lm and mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            raise NotImplementedError\n",
    "            # try:\n",
    "            #     mask_lm_top = MaskLM(self.params)\n",
    "            #     return_dict['augument_mask_lm'] = \\\n",
    "            #         mask_lm_top(features,\n",
    "            #                     hidden_feature, mode, 'dummy')\n",
    "            # except ValueError:\n",
    "            #     pass\n",
    "        return return_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n"
     ]
    }
   ],
   "source": [
    "# use embedding table as the classification top of mask lm\n",
    "_ = bert_multitask_learning.utils.get_embedding_table_from_model(mtl_body.bert.bert_model)\n",
    "main_model = bert_multitask_learning.utils.get_transformer_main_model(mtl_body.bert.bert_model)\n",
    "input_embeddings = main_model.embeddings\n",
    "mtl_top = BertMultiTaskTop(params=params, input_embeddings=input_embeddings)\n",
    "logit_dict = mtl_top((features, hidden_features), mode=bert_multitask_learning.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for problem, problem_logit in logit_dict.items():\n",
    "    # last dim of logits equals to num_classes\n",
    "    assert problem_logit.shape[-1] == params.num_classes[problem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "class BertMultiTask(tf.keras.Model):\n",
    "    def __init__(self, params: BaseParams, name='BertMultiTask') -> None:\n",
    "        super(BertMultiTask, self).__init__(name=name)\n",
    "        self.params = params\n",
    "        # initialize body model, aka transformers\n",
    "        self.body = BertMultiTaskBody(params=self.params)\n",
    "        # mlm might need word embedding from bert\n",
    "        # build sub-model\n",
    "        _ = get_embedding_table_from_model(self.body.bert.bert_model)\n",
    "        main_model = get_transformer_main_model(self.body.bert.bert_model)\n",
    "        # input_embeddings = self.body.bert.bert_model.bert.embeddings\n",
    "        input_embeddings = main_model.embeddings\n",
    "        self.top = BertMultiTaskTop(\n",
    "            params=self.params, input_embeddings=input_embeddings)\n",
    "\n",
    "    def call(self, inputs, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "        feature_per_problem, hidden_feature_per_problem = self.body(\n",
    "            inputs, mode)\n",
    "        pred_per_problem = self.top(\n",
    "            (feature_per_problem, hidden_feature_per_problem), mode)\n",
    "        return pred_per_problem\n",
    "\n",
    "    def compile(self):\n",
    "        super(BertMultiTask, self).compile()\n",
    "        logger = tf.get_logger()\n",
    "        logger.info('Initial lr: {}'.format(self.params.lr))\n",
    "        logger.info('Train steps: {}'.format(self.params.train_steps))\n",
    "        logger.info('Warmup steps: {}'.format(self.params.num_warmup_steps))\n",
    "        self.optimizer, self.lr_scheduler = transformers.optimization_tf.create_optimizer(\n",
    "            init_lr=self.params.lr,\n",
    "            num_train_steps=self.params.train_steps,\n",
    "            num_warmup_steps=self.params.num_warmup_steps,\n",
    "            weight_decay_rate=0.01\n",
    "        )\n",
    "        self.mean_acc = tf.keras.metrics.Mean(name='mean_acc')\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            _ = self(data, mode=tf.estimator.ModeKeys.TRAIN)\n",
    "            # gather losses from all problems\n",
    "            loss_dict = {'{}_loss'.format(problem_name): tf.reduce_sum(top_layer.losses) for problem_name,\n",
    "                         top_layer in self.top.top_layer_dict.items()}\n",
    "            # metric_dict = {'{}_metric'.format(problem_name): tf.reduce_mean(top_layer.metrics) for problem_name,\n",
    "            #                top_layer in self.top.top_layer_dict.items()}\n",
    "            metric_dict = {m.name: m.result() for m in self.metrics}\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(self.losses, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        self.mean_acc.update_state(\n",
    "            [v for n, v in metric_dict.items() if n != 'mean_acc'])\n",
    "\n",
    "        return_dict = metric_dict\n",
    "        return_dict.update(loss_dict)\n",
    "        return_dict[self.mean_acc.name] = self.mean_acc.result()\n",
    "\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return return_dict\n",
    "\n",
    "    def test_step(self, data):\n",
    "        \"\"\"The logic for one evaluation step.\n",
    "\n",
    "        This method can be overridden to support custom evaluation logic.\n",
    "        This method is called by `Model.make_test_function`.\n",
    "\n",
    "        This function should contain the mathemetical logic for one step of\n",
    "        evaluation.\n",
    "        This typically includes the forward pass, loss calculation, and metrics\n",
    "        updates.\n",
    "\n",
    "        Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
    "        `tf.distribute.Strategy` settings), should be left to\n",
    "        `Model.make_test_function`, which can also be overridden.\n",
    "\n",
    "        Arguments:\n",
    "        data: A nested structure of `Tensor`s.\n",
    "\n",
    "        Returns:\n",
    "        A `dict` containing values that will be passed to\n",
    "        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
    "        values of the `Model`'s metrics are returned.\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = self(data, mode=tf.estimator.ModeKeys.EVAL)\n",
    "        # Updates stateful loss metrics.\n",
    "        self.compiled_loss(\n",
    "            None, y_pred, None, regularization_losses=self.losses)\n",
    "\n",
    "        self.compiled_metrics.update_state(None, y_pred, None)\n",
    "\n",
    "        # get metrics to calculate mean\n",
    "        m_list = []\n",
    "        for metric in self.metrics:\n",
    "            if 'mean_acc' in metric.name:\n",
    "                continue\n",
    "            if 'acc' in metric.name:\n",
    "                m_list.append(metric.result())\n",
    "\n",
    "            if 'f1' in metric.name:\n",
    "                m_list.append(metric.result())\n",
    "\n",
    "        self.mean_acc.update_state(\n",
    "            m_list)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        return self(data, mode=tf.estimator.ModeKeys.PREDICT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/voidful/albert_chinese_tiny/resolve/main/tf_model.h5\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7fea9cd45dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7fea9cd45dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7fea9cd45dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7fea9cd45dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 10 calls to <function empty_tensor_handling_loss at 0x7feaf0e65af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function empty_tensor_handling_loss at 0x7feaf0e65af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "mtl = BertMultiTask(params=params)\n",
    "logit_dict = mtl(one_batch_data)\n",
    "for problem, problem_logit in logit_dict.items():\n",
    "    # last dim of logits equals to num_classes\n",
    "    assert problem_logit.shape[-1] == params.num_classes[problem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
