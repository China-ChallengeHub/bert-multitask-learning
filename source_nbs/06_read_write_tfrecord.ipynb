{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp read_write_tfrecord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Write TFRecord\n",
    "\n",
    "Write bert features to TFRecord and read bert features from TFRecord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from bert_multitask_learning.bert_preprocessing.create_bert_features import (\n",
    "    create_bert_features, create_multimodal_bert_features)\n",
    "from bert_multitask_learning.special_tokens import EVAL, TRAIN\n",
    "from bert_multitask_learning.params import BaseParams\n",
    "\n",
    "\n",
    "def _float_list_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_list_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_list_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def serialize_fn(features: dict, return_feature_desc=False):\n",
    "    features_tuple = {}\n",
    "    feature_desc = {}\n",
    "    for feature_name, feature in features.items():\n",
    "        if type(feature) is list:\n",
    "            feature = np.array(feature)\n",
    "        if type(feature) is np.ndarray:\n",
    "            if issubclass(feature.dtype.type, np.integer):\n",
    "                features_tuple[feature_name] = _int64_list_feature(\n",
    "                    feature.flatten())\n",
    "                feature_desc[feature_name] = 'int64'\n",
    "            # elif issubclass(feature.dtype.type, np.float):\n",
    "            else:\n",
    "                features_tuple[feature_name] = _float_list_feature(\n",
    "                    feature.flatten())\n",
    "                feature_desc[feature_name] = 'float32'\n",
    "\n",
    "            features_tuple['{}_shape'.format(\n",
    "                feature_name)] = _int64_list_feature(feature.shape)\n",
    "            feature_desc['{}_shape_value'.format(feature_name)] = feature.shape\n",
    "\n",
    "            feature_desc['{}_shape'.format(\n",
    "                feature_name)] = 'int64'\n",
    "\n",
    "            # this seems not a good idea\n",
    "            if len(feature.shape) > 1:\n",
    "                feature_desc['{}_shape_value'.format(feature_name)] = [\n",
    "                    None] + list(feature.shape[1:])\n",
    "            else:\n",
    "                feature_desc['{}_shape_value'.format(feature_name)] = [\n",
    "                    None for _ in feature.shape]\n",
    "\n",
    "        elif np.issubdtype(type(feature), np.float):\n",
    "            features_tuple[feature_name] = _float_feature(feature)\n",
    "            features_tuple['{}_shape'.format(\n",
    "                feature_name)] = _int64_list_feature([])\n",
    "            feature_desc[feature_name] = 'float32'\n",
    "\n",
    "            feature_desc['{}_shape'.format(\n",
    "                feature_name)] = 'int64'\n",
    "            feature_desc['{}_shape_value'.format(feature_name)] = []\n",
    "        elif np.issubdtype(type(feature), np.integer):\n",
    "            features_tuple[feature_name] = _int64_feature(feature)\n",
    "            features_tuple['{}_shape'.format(\n",
    "                feature_name)] = _int64_list_feature([])\n",
    "            feature_desc[feature_name] = 'int64'\n",
    "            feature_desc['{}_shape'.format(\n",
    "                feature_name)] = 'int64'\n",
    "            feature_desc['{}_shape_value'.format(feature_name)] = []\n",
    "        else:\n",
    "            if isinstance(feature, str):\n",
    "                feature = feature.encode('utf8')\n",
    "            features_tuple[feature_name] = _bytes_feature(feature)\n",
    "            features_tuple['{}_shape'.format(\n",
    "                feature_name)] = _int64_list_feature([])\n",
    "            feature_desc[feature_name] = 'string'\n",
    "            feature_desc['{}_shape'.format(\n",
    "                feature_name)] = 'int64'\n",
    "            feature_desc['{}_shape_value'.format(feature_name)] = []\n",
    "\n",
    "    example_proto = tf.train.Example(\n",
    "        features=tf.train.Features(feature=features_tuple))\n",
    "    if return_feature_desc:\n",
    "        return example_proto.SerializeToString(), feature_desc\n",
    "\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new problem weibo_ner, problem type: seq_tag\nAdding new problem weibo_cws, problem type: seq_tag\nAdding new problem weibo_fake_multi_cls, problem type: multi_cls\nAdding new problem weibo_fake_cls, problem type: cls\nAdding new problem weibo_masklm, problem type: masklm\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from bert_multitask_learning.test_base import TestBase\n",
    "test_base = TestBase()\n",
    "test_features = {\n",
    "    'int_scalar': 1,\n",
    "    'float_scalar': 2.0,\n",
    "    'int_array': [1, 2, 3],\n",
    "    'float_array': np.array([4, 5, 6], dtype='float32'),\n",
    "    'int_matrix': [[1, 2, 3], [4, 5, 6]],\n",
    "    'float_matrix': np.random.uniform(size=(32, 5, 5)),\n",
    "    'string': 'this is test'\n",
    "}\n",
    "expected_desc = {'int_scalar': 'int64', 'int_scalar_shape': 'int64', 'int_scalar_shape_value': [],\n",
    "                    'float_scalar': 'float32', 'float_scalar_shape': 'int64', 'float_scalar_shape_value': [],\n",
    "                    'int_array': 'int64', 'int_array_shape_value': [None], 'int_array_shape': 'int64',\n",
    "                    'float_array': 'float32', 'float_array_shape_value': [None], 'float_array_shape': 'int64',\n",
    "                    'int_matrix': 'int64', 'int_matrix_shape_value': [None, 3], 'int_matrix_shape': 'int64',\n",
    "                    'float_matrix': 'float32', 'float_matrix_shape_value': [None, 5, 5], 'float_matrix_shape': 'int64',\n",
    "                    'string': 'string', 'string_shape': 'int64', 'string_shape_value': []}\n",
    "ser_str, feat_desc = serialize_fn(\n",
    "    features=test_features, return_feature_desc=True)\n",
    "assert feat_desc == expected_desc\n",
    "\n",
    "example = tf.train.Example()\n",
    "example.ParseFromString(ser_str)\n",
    "assert example.features.feature['int_array'].int64_list.value == [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def make_tfrecord(data_list, output_dir, serialize_fn, mode='train', shards_per_file=100000, prefix='', **kwargs):\n",
    "\n",
    "    # create output tfrecord path\n",
    "    os.makedirs(os.path.join(\n",
    "        output_dir, prefix), exist_ok=True)\n",
    "\n",
    "    def _write_fn(d_list, path, serialize_fn, mode='train'):\n",
    "        logging.info('Writing {}'.format(path))\n",
    "        feature_desc_path = os.path.join(os.path.dirname(\n",
    "            path), '{}_feature_desc.json'.format(mode))\n",
    "\n",
    "        with tf.io.TFRecordWriter(path) as writer:\n",
    "            for features in d_list:\n",
    "                example, feature_desc = serialize_fn(\n",
    "                    features, return_feature_desc=True)\n",
    "                writer.write(example)\n",
    "                if not os.path.exists(feature_desc_path):\n",
    "                    json.dump(feature_desc, open(\n",
    "                        feature_desc_path, 'w', encoding='utf8'))\n",
    "\n",
    "    _write_part_fn = partial(_write_fn, serialize_fn=serialize_fn, mode=mode)\n",
    "\n",
    "    x = []\n",
    "    shard_count = 0\n",
    "    for idx, example in enumerate(data_list):\n",
    "        x.append(example)\n",
    "        if idx % shards_per_file == 0 and idx:  # pragma: no cover\n",
    "            path = os.path.join(\n",
    "                output_dir, prefix, '{}_{:05d}.tfrecord'.format(mode, shard_count))\n",
    "            shard_count += 1\n",
    "            _write_part_fn(d_list=x, path=path)\n",
    "            x = []\n",
    "\n",
    "    # add remaining\n",
    "    if x:\n",
    "        path = os.path.join(\n",
    "            output_dir, prefix, '{}_{:05d}.tfrecord'.format(mode, shard_count))\n",
    "        _write_part_fn(d_list=x, path=path)\n",
    "\n",
    "\n",
    "def make_feature_desc(feature_desc_dict: dict):\n",
    "    feature_desc = {}\n",
    "    for feature_name, feature_type in feature_desc_dict.items():\n",
    "        if feature_type == 'int64':\n",
    "            feature_desc[feature_name] = tf.io.VarLenFeature(tf.int64)\n",
    "        elif feature_type == 'float32':\n",
    "            feature_desc[feature_name] = tf.io.VarLenFeature(tf.float32)\n",
    "\n",
    "    return feature_desc\n",
    "\n",
    "def write_single_problem_chunk_tfrecord(problem,\n",
    "                                        inputs_list,\n",
    "                                        target_list,\n",
    "                                        label_encoder,\n",
    "                                        params,\n",
    "                                        tokenizer,\n",
    "                                        mode):\n",
    "\n",
    "    def _make_single_problem_data_list(problem, inputs_list, target_list, label_encoder):\n",
    "        problem_type = params.problem_type[problem]\n",
    "\n",
    "        # whether this problem is sequential labeling\n",
    "        # for sequential labeling, targets needs to align with any\n",
    "        # change of inputs\n",
    "        is_seq = problem_type in ['seq_tag']\n",
    "        try:\n",
    "            example_list = list(zip(inputs_list, target_list))\n",
    "        except TypeError:\n",
    "            # target_list is None\n",
    "            example_list = inputs_list\n",
    "        # split data_list by shards_per_file\n",
    "        data_shards = []\n",
    "        for i in range(0, len(example_list), 10000):\n",
    "            data_shards.append(example_list[i:i + 10000])\n",
    "\n",
    "        if isinstance(inputs_list[0], dict) and 'a' not in inputs_list[0]:\n",
    "            part_fn = partial(create_multimodal_bert_features, problem=problem,\n",
    "                              label_encoder=label_encoder,\n",
    "                              params=params,\n",
    "                              tokenizer=tokenizer,\n",
    "                              mode=mode,\n",
    "                              problem_type=problem_type,\n",
    "                              is_seq=is_seq)\n",
    "        else:\n",
    "            part_fn = partial(create_bert_features, problem=problem,\n",
    "                              label_encoder=label_encoder,\n",
    "                              params=params,\n",
    "                              tokenizer=tokenizer,\n",
    "                              mode=mode,\n",
    "                              problem_type=problem_type,\n",
    "                              is_seq=is_seq)\n",
    "        data_list = Parallel(min(params.num_cpus, len(data_shards)))(delayed(part_fn)(example_list=d_list)\n",
    "                                                                     for d_list in data_shards)\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    # single problem in problem_chunk\n",
    "    if isinstance(problem, str) or (isinstance(problem, list) and len(problem) == 1):\n",
    "        if isinstance(problem, list):\n",
    "            problem = problem[0]\n",
    "        data_list = _make_single_problem_data_list(\n",
    "            problem, inputs_list=inputs_list, target_list=target_list, label_encoder=label_encoder)\n",
    "        data_list = [\n",
    "            item for sublist in data_list for item in sublist]\n",
    "\n",
    "    # multiple problem in problem chunk\n",
    "    else:\n",
    "        assert (type(problem), type(inputs_list),\n",
    "                type(target_list)) == (list, dict, dict)\n",
    "        # convert data_list to dataframe and join by input_ids\n",
    "        data_dict = {}\n",
    "        column_list = []\n",
    "        for pro in problem:\n",
    "            data_shards = _make_single_problem_data_list(\n",
    "                pro, inputs_list=inputs_list[pro], target_list=target_list[pro], label_encoder=label_encoder[pro])\n",
    "\n",
    "            data_dict[pro] = [\n",
    "                item for sublist in data_shards for item in sublist]\n",
    "            try:\n",
    "                column_list.append(list(data_dict[pro][0].keys()))\n",
    "            except IndexError:\n",
    "                raise IndexError(\"Problem {} has no data\".format(pro))\n",
    "\n",
    "        # get intersection and use as ensure features are the same\n",
    "        join_key = list(set(column_list[0]).intersection(*column_list[1:]))\n",
    "\n",
    "        flat_data_list = []\n",
    "        while data_dict[problem[0]]:\n",
    "            d = {}\n",
    "            for pro in data_dict:\n",
    "                if not d:\n",
    "                    d = data_dict[pro].pop(0)\n",
    "                else:\n",
    "                    for k in join_key:\n",
    "                        assert d[k] == data_dict[pro][0][k], 'At iteration {}, feature {} not align. Expected {}, got: {}'.format(\n",
    "                            len(flat_data_list), k, d[k], data_dict[pro][0][k]\n",
    "                        )\n",
    "                    d.update(data_dict[pro].pop(0))\n",
    "            flat_data_list.append(d)\n",
    "        data_list = flat_data_list\n",
    "\n",
    "    if isinstance(problem, list):\n",
    "        problem = '_'.join(sorted(problem))\n",
    "\n",
    "    make_tfrecord(data_list=data_list,\n",
    "                  output_dir=params.tmp_file_dir,\n",
    "                  serialize_fn=serialize_fn,\n",
    "                  prefix=problem,\n",
    "                  mode=mode)\n",
    "\n",
    "\n",
    "def write_single_problem_gen_tfrecord(problem,\n",
    "                                      gen,\n",
    "                                      label_encoder,\n",
    "                                      params,\n",
    "                                      tokenizer,\n",
    "                                      mode):\n",
    "\n",
    "    def _make_single_problem_data_gen(problem, example_list, label_encoder):\n",
    "        problem_type = params.problem_type[problem]\n",
    "\n",
    "        # whether this problem is sequential labeling\n",
    "        # for sequential labeling, targets needs to align with any\n",
    "        # change of inputs\n",
    "        is_seq = problem_type in ['seq_tag']\n",
    "        first_example = next(gen)\n",
    "\n",
    "        if isinstance(first_example[0], dict) and 'a' not in first_example[0]:\n",
    "            part_fn = partial(create_multimodal_bert_features, problem=problem,\n",
    "                              label_encoder=label_encoder,\n",
    "                              params=params,\n",
    "                              tokenizer=tokenizer,\n",
    "                              mode=mode,\n",
    "                              problem_type=problem_type,\n",
    "                              is_seq=is_seq)\n",
    "        else:\n",
    "            part_fn = partial(create_bert_features, problem=problem,\n",
    "                              label_encoder=label_encoder,\n",
    "                              params=params,\n",
    "                              tokenizer=tokenizer,\n",
    "                              mode=mode,\n",
    "                              problem_type=problem_type,\n",
    "                              is_seq=is_seq)\n",
    "\n",
    "        # get num_cpus*per_cpu_buffer instances\n",
    "        cpus = params.num_cpus\n",
    "        tmp_instances_list = [[] for _ in range(cpus)]\n",
    "        per_cpu_buffer = params.per_cpu_buffer\n",
    "        for example in example_list:\n",
    "            is_full = False\n",
    "            for i in range(cpus):\n",
    "                if len(tmp_instances_list[i]) < per_cpu_buffer:\n",
    "                    tmp_instances_list[i].append(example)\n",
    "                    if i == cpus - 1 and len(tmp_instances_list[i]) == per_cpu_buffer:\n",
    "                        is_full = True\n",
    "                    break\n",
    "\n",
    "            # if is full, process and clear\n",
    "            if is_full:\n",
    "                data_list = Parallel(min(params.num_cpus, len(tmp_instances_list)))(delayed(part_fn)(example_list=d_list)\n",
    "                                                                                    for d_list in tmp_instances_list)\n",
    "                # data_list = [part_fn(example_list=d_list)\n",
    "                #              for d_list in tmp_instances_list]\n",
    "                for d_list in data_list:\n",
    "                    for d in d_list:\n",
    "                        yield d\n",
    "\n",
    "                tmp_instances_list = [[] for _ in range(cpus)]\n",
    "        # process remaining\n",
    "        for tmp_list in tmp_instances_list:\n",
    "            if tmp_list:\n",
    "                d_list = part_fn(example_list=tmp_list)\n",
    "                for d in d_list:\n",
    "                    yield d\n",
    "\n",
    "    # single problem in problem_chunk\n",
    "    if isinstance(problem, str) or (isinstance(problem, list) and len(problem) == 1):\n",
    "        if isinstance(problem, list):\n",
    "            problem = problem[0]\n",
    "        data_list = _make_single_problem_data_gen(\n",
    "            problem, example_list=gen, label_encoder=label_encoder)\n",
    "\n",
    "    # multiple problem in problem chunk\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            'The problem returns generator, which dose not support & chaining')\n",
    "\n",
    "    if isinstance(problem, list):\n",
    "        problem = '_'.join(sorted(problem))\n",
    "\n",
    "    make_tfrecord(data_list=data_list,\n",
    "                  output_dir=params.tmp_file_dir,\n",
    "                  serialize_fn=serialize_fn,\n",
    "                  prefix=problem,\n",
    "                  mode=mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "make_tfrecord(\n",
    "    [test_features], output_dir=test_base.tmpfiledir, serialize_fn=serialize_fn)\n",
    "assert os.path.exists(os.path.join(\n",
    "    test_base.tmpfiledir, 'train_feature_desc.json'))\n",
    "assert os.path.exists(os.path.join(\n",
    "    test_base.tmpfiledir, 'train_00000.tfrecord'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def write_tfrecord(params, replace=False):\n",
    "    \"\"\"Write TFRecord for every problem chunk\n",
    "\n",
    "    Output location: params.tmp_file_dir\n",
    "\n",
    "    Arguments:\n",
    "        params {params} -- params\n",
    "\n",
    "    Keyword Arguments:\n",
    "        replace {bool} -- Whether to replace existing tfrecord (default: {False})\n",
    "    \"\"\"\n",
    "\n",
    "    read_data_fn_dict = params.read_data_fn\n",
    "    path_list = []\n",
    "    for problem_list in params.problem_chunk:\n",
    "        # if only one problem in problem chunk, create individual tf record\n",
    "        if len(problem_list) == 1:\n",
    "            problem = problem_list[0]\n",
    "            read_fn = read_data_fn_dict[problem]\n",
    "            file_dir = os.path.join(\n",
    "                params.tmp_file_dir, problem, 'train_feature_desc.json')\n",
    "            eval_file_dir = os.path.join(\n",
    "                params.tmp_file_dir, problem, 'eval_feature_desc.json')\n",
    "            if not os.path.exists(file_dir) or replace:\n",
    "                read_fn(params, TRAIN)\n",
    "            if not os.path.exists(eval_file_dir) or replace:\n",
    "                read_fn(params, EVAL)\n",
    "        # if more than one problem in problem chunk, that means multiple\n",
    "        # same feature space problems are chained by &. In this case, we\n",
    "        # need to aggregate data from these problems first, then write to one\n",
    "        # tf record.\n",
    "        else:\n",
    "            problem_str = '_'.join(sorted(problem_list))\n",
    "            file_dir = os.path.join(params.tmp_file_dir, problem_str)\n",
    "            if not os.path.exists(file_dir) or replace:\n",
    "                for mode in [TRAIN, EVAL]:\n",
    "\n",
    "                    input_list_dict = {}\n",
    "                    target_list_dict = {}\n",
    "                    label_encoder_dict = {}\n",
    "                    for p_idx, p in enumerate(problem_list):\n",
    "\n",
    "                        res_dict = read_data_fn_dict[p](\n",
    "                            params=params, mode=mode, get_data_num=False, write_tfrecord=False)\n",
    "                        if p_idx == 0:\n",
    "                            tokenizer = res_dict['tokenizer']\n",
    "\n",
    "                        input_list_dict[p] = res_dict['inputs_list']\n",
    "                        target_list_dict[p] = res_dict['target_list']\n",
    "                        label_encoder_dict[p] = res_dict['label_encoder']\n",
    "\n",
    "                    write_single_problem_chunk_tfrecord(\n",
    "                        problem=problem_list,\n",
    "                        inputs_list=input_list_dict,\n",
    "                        target_list=target_list_dict,\n",
    "                        label_encoder=label_encoder_dict,\n",
    "                        params=params,\n",
    "                        tokenizer=tokenizer,\n",
    "                        mode=mode\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NFO:tensorflow:image_input: [[3.28587825e-01 8.58212792e-01 6.34703711e-01 5.66090819e-01\n",
      "  8.68296057e-01 8.87346043e-01 9.77046121e-02 1.37516942e-01\n",
      "  5.90684283e-01 2.61914664e-01]\n",
      " [8.75170300e-01 1.42589392e-01 8.85425846e\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: ['�', '�', '就', '爱', '这', '个', '牌', '子', '的', '糖', '果']\n",
      "INFO:tensorflow:image: [[0.85081319 0.41690878 0.47734927 0.04293305 0.7326147  0.13276684\n",
      "  0.51812267 0.81706754 0.01828652 0.39328008]\n",
      " [0.13407091 0.6471253  0.60215649 0.49953116 0.89615452 0.87681191\n",
      "  0.75876144 0.63\n",
      "INFO:tensorflow:input_ids: [101, 2218, 4263, 6821, 702, 4277, 2094, 4638, 5131, 3362, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:weibo_fake_multi_cls_label_ids: [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:image_input: [[0.85081319 0.41690878 0.47734927 0.04293305 0.7326147  0.13276684\n",
      "  0.51812267 0.81706754 0.01828652 0.39328008]\n",
      " [0.13407091 0.6471253  0.60215649 0.49953116 0.89615452 0.87681191\n",
      "  0.75876144 0.63\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: ['延', '参', '法', '师', '品', '味', '人', '生', '如', '同', '走', '进', '一', '片', '山', '水', '，', '静', '静', '的', '呼', '吸', '，', '安', '静', '的', '欣', '赏', '，', '这', '就', '是', '生', '活', '。']\n",
      "INFO:tensorflow:image: [[0.56948246 0.59080707 0.20481084 0.23073657 0.44994621 0.26748053\n",
      "  0.99031511 0.47271069 0.72776571 0.41165349]\n",
      " [0.12807996 0.74033266 0.49254179 0.80222771 0.62185814 0.60058085\n",
      "  0.29614714 0.43\n",
      "INFO:tensorflow:input_ids: [101, 2454, 1346, 3791, 2360, 1501, 1456, 782, 4495, 1963, 1398, 6624, 6822, 671, 4275, 2255, 3717, 8024, 7474, 7474, 4638, 1461, 1429, 8024, 2128, 7474, 4638, 3615, 6605, 8024, 6821, 2218, 3221, 4495\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:weibo_fake_multi_cls_label_ids: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:image_input: [[0.56948246 0.59080707 0.20481084 0.23073657 0.44994621 0.26748053\n",
      "  0.99031511 0.47271069 0.72776571 0.41165349]\n",
      " [0.12807996 0.74033266 0.49254179 0.80222771 0.62185814 0.60058085\n",
      "  0.29614714 0.43\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:['对', '，', '输', '给', '一', '个', '女', '人', '，', '的', '成', '绩', '。', '失', '望']\n",
      "INFO:tensorflow:input_ids: [101, 2190, 8024, 6783, 5314, 671, 702, 1957, 103, 8024, 4638, 2768, 5327, 511, 103, 3307, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [8, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [ 782 1927    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['今', '天', '下', '午', '起', '来', '看', '到', '外', '面', '的', '太', '阳', '。', '。', '。', '。', '我', '第', '一', '反', '应', '竟', '然', '是', '强', '烈', '的', '想', '回', '家', '泪', '想', '我', '们', '一', '起', '在', '嘉', '鱼',\n",
      "INFO:tensorflow:input_ids: [101, 791, 1921, 678, 103, 6629, 3341, 4692, 1168, 1912, 7481, 4638, 103, 7345, 511, 511, 511, 511, 2769, 5018, 671, 103, 2418, 103, 4197, 3221, 2487, 4164, 4638, 2682, 1726, 2157, 3801, 2682, 2769, 8\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [4, 12, 21, 23, 39, 42, 43, 48, 53, 55, 67, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1286 1922 1353 4994 1649 3198  952  511 1914 6413 4607    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['今', '年', '拜', '年', '不', '短', '信', '，', '就', '在', '微', '博', '拜', '大', '年', '寻', '龙', '记']\n",
      "INFO:tensorflow:input_ids: [101, 791, 2399, 103, 103, 679, 4764, 928, 8024, 103, 1762, 2544, 1300, 2876, 1920, 2399, 2192, 7987, 6381, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [3, 4, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [2876 2399 2218    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['浑', '身', '酸', '疼', '，', '两', '腿', '无', '力', '，', '眼', '神', '呆', '滞', '，', '怎', '么', '了']\n",
      "INFO:tensorflow:input_ids: [101, 3847, 6716, 7000, 4563, 8024, 697, 5597, 3187, 1213, 8024, 4706, 4868, 103, 4005, 103, 103, 720, 749, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [13, 15, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1438 8024 2582    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['明', '显', '紧', '张', '状', '态', '没', '出', '来', '，', '失', '误', '多', '。']\n",
      "INFO:tensorflow:input_ids: [101, 3209, 3227, 5165, 2476, 4307, 2578, 3766, 1139, 3341, 8024, 1927, 103, 1914, 511, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [6428    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['三', '十', '年', '前', '，', '老', '爹', '带', '我', '来', '这', '里', '开', '会', '，', '今', '天', '打', '了', '个', '颠', '倒', '。', '我', '在', '这', '里']\n",
      "INFO:tensorflow:input_ids: [101, 676, 1282, 2399, 1184, 8024, 5439, 4269, 2372, 2769, 3341, 6821, 7027, 2458, 833, 8024, 791, 1921, 2802, 749, 702, 7585, 103, 511, 2769, 1762, 6821, 103, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [22, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [ 948 7027    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['好', '活', '动', '呀', '，', '给', '力', '的', '商', '家', '，', '必', '须', '支', '持', '，', '希', '望', '以', '后', '多', '多', '办', '活', '动', '跟', '我', '们', '互', '动', '哈', '巧', '慧', '铣']\n",
      "INFO:tensorflow:input_ids: [101, 1962, 3833, 103, 1435, 8024, 103, 1213, 4638, 1555, 2157, 8024, 2553, 103, 103, 2898, 8024, 2361, 3307, 809, 103, 1914, 1914, 1215, 3833, 1220, 103, 2769, 812, 757, 1220, 1506, 103, 103, 7203, 1\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [3, 6, 13, 14, 20, 26, 32, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1220 5314 7557 3118 1400 6656 2341 2716    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['听', '说', '小', '米', '开', '卖', '了', '，', '刚', '刚', '预', '约', '了', '台', '。', '我', '爱', '小', '米', '手', '机', '因', '为', '他', '是', '迄', '今', '为', '止', '最', '快', '的', '小', '米', '手', '机', '月', '日', '中', '午',\n",
      "INFO:tensorflow:input_ids: [101, 1420, 6432, 103, 5101, 103, 1297, 749, 8024, 1157, 1157, 7564, 103, 749, 1378, 511, 2769, 103, 2207, 5101, 2797, 3322, 1728, 711, 800, 3221, 6812, 791, 711, 3632, 3297, 103, 4638, 2207, 5101, 27\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [3, 5, 12, 17, 31, 41, 48, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [2207 2458 5276 4263 2571 4157 2843    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['最', '热', '时', '尚', '榜', '女', '人', '不', '坏', '，', '男', '人', '不', '爱', '，', '一', '个', '男', '女', '必', '看', '的', '微', '博', '花', '心']\n",
      "INFO:tensorflow:input_ids: [101, 3297, 4178, 103, 2213, 3528, 1957, 782, 679, 1776, 8024, 4511, 782, 679, 4263, 8024, 103, 702, 4511, 1957, 2553, 4692, 4638, 103, 1300, 5709, 2552, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [3, 16, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3198  671 2544    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['回', '复', '支', '持', '，', '赞', '成', '，', '哈', '哈', '米', '八', '吴', '够', '历', '史', '要', '的', '陈', '小', '奥', '丁', '丁', '我', '爱', '小', '肥', '肥', '一', '族', '大', '头', '仔', '大', '家', '团', '结', '一', '致', '，',\n",
      "INFO:tensorflow:input_ids: [101, 1726, 1908, 3118, 2898, 8024, 6614, 2768, 8024, 103, 1506, 5101, 103, 1426, 1916, 1325, 1380, 6206, 4638, 7357, 2207, 1952, 672, 103, 2769, 4263, 2207, 5503, 5503, 671, 103, 1920, 1928, 103, 192\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [9, 12, 23, 30, 33, 36, 45, 48, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1506 1061  672 3184  798 1730 3968 6983 7030    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['剑', '网', '乱', '世', '长', '安', '公', '测', '盛', '典', '今', '日', '开', '启', '，', '海', '量', '豪', '礼', '火', '爆', '开', '送', '精', '美', '挂', '件', '、', '听', '雨', '·', '汉', '服', '娃', '娃', '、', '诙', '谐', '双', '骑',\n",
      "INFO:tensorflow:input_ids: [101, 1187, 5381, 744, 686, 7270, 2128, 1062, 103, 4670, 103, 791, 3189, 2458, 1423, 103, 3862, 7030, 6498, 4851, 4125, 4255, 2458, 6843, 5125, 5401, 103, 816, 510, 1420, 103, 185, 3727, 3302, 2015, 2\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [8, 10, 15, 26, 30, 39, 47, 51, 53, 68, 69, 75, 80, 85, 87, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3844 1073 8024 2899 7433 1352 2897 1283 5273 3833 1220  782 2135 5381\n",
      " 2787    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
      "INFO:tensorflow:['在', '街', '上', '听', '见', '音', '乐', '我', '舞', '动', '起', '来', '很', '丢', '人', '？', '真', '的', '很', '丢', '人', '吗', '？']\n",
      "INFO:tensorflow:input_ids: [101, 1762, 6125, 677, 1420, 6224, 7509, 727, 103, 5659, 103, 6629, 103, 103, 696, 782, 103, 4696, 4638, 103, 696, 782, 1408, 8043, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [8, 10, 12, 13, 16, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [2769 1220 3341 2523 8043 2523    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['三', '毛', '说', '我', '唯', '一', '锲', '而', '不', '舍', '，', '愿', '意', '以', '自', '己', '的', '生', '命', '去', '努', '力', '的', '，', '只', '不', '过', '是', '保', '守', '我', '个', '人', '的', '心', '怀', '意', '念', '，', '在',\n",
      "INFO:tensorflow:input_ids: [101, 676, 3688, 6432, 2769, 1546, 671, 7244, 5445, 679, 5650, 103, 2703, 2692, 809, 5632, 2346, 4638, 4495, 1462, 1343, 1222, 103, 4638, 8024, 1372, 679, 6814, 103, 924, 2127, 2769, 702, 103, 4638, 2\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [11, 22, 28, 33, 38, 40, 42, 43, 59, 63, 73, 78, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [8024 1213 3221  782 2573 1762 3300 4495 4495 4263 4958 7361    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['星', '期', '天', '的', '早', '晨', '七', '点', '学', '车', '，', '驾', '校', '太', '给', '力', '。', '我', '的', '头', '发', '都', '没', '有', '洗']\n",
      "INFO:tensorflow:input_ids: [101, 103, 3309, 103, 103, 3193, 3247, 673, 4157, 2110, 6756, 8024, 7730, 103, 1922, 5314, 103, 511, 2769, 4638, 103, 1355, 6963, 3766, 103, 103, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [1, 3, 4, 13, 16, 20, 24, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3215 1921 4638 3413 1213 1928 3300 3819    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['发', '表', '了', '博', '文', '小', '学', '美', '术', '新', '课', '标', '的', '反', '思', '学', '习', '新', '学', '期', '开', '学', '有', '两', '个', '多', '月', '了', '，', '在', '这', '段', '时', '间', '的', '教', '学', '计', '划', '、',\n",
      "INFO:tensorflow:input_ids: [101, 1355, 6134, 749, 1300, 103, 2207, 2110, 103, 3318, 3173, 6440, 3403, 4638, 1353, 2590, 2110, 739, 3173, 2110, 3309, 2458, 103, 3300, 697, 702, 1914, 3299, 749, 8024, 1762, 6821, 3667, 3198, 7313\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [5, 8, 22, 38, 42, 45, 48, 53, 54, 73, 74, 76, 87, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3152 5401 2110 6369 3428 1091  704 2207 2110 4385 1762 4500 4294    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['哈', '哈', '哈', '哈', '哈', '哈', '镰', '刀', '刮', '腋', '毛', '哈', '哈', '哈', '哈', '哈', '哈', '我', '的', '朋', '友', '是', '个', '呆', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '十', '万', '个',\n",
      "INFO:tensorflow:input_ids: [101, 1506, 1506, 1506, 1506, 1506, 1506, 7266, 103, 1167, 5573, 3688, 1506, 1506, 1506, 1506, 1506, 1506, 103, 4638, 3301, 1351, 3221, 702, 1438, 1506, 1506, 1506, 1506, 1506, 1506, 1506, 1506, 1506,\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [8, 18, 44, 50, 55, 73, 81, 84, 86, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1143 2769 2135 2105 1506 4331 1506 1506 5529 1506    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['�', '�', '就', '爱', '这', '个', '牌', '子', '的', '糖', '果']\n",
      "INFO:tensorflow:input_ids: [101, 2218, 4263, 6821, 702, 103, 2094, 4638, 5131, 3362, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [4277    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['延', '参', '法', '师', '品', '味', '人', '生', '如', '同', '走', '进', '一', '片', '山', '水', '，', '静', '静', '的', '呼', '吸', '，', '安', '静', '的', '欣', '赏', '，', '这', '就', '是', '生', '活', '。']\n",
      "INFO:tensorflow:input_ids: [101, 2454, 103, 3791, 103, 1501, 1456, 782, 4495, 1963, 103, 6624, 103, 103, 4275, 2255, 3717, 8024, 103, 103, 4638, 103, 1429, 103, 2128, 7474, 4638, 103, 6605, 8024, 6821, 2218, 3221, 103, 3833, 51\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [2, 4, 10, 12, 13, 18, 19, 21, 23, 27, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1346 2360 1398 6822  671 7474 7474 1461 8024 3615 4495    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "test_base.params.assign_problem(\n",
    "    'weibo_ner&weibo_fake_cls|weibo_fake_multi_cls|weibo_masklm')\n",
    "write_tfrecord(\n",
    "    params=test_base.params)\n",
    "assert os.path.exists(os.path.join(\n",
    "    test_base.tmpfiledir, 'weibo_fake_cls_weibo_ner'))\n",
    "assert os.path.exists(os.path.join(\n",
    "    test_base.tmpfiledir, 'weibo_fake_multi_cls'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def reshape_tensors_in_dataset(example):  # pragma: no cover\n",
    "    \"\"\"Reshape serialized tensor back to its original shape\n",
    "\n",
    "    Arguments:\n",
    "        example {Example} -- Example\n",
    "\n",
    "    Returns:\n",
    "        Example -- Example\n",
    "    \"\"\"\n",
    "\n",
    "    for feature_key in example:\n",
    "        example[feature_key] = tf.sparse.to_dense(example[feature_key])\n",
    "\n",
    "    for feature_key in example:\n",
    "        if '_shape' in feature_key:\n",
    "            continue\n",
    "\n",
    "        shape_tensor = example['{}_shape'.format(feature_key)]\n",
    "        example[feature_key] = tf.reshape(example[feature_key], shape_tensor)\n",
    "\n",
    "    for feature_key in list(example.keys()):\n",
    "        if '_shape' in feature_key:\n",
    "            del example[feature_key]\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "def add_loss_multiplier(example, problem):  # pragma: no cover\n",
    "    example['{}_loss_multiplier'.format(problem)] = tf.constant(\n",
    "        value=1, shape=(), dtype=tf.int32)\n",
    "    return example\n",
    "\n",
    "\n",
    "def set_shape_for_dataset(example, feature_desc_dict):  # pragma: no cover\n",
    "    for feature_key in example:\n",
    "        example[feature_key].set_shape(\n",
    "            feature_desc_dict['{}_shape_value'.format(feature_key)])\n",
    "    return example\n",
    "\n",
    "\n",
    "def get_dummy_features(dataset_dict, feature_desc_dict):\n",
    "    \"\"\"Get dummy features.\n",
    "    Dummy features are used to make sure every feature dict\n",
    "    at every iteration has the same keys.\n",
    "\n",
    "    Example:\n",
    "        problem A: {'input_ids': [1,2,3], 'A_label_ids': 1}\n",
    "        problem B: {'input_ids': [1,2,3], 'B_label_ids': 2}\n",
    "\n",
    "    Then dummy features:\n",
    "        {'A_label_ids': 0, 'B_label_ids': 0}\n",
    "\n",
    "    At each iteration, we sample a problem, let's say we sampled A\n",
    "    Then:\n",
    "        feature dict without dummy:\n",
    "            {'input_ids': [1,2,3], 'A_label_ids': 1}\n",
    "        feature dict with dummy:\n",
    "            {'input_ids': [1,2,3], 'A_label_ids': 1, 'B_label_ids':0}\n",
    "\n",
    "    Arguments:\n",
    "        dataset_dict {dict} -- dict of datasets of all problems\n",
    "\n",
    "    Returns:\n",
    "        dummy_features -- dict of dummy tensors\n",
    "    \"\"\"\n",
    "\n",
    "    feature_keys = [list(d.element_spec.keys())\n",
    "                    for _, d in dataset_dict.items()]\n",
    "    common_features_accross_problems = set(\n",
    "        feature_keys[0]).intersection(*feature_keys[1:])\n",
    "\n",
    "    dummy_features = {}\n",
    "    for problem, problem_dataset in dataset_dict.items():\n",
    "        output_types = {k: v.dtype for k,\n",
    "                        v in problem_dataset.element_spec.items()}\n",
    "        dummy_features.update({k: tf.cast(tf.constant(shape=[1 if s is None else s for s in feature_desc_dict.get('{}_shape_value'.format(k), [])], value=0), v)\n",
    "                               for k, v in output_types.items()\n",
    "                               if k not in common_features_accross_problems})\n",
    "\n",
    "    return dummy_features\n",
    "\n",
    "\n",
    "def add_dummy_features_to_dataset(example, dummy_features):  # pragma: no cover\n",
    "    \"\"\"Add dummy features to dataset\n",
    "\n",
    "    feature dict without dummy:\n",
    "        {'input_ids': [1,2,3], 'A_label_ids': 1}\n",
    "    feature dict with dummy:\n",
    "        {'input_ids': [1,2,3], 'A_label_ids': 1, 'B_label_ids':0}\n",
    "\n",
    "    Arguments:\n",
    "        example {data example} -- dataset example\n",
    "        dummy_features {dict} -- dict of dummy tensors\n",
    "    \"\"\"\n",
    "    for feature_name in dummy_features:\n",
    "        if feature_name not in example:\n",
    "            example[feature_name] = tf.identity(dummy_features[feature_name])\n",
    "    return example\n",
    "\n",
    "\n",
    "def read_tfrecord(params: BaseParams, mode: str):\n",
    "    \"\"\"Read and parse TFRecord for every problem\n",
    "\n",
    "    The returned dataset is parsed, reshaped, to_dense tensors\n",
    "    with dummy features.\n",
    "\n",
    "    Arguments:\n",
    "        params {params} -- params\n",
    "        mode {str} -- mode, train, eval or predict\n",
    "\n",
    "    Returns:\n",
    "        dict -- dict with keys: problem name, values: dataset\n",
    "    \"\"\"\n",
    "    dataset_dict = {}\n",
    "    all_feature_desc_dict = {}\n",
    "    for problem_list in params.problem_chunk:\n",
    "        problem = '_'.join(sorted(problem_list))\n",
    "        file_dir = os.path.join(params.tmp_file_dir, problem)\n",
    "        tfrecord_path_list = glob(os.path.join(\n",
    "            file_dir, '{}_*.tfrecord'.format(mode)))\n",
    "        feature_desc_dict = json.load(\n",
    "            open(os.path.join(file_dir, '{}_feature_desc.json'.format(mode))))\n",
    "        all_feature_desc_dict.update(feature_desc_dict)\n",
    "        feature_desc = make_feature_desc(feature_desc_dict)\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            tfrecord_path_list, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.map(lambda x: tf.io.parse_single_example(\n",
    "            serialized=x, features=feature_desc), num_parallel_calls=tf.data.experimental.AUTOTUNE).map(\n",
    "                reshape_tensors_in_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.map(\n",
    "            lambda x: set_shape_for_dataset(x, feature_desc_dict),\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "        for p in problem_list:\n",
    "            dataset = dataset.map(lambda x: add_loss_multiplier(x, p),\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        dataset_dict[problem] = dataset\n",
    "\n",
    "    # add dummy features\n",
    "    dummy_features = get_dummy_features(dataset_dict, all_feature_desc_dict)\n",
    "    for problem in params.get_problem_chunk(as_str=True):\n",
    "        dataset_dict[problem] = dataset_dict[problem].map(\n",
    "            lambda x: add_dummy_features_to_dataset(x, dummy_features),\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:bert_config not exists. will load model from huggingface checkpoint.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "test_base.params.assign_problem(\n",
    "    'weibo_ner&weibo_fake_cls|weibo_fake_multi_cls|weibo_masklm')\n",
    "write_tfrecord(\n",
    "    params=test_base.params, replace=False)\n",
    "dataset_dict = read_tfrecord(\n",
    "    params=test_base.params, mode='train')\n",
    "dataset: tf.data.Dataset = dataset_dict['weibo_fake_cls_weibo_ner']\n",
    "assert sorted(list(dataset.element_spec.keys())) == ['image_input',\n",
    "                    'image_mask',\n",
    "                    'image_segment_ids',\n",
    "                    'input_ids',\n",
    "                    'input_mask',\n",
    "                    'masked_lm_ids',\n",
    "                    'masked_lm_positions',\n",
    "                    'masked_lm_weights',\n",
    "                    'segment_ids',\n",
    "                    'weibo_fake_cls_label_ids',\n",
    "                    'weibo_fake_cls_loss_multiplier',\n",
    "                    'weibo_fake_multi_cls_label_ids',\n",
    "                    'weibo_fake_multi_cls_loss_multiplier',\n",
    "                    'weibo_masklm_loss_multiplier',\n",
    "                    'weibo_ner_label_ids',\n",
    "                    'weibo_ner_loss_multiplier']\n",
    "# make sure loss multiplier is correct\n",
    "ele = next(dataset.as_numpy_iterator())\n",
    "assert ele['weibo_fake_cls_loss_multiplier'] == 1\n",
    "assert ele['weibo_ner_loss_multiplier'] == 1\n",
    "assert ele['weibo_fake_multi_cls_loss_multiplier'] == 0\n",
    "\n",
    "# multimodal dataset\n",
    "dataset: tf.data.Dataset = dataset_dict['weibo_fake_multi_cls']\n",
    "_ = next(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
