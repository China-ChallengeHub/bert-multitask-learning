{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp run_bert_multitask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Bert Multitask Learning\n",
    "\n",
    "Train, eval and predict api for bert multitask learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Callable\n",
    "from shutil import copytree, ignore_patterns, rmtree\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.errors_impl import NotFoundError as TFNotFoundError\n",
    "\n",
    "from bert_multitask_learning.input_fn import predict_input_fn, train_eval_input_fn\n",
    "from bert_multitask_learning.model_fn import BertMultiTask\n",
    "from bert_multitask_learning.params import DynamicBatchSizeParams, BaseParams\n",
    "from bert_multitask_learning.special_tokens import EVAL\n",
    "\n",
    "# Fix duplicate log\n",
    "LOGGER = tf.get_logger()\n",
    "LOGGER.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def create_keras_model(\n",
    "        mirrored_strategy: tf.distribute.MirroredStrategy,\n",
    "        params: BaseParams,\n",
    "        mode='train',\n",
    "        inputs_to_build_model=None,\n",
    "        model=None):\n",
    "    \"\"\"init model in various mode\n",
    "\n",
    "    train: model will be loaded from huggingface\n",
    "    resume: model will be loaded from params.ckpt_dir, if params.ckpt_dir dose not contain valid checkpoint, then load from huggingface\n",
    "    transfer: model will be loaded from params.init_checkpoint, the correspongding path should contain checkpoints saved using bert-multitask-learning\n",
    "    predict: model will be loaded from params.ckpt_dir except optimizers' states\n",
    "    eval: model will be loaded from params.ckpt_dir except optimizers' states, model will be compiled\n",
    "\n",
    "    Args:\n",
    "        mirrored_strategy (tf.distribute.MirroredStrategy): mirrored strategy\n",
    "        params (BaseParams): params\n",
    "        mode (str, optional): Mode, see above explaination. Defaults to 'train'.\n",
    "        inputs_to_build_model (Dict, optional): A batch of data. Defaults to None.\n",
    "        model (Model, optional): Keras model. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        model: loaded model\n",
    "    \"\"\"\n",
    "   \n",
    "    def _get_model_wrapper(params, mode, inputs_to_build_model, model):\n",
    "        if model is None:\n",
    "            model = BertMultiTask(params)\n",
    "            # model.run_eagerly = True\n",
    "        if mode == 'resume':\n",
    "            model.compile()\n",
    "            # build training graph\n",
    "            # model.train_step(inputs_to_build_model)\n",
    "            _ = model(inputs_to_build_model,\n",
    "                      mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            # load ALL vars including optimizers' states\n",
    "            try:\n",
    "                model.load_weights(os.path.join(\n",
    "                    params.ckpt_dir, 'model'), skip_mismatch=False)\n",
    "            except TFNotFoundError:\n",
    "                LOGGER.warn('Not resuming since no mathcing ckpt found')\n",
    "        elif mode == 'transfer':\n",
    "            # build graph without optimizers' states\n",
    "            # calling compile again should reset optimizers' states but we're playing safe here\n",
    "            _ = model(inputs_to_build_model,\n",
    "                      mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            # load weights without loading optimizers' vars\n",
    "            model.load_weights(os.path.join(params.init_checkpoint, 'model'))\n",
    "            # compile again\n",
    "            model.compile()\n",
    "        elif mode == 'predict':\n",
    "            _ = model(inputs_to_build_model,\n",
    "                      mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            # load weights without loading optimizers' vars\n",
    "            model.load_weights(os.path.join(params.ckpt_dir, 'model'))\n",
    "        elif mode == 'eval':\n",
    "            _ = model(inputs_to_build_model,\n",
    "                      mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            # load weights without loading optimizers' vars\n",
    "            model.load_weights(os.path.join(params.ckpt_dir, 'model'))\n",
    "            model.compile()\n",
    "        else:\n",
    "            model.compile()\n",
    "\n",
    "        return model\n",
    "    if mirrored_strategy is not None:\n",
    "         with mirrored_strategy.scope():\n",
    "             model = _get_model_wrapper(params, mode, inputs_to_build_model, model)\n",
    "    else:\n",
    "        model = _get_model_wrapper(params, mode, inputs_to_build_model, model)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _train_bert_multitask_keras_model(train_dataset: tf.data.Dataset,\n",
    "                                      eval_dataset: tf.data.Dataset,\n",
    "                                      model: tf.keras.Model,\n",
    "                                      params: BaseParams,\n",
    "                                      mirrored_strategy: tf.distribute.MirroredStrategy = None):\n",
    "    # can't save whole model with model subclassing api due to tf bug\n",
    "    # see: https://github.com/tensorflow/tensorflow/issues/42741\n",
    "    # https://github.com/tensorflow/tensorflow/issues/40366\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(params.ckpt_dir, 'model'),\n",
    "        save_weights_only=True,\n",
    "        monitor='val_mean_acc',\n",
    "        mode='auto',\n",
    "        save_best_only=False)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=params.ckpt_dir)\n",
    "    if mirrored_strategy is not None:\n",
    "        with mirrored_strategy.scope():\n",
    "            model.fit(\n",
    "                x=train_dataset,\n",
    "                validation_data=eval_dataset,\n",
    "                epochs=params.train_epoch,\n",
    "                callbacks=[model_checkpoint_callback, tensorboard_callback],\n",
    "                steps_per_epoch=params.train_steps_per_epoch\n",
    "            )\n",
    "    else:\n",
    "        model.fit(\n",
    "            x=train_dataset,\n",
    "            validation_data=eval_dataset,\n",
    "            epochs=params.train_epoch,\n",
    "            callbacks=[model_checkpoint_callback, tensorboard_callback],\n",
    "            steps_per_epoch=params.train_steps_per_epoch\n",
    "        )\n",
    "    model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_params_ready(problem, num_gpus, model_dir, params, problem_type_dict, processing_fn_dict, mode='train', json_path=''):\n",
    "    if params is None:\n",
    "        params = DynamicBatchSizeParams()\n",
    "    if not os.path.exists('models'):\n",
    "        os.mkdir('models')\n",
    "    if model_dir:\n",
    "        base_dir, dir_name = os.path.split(model_dir)\n",
    "    else:\n",
    "        base_dir, dir_name = None, None\n",
    "    # add new problem to params if problem_type_dict and processing_fn_dict provided\n",
    "    if problem_type_dict:\n",
    "        params.add_multiple_problems(\n",
    "            problem_type_dict=problem_type_dict, processing_fn_dict=processing_fn_dict)\n",
    "    if mode == 'train':\n",
    "        params.assign_problem(problem, gpu=int(num_gpus),\n",
    "                              base_dir=base_dir, dir_name=dir_name)\n",
    "        params.to_json()\n",
    "    else:\n",
    "        params.from_json(json_path)\n",
    "        params.assign_problem(problem, gpu=int(num_gpus),\n",
    "                              base_dir=base_dir, dir_name=dir_name)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_bert_multitask(\n",
    "        problem='weibo_ner',\n",
    "        num_gpus=1,\n",
    "        num_epochs=10,\n",
    "        model_dir='',\n",
    "        params: BaseParams = None,\n",
    "        problem_type_dict: Dict[str, str] = None,\n",
    "        processing_fn_dict: Dict[str, Callable] = None,\n",
    "        model: tf.keras.Model = None,\n",
    "        create_tf_record_only=False,\n",
    "        steps_per_epoch=None,\n",
    "        warmup_ratio=0.1,\n",
    "        continue_training=False,\n",
    "        mirrored_strategy=None):\n",
    "    \"\"\"Train Multi-task Bert model\n",
    "\n",
    "    About problem:\n",
    "        There are two types of chaining operations can be used to chain problems.\n",
    "            - `&`. If two problems have the same inputs, they can be chained using `&`.\n",
    "                Problems chained by `&` will be trained at the same time.\n",
    "            - `|`. If two problems don't have the same inputs, they need to be chained using `|`.\n",
    "                Problems chained by `|` will be sampled to train at every instance.\n",
    "\n",
    "        For example, `cws|NER|weibo_ner&weibo_cws`, one problem will be sampled at each turn, say `weibo_ner&weibo_cws`, then `weibo_ner` and `weibo_cws` will trained for this turn together. Therefore, in a particular batch, some tasks might not be sampled, and their loss could be 0 in this batch.\n",
    "\n",
    "    About problem_type_dict and processing_fn_dict:\n",
    "        If the problem is not predefined, you need to tell the model what's the new problem's problem_type\n",
    "        and preprocessing function.\n",
    "            For example, a new problem: fake_classification\n",
    "            problem_type_dict = {'fake_classification': 'cls'}\n",
    "            processing_fn_dict = {'fake_classification': lambda: return ...}\n",
    "\n",
    "        Available problem type:\n",
    "            cls: Classification\n",
    "            seq_tag: Sequence Labeling\n",
    "            seq2seq_tag: Sequence to Sequence tag problem\n",
    "            seq2seq_text: Sequence to Sequence text generation problem\n",
    "\n",
    "        Preprocessing function example:\n",
    "        Please refer to https://github.com/JayYip/bert-multitask-learning/blob/master/README.md\n",
    "\n",
    "    Keyword Arguments:\n",
    "        problem {str} -- Problems to train (default: {'weibo_ner'})\n",
    "        num_gpus {int} -- Number of GPU to use (default: {1})\n",
    "        num_epochs {int} -- Number of epochs to train (default: {10})\n",
    "        model_dir {str} -- model dir (default: {''})\n",
    "        params {BaseParams} -- Params to define training and models (default: {DynamicBatchSizeParams()})\n",
    "        problem_type_dict {dict} -- Key: problem name, value: problem type (default: {{}})\n",
    "        processing_fn_dict {dict} -- Key: problem name, value: problem data preprocessing fn (default: {{}})\n",
    "    \"\"\"\n",
    "    params.train_epoch = num_epochs\n",
    "    params = get_params_ready(problem, num_gpus, model_dir,\n",
    "                              params, problem_type_dict, processing_fn_dict)\n",
    "\n",
    "    train_dataset = train_eval_input_fn(params)\n",
    "    eval_dataset = train_eval_input_fn(params, mode=EVAL)\n",
    "    if create_tf_record_only:\n",
    "        return\n",
    "\n",
    "    # get train_steps and update params\n",
    "    if steps_per_epoch is not None:\n",
    "        train_steps = steps_per_epoch\n",
    "    else:\n",
    "        train_steps = 0\n",
    "        for _ in train_dataset:\n",
    "            train_steps += 1\n",
    "    params.update_train_steps(train_steps, warmup_ratio=warmup_ratio)\n",
    "    \n",
    "    train_dataset = train_eval_input_fn(params)\n",
    "    train_dataset = train_dataset.repeat(10)\n",
    "\n",
    "    one_batch = next(train_dataset.as_numpy_iterator())\n",
    "\n",
    "    if mirrored_strategy is None:\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    elif mirrored_strategy is False:\n",
    "        mirrored_strategy = None\n",
    "\n",
    "    if num_gpus > 1 and mirrored_strategy is not False:\n",
    "        train_dataset = mirrored_strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        eval_dataset = mirrored_strategy.experimental_distribute_dataset(\n",
    "            eval_dataset)\n",
    "\n",
    "    # restore priority: self > transfer > huggingface\n",
    "    if continue_training and tf.train.latest_checkpoint(params.ckpt_dir):\n",
    "        mode = 'resume'\n",
    "    elif tf.train.latest_checkpoint(params.init_checkpoint):\n",
    "        mode = 'transfer'\n",
    "    else:\n",
    "        mode = 'train'\n",
    "\n",
    "    model = create_keras_model(\n",
    "        mirrored_strategy=mirrored_strategy, params=params, mode=mode, inputs_to_build_model=one_batch)\n",
    "\n",
    "    _train_bert_multitask_keras_model(\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        model=model,\n",
    "        params=params,\n",
    "        mirrored_strategy=mirrored_strategy\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multitask\n",
    "\n",
    "Before training, we need to do the following things:\n",
    "- pass transformers corresponding configuration to params, we use `voidful/albert_chinese_tiny` as example here\n",
    "- configure the problems we want to train, which includes\n",
    "    - training problems\n",
    "    - their problem type as a dict\n",
    "    - their preprocessing functions as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from bert_multitask_learning.predefined_problems import *\n",
    "\n",
    "from bert_multitask_learning import DynamicBatchSizeParams\n",
    "import os\n",
    "from bert_multitask_learning import predict_input_fn\n",
    "params = DynamicBatchSizeParams()\n",
    "params.shuffle_buffer = 1000\n",
    "\n",
    "# configure transformers\n",
    "params.transformer_tokenizer_loading = 'BertTokenizer'\n",
    "params.transformer_model_loading = 'AlbertForMaskedLM'\n",
    "params.transformer_config_loading = 'AlbertConfig'\n",
    "params.transformer_model_name = 'voidful/albert_chinese_tiny'\n",
    "params.transformer_config_name = 'voidful/albert_chinese_tiny'\n",
    "params.transformer_tokenizer_name = 'voidful/albert_chinese_tiny'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import tempfile\n",
    "params.tmp_file_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "problem = 'weibo_fake_ner&weibo_fake_cls|weibo_fake_multi_cls|weibo_masklm|weibo_pretrain'\n",
    "problem_type_dict = {\n",
    "    'weibo_fake_ner': 'seq_tag',\n",
    "    'weibo_cws': 'seq_tag',\n",
    "    'weibo_fake_multi_cls': 'multi_cls',\n",
    "    'weibo_fake_cls': 'cls',\n",
    "    'weibo_masklm': 'masklm',\n",
    "    'weibo_pretrain': 'pretrain'\n",
    "}\n",
    "\n",
    "processing_fn_dict = {\n",
    "    'weibo_fake_ner': get_weibo_fake_ner_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_cws': get_weibo_cws_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_fake_cls': get_weibo_fake_cls_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_fake_multi_cls': get_weibo_fake_multi_cls_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_masklm': get_weibo_masklm(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_pretrain': get_weibo_pretrain_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "749, 8024, 1157, 1157, 7564, 5276, 749, 1378, 103, 103, 4263, 2207, 5101, 2797, 3322, 1728, 711, 800, 3221, 6812, 791, 711, 103, 3297, 2571, 4638, 2207, 5101, \n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [3, 15, 16, 29, 39, 40, 43, 53, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [2207  511 2769 3632  704 1286 2361 2797    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['最', '热', '时', '尚', '榜', '女', '人', '不', '坏', '，', '男', '人', '不', '爱', '，', '一', '个', '男', '女', '必', '看', '的', '微', '博', '花', '心']\n",
      "INFO:tensorflow:input_ids: [101, 3297, 4178, 103, 2213, 3528, 1957, 782, 679, 1776, 8024, 4511, 782, 679, 4263, 8024, 671, 702, 103, 1957, 103, 4692, 4638, 2544, 1300, 5709, 103, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [3, 18, 20, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3198 4511 2553 2552    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['回', '复', '支', '持', '，', '赞', '成', '，', '哈', '哈', '米', '八', '吴', '够', '历', '史', '要', '的', '陈', '小', '奥', '丁', '丁', '我', '爱', '小', '肥', '肥', '一', '族', '大', '头', '仔', '大', '家', '团', '结', '一', '致', '，',\n",
      "INFO:tensorflow:input_ids: [101, 103, 103, 3118, 2898, 8024, 6614, 2768, 8024, 1506, 1506, 5101, 1061, 1426, 1916, 103, 103, 6206, 4638, 7357, 2207, 1952, 672, 672, 2769, 4263, 2207, 5503, 5503, 103, 3184, 1920, 103, 798, 103, \n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [1, 2, 15, 16, 29, 32, 34, 40, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1726 1908 1325 1380  671 1928 1920 8024 7030    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['剑', '网', '乱', '世', '长', '安', '公', '测', '盛', '典', '今', '日', '开', '启', '，', '海', '量', '豪', '礼', '火', '爆', '开', '送', '精', '美', '挂', '件', '、', '听', '雨', '·', '汉', '服', '娃', '娃', '、', '诙', '谐', '双', '骑',\n",
      "INFO:tensorflow:input_ids: [101, 1187, 5381, 744, 686, 7270, 2128, 1062, 3844, 4670, 1073, 103, 3189, 2458, 1423, 8024, 3862, 7030, 6498, 4851, 4125, 4255, 2458, 103, 5125, 5401, 2899, 816, 510, 1420, 7433, 103, 3727, 3302, 201\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [11, 23, 31, 42, 53, 58, 61, 65, 66, 80, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [ 791 6843  185 6762 5273 5023 7028 1355 1315 2135    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['在', '街', '上', '听', '见', '音', '乐', '我', '舞', '动', '起', '来', '很', '丢', '人', '？', '真', '的', '很', '丢', '人', '吗', '？']\n",
      "INFO:tensorflow:input_ids: [101, 1762, 6125, 677, 1420, 103, 7509, 727, 2769, 5659, 1220, 6629, 3341, 2523, 103, 782, 8043, 4696, 4638, 2523, 696, 103, 1408, 103, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [5, 14, 21, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [6224  696  782 8043    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['三', '毛', '说', '我', '唯', '一', '锲', '而', '不', '舍', '，', '愿', '意', '以', '自', '己', '的', '生', '命', '去', '努', '力', '的', '，', '只', '不', '过', '是', '保', '守', '我', '个', '人', '的', '心', '怀', '意', '念', '，', '在',\n",
      "INFO:tensorflow:input_ids: [101, 676, 3688, 6432, 2769, 1546, 671, 7244, 5445, 679, 5650, 8024, 2703, 2692, 809, 5632, 2346, 4638, 4495, 1462, 1343, 1222, 1213, 4638, 8024, 1372, 679, 6814, 3221, 924, 103, 2769, 103, 782, 4638,\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [30, 32, 35, 36, 39, 40, 54, 68, 69, 78, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [2127  702 2552 2577 8024 1762 8024 1762 3300 7361    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['星', '期', '天', '的', '早', '晨', '七', '点', '学', '车', '，', '驾', '校', '太', '给', '力', '。', '我', '的', '头', '发', '都', '没', '有', '洗']\n",
      "INFO:tensorflow:input_ids: [101, 3215, 3309, 1921, 4638, 3193, 3247, 673, 4157, 2110, 6756, 8024, 7730, 3413, 1922, 5314, 103, 511, 2769, 4638, 1928, 1355, 6963, 3766, 3300, 103, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [16, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1213 3819    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['发', '表', '了', '博', '文', '小', '学', '美', '术', '新', '课', '标', '的', '反', '思', '学', '习', '新', '学', '期', '开', '学', '有', '两', '个', '多', '月', '了', '，', '在', '这', '段', '时', '间', '的', '教', '学', '计', '划', '、',\n",
      "INFO:tensorflow:input_ids: [101, 1355, 6134, 749, 1300, 3152, 2207, 2110, 5401, 3318, 3173, 6440, 3403, 4638, 1353, 2590, 2110, 739, 3173, 2110, 103, 2458, 2110, 103, 697, 103, 103, 3299, 749, 103, 1762, 6821, 3667, 3198, 7313,\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [20, 23, 25, 26, 29, 43, 46, 49, 51, 61, 70, 85, 86, 88, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3309 3300  702 1914 8024 4638 2339 8024 2769 1114  511 2552 4415 4157\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['哈', '哈', '哈', '哈', '哈', '哈', '镰', '刀', '刮', '腋', '毛', '哈', '哈', '哈', '哈', '哈', '哈', '我', '的', '朋', '友', '是', '个', '呆', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '十', '万', '个',\n",
      "INFO:tensorflow:input_ids: [101, 1506, 1506, 1506, 1506, 1506, 1506, 7266, 1143, 1167, 5573, 3688, 1506, 1506, 1506, 103, 1506, 1506, 2769, 4638, 3301, 1351, 103, 702, 1438, 1506, 103, 1506, 103, 1506, 1506, 1506, 1506, 1506, 1\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [15, 22, 26, 28, 36, 38, 42, 50, 54, 58, 85, 86, 88, 97, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1506 3221 1506 1506 1506 1282 5010 2105 1506 5857 1506 5529 1506 3392\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['�', '�', '就', '爱', '这', '个', '牌', '子', '的', '糖', '果']\n",
      "INFO:tensorflow:input_ids: [101, 103, 2218, 4263, 6821, 702, 103, 2094, 4638, 5131, 3362, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [4277    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['延', '参', '法', '师', '品', '味', '人', '生', '如', '同', '走', '进', '一', '片', '山', '水', '，', '静', '静', '的', '呼', '吸', '，', '安', '静', '的', '欣', '赏', '，', '这', '就', '是', '生', '活', '。']\n",
      "INFO:tensorflow:input_ids: [101, 103, 1346, 3791, 103, 103, 1456, 782, 4495, 1963, 1398, 6624, 6822, 671, 103, 2255, 3717, 8024, 7474, 7474, 4638, 103, 1429, 8024, 2128, 103, 4638, 103, 6605, 103, 6821, 2218, 3221, 4495, 3833, \n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [1, 4, 5, 14, 21, 25, 27, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [2454 2360 1501 4275 1461 7474 3615 8024    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['就', '感', '恩', '吧', '越', '来', '越', '没', '战', '斗', '力', '了', '。']\n",
      "INFO:tensorflow:input_ids: [101, 2218, 2697, 2617, 103, 6632, 3341, 103, 3766, 103, 3159, 1213, 749, 511, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [4, 7, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1416 6632 2773    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.4\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.2\n",
      "INFO:tensorflow:weibo_masklm: 0.2\n",
      "INFO:tensorflow:weibo_pretrain: 0.2\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.4\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.2\n",
      "INFO:tensorflow:weibo_masklm: 0.2\n",
      "INFO:tensorflow:weibo_pretrain: 0.2\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.4\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.2\n",
      "INFO:tensorflow:weibo_masklm: 0.2\n",
      "INFO:tensorflow:weibo_pretrain: 0.2\n",
      "404 Client Error: Not Found for url: https://huggingface.co/voidful/albert_chinese_tiny/resolve/main/tf_model.h5\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "WARNING:tensorflow:From /data/anaconda3/lib/python3.8/inspect.py:350: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /data/anaconda3/lib/python3.8/inspect.py:350: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "INFO:tensorflow:Initial lr: 2e-05\n",
      "INFO:tensorflow:Train steps: 1\n",
      "INFO:tensorflow:Warmup steps: 0\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f93d5015e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f948048c2e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f948048c2e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1/1 [==============================] - ETA: 0s - mean_acc: 0.2099 - weibo_fake_cls_acc: 0.0000e+00 - weibo_fake_ner_acc: 0.4198 - weibo_fake_ner_loss: 1.6278 - weibo_fake_cls_loss: 0.9897 - weibo_fake_multi_cls_loss: 0.5451 - weibo_masklm_loss: 9.9301 - weibo_pretrain_loss: 10.5090WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1/1 [==============================] - 9s 9s/step - mean_acc: 0.2099 - weibo_fake_cls_acc: 0.0000e+00 - weibo_fake_ner_acc: 0.4198 - weibo_fake_ner_loss: 1.6278 - weibo_fake_cls_loss: 0.9897 - weibo_fake_multi_cls_loss: 0.5451 - weibo_masklm_loss: 9.9301 - weibo_pretrain_loss: 10.5090 - val_loss: 16.2879 - val_mean_acc: 0.4824 - val_weibo_fake_cls_acc: 0.4500 - val_weibo_fake_ner_acc: 0.4549\n",
      "Model: \"BertMultiTask\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "BertMultiTaskBody (BertMulti multiple                  4081928   \n",
      "_________________________________________________________________\n",
      "BertMultiTaskTop (BertMultiT multiple                  13231453  \n",
      "_________________________________________________________________\n",
      "mean_acc (Mean)              multiple                  2         \n",
      "=================================================================\n",
      "Total params: 17,313,383\n",
      "Trainable params: 17,313,377\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = train_bert_multitask(\n",
    "    problem=problem,\n",
    "    num_epochs=1,\n",
    "    params=params,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict,\n",
    "    steps_per_epoch=1,\n",
    "    continue_training=True,\n",
    "    mirrored_strategy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nsors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f9398500820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f9398500820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f9398500820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INFO:tensorflow:Initial lr: 2e-05\n",
      "INFO:tensorflow:Train steps: 1\n",
      "INFO:tensorflow:Warmup steps: 0\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.multimodal_dense.image.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.multimodal_dense.image.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_ner.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_ner.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_cls.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_cls.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_multi_cls.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_multi_cls.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_masklm.share_embedding_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_masklm.share_embedding_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_pretrain.share_embedding_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_pretrain.share_embedding_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.weight\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.pooler.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.pooler.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_pretrain.nsp.seq_relationship.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_pretrain.nsp.seq_relationship.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.position_embeddings.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.token_type_embeddings.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.embedding_hidden_mapping_in.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.embedding_hidden_mapping_in.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.multimodal_dense.image.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.multimodal_dense.image.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_ner.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_ner.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_cls.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_cls.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_multi_cls.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_multi_cls.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_masklm.share_embedding_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_masklm.share_embedding_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_pretrain.share_embedding_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_pretrain.share_embedding_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.weight\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.pooler.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.pooler.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_pretrain.nsp.seq_relationship.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_pretrain.nsp.seq_relationship.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.position_embeddings.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.token_type_embeddings.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.embedding_hidden_mapping_in.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.embedding_hidden_mapping_in.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.beta\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1/1 [==============================] - ETA: 0s - mean_acc: 0.1534 - weibo_fake_cls_acc: 0.2000 - weibo_fake_ner_acc: 0.1068 - weibo_fake_ner_loss: 1.9552 - weibo_fake_cls_loss: 0.9134 - weibo_fake_multi_cls_loss: 0.7625 - weibo_masklm_loss: 9.9983 - weibo_pretrain_loss: 10.6436WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1/1 [==============================] - 7s 7s/step - mean_acc: 0.1534 - weibo_fake_cls_acc: 0.2000 - weibo_fake_ner_acc: 0.1068 - weibo_fake_ner_loss: 1.9552 - weibo_fake_cls_loss: 0.9134 - weibo_fake_multi_cls_loss: 0.7625 - weibo_masklm_loss: 9.9983 - weibo_pretrain_loss: 10.6436 - val_loss: 16.5807 - val_mean_acc: 0.2441 - val_weibo_fake_cls_acc: 0.4500 - val_weibo_fake_ner_acc: 0.1834\n",
      "Model: \"BertMultiTask\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "BertMultiTaskBody (BertMulti multiple                  4081928   \n",
      "_________________________________________________________________\n",
      "BertMultiTaskTop (BertMultiT multiple                  13231453  \n",
      "_________________________________________________________________\n",
      "mean_acc (Mean)              multiple                  2         \n",
      "=================================================================\n",
      "Total params: 17,313,383\n",
      "Trainable params: 17,313,377\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "model = train_bert_multitask(\n",
    "    problem=problem,\n",
    "    num_epochs=1,\n",
    "    params=params,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict,\n",
    "    continue_training=True\n",
    ")\n",
    "\n",
    "# fresh train\n",
    "_ = train_bert_multitask(\n",
    "    problem=problem,\n",
    "    num_epochs=1,\n",
    "    params=params,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict,\n",
    "    steps_per_epoch=1,\n",
    "    continue_training=False,\n",
    "    mirrored_strategy=False,\n",
    "    model_dir='./models/fresh_train'\n",
    ")\n",
    "\n",
    "# transfer train\n",
    "params.init_checkpoint = './models/fresh_train'\n",
    "_ = train_bert_multitask(\n",
    "    problem=problem,\n",
    "    num_epochs=1,\n",
    "    params=params,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict,\n",
    "    steps_per_epoch=1,\n",
    "    continue_training=False,\n",
    "    mirrored_strategy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def trim_checkpoint_for_prediction(problem: str,\n",
    "                                   input_dir: str,\n",
    "                                   output_dir: str,\n",
    "                                   problem_type_dict: Dict[str, str] = None,\n",
    "                                   overwrite=True,\n",
    "                                   fake_input_list=None,\n",
    "                                   params=None):\n",
    "    \"\"\"Minimize checkpoint size for prediction.\n",
    "\n",
    "    Since the original checkpoint contains optimizer's variable,\n",
    "        for instance, if the use adam, the checkpoint size will \n",
    "        be three times of the size of model weights. This function \n",
    "        will remove those unused variables in prediction to save space.\n",
    "\n",
    "    Note: if the model is a multimodal model, you have to provide fake_input_list that\n",
    "        mimic the structure of real input.\n",
    "\n",
    "    Args:\n",
    "        problem (str): problem\n",
    "        input_dir (str): input dir\n",
    "        output_dir (str): output dir\n",
    "        problem_type_dict (Dict[str, str], optional): problem type dict. Defaults to None.\n",
    "        fake_input_list (List): fake input list to create dummy dataset\n",
    "    \"\"\"\n",
    "    if overwrite and os.path.exists(output_dir):\n",
    "        rmtree(output_dir)\n",
    "    copytree(input_dir, output_dir, ignore=ignore_patterns(\n",
    "        'checkpoint', '*.index', '*.data-000*'))\n",
    "    base_dir, dir_name = os.path.split(output_dir)\n",
    "    if params is None:\n",
    "        params = DynamicBatchSizeParams()\n",
    "    params.add_multiple_problems(problem_type_dict=problem_type_dict)\n",
    "    params.from_json(os.path.join(input_dir, 'params.json'))\n",
    "    params.assign_problem(problem, base_dir=base_dir,\n",
    "                          dir_name=dir_name, predicting=True)\n",
    "\n",
    "    model = BertMultiTask(params)\n",
    "    if fake_input_list is None:\n",
    "        dummy_dataset = predict_input_fn(['fake']*5, params)\n",
    "    else:\n",
    "        dummy_dataset = predict_input_fn(fake_input_list*5, params)\n",
    "    _ = model(next(dummy_dataset.as_numpy_iterator()),\n",
    "              mode=tf.estimator.ModeKeys.PREDICT)\n",
    "    model.load_weights(os.path.join(input_dir, 'model'))\n",
    "    model.save_weights(os.path.join(params.ckpt_dir, 'model'))\n",
    "    params.to_json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim checkpoints\n",
    "\n",
    "The checkpoints contains optimizers' states which is not needed once training is done and it makes the checkpoint size two times larger. We provide an api to trim down the size of checkpoint by removing optimizers' states.\n",
    "\n",
    "Note: in multimodal setting, you need to provide a fake input to build the model correctly. Otherwise modal embeddings will be randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new problem weibo_fake_ner, problem type: seq_tag\n",
      "Adding new problem weibo_cws, problem type: seq_tag\n",
      "Adding new problem weibo_fake_multi_cls, problem type: multi_cls\n",
      "Adding new problem weibo_fake_cls, problem type: cls\n",
      "Adding new problem weibo_masklm, problem type: masklm\n",
      "Adding new problem weibo_pretrain, problem type: pretrain\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n"
     ]
    }
   ],
   "source": [
    "# fake inputs\n",
    "import numpy as np\n",
    "fake_inputs = [{'text': 'test', 'image': np.random.uniform(\n",
    "            size=(5, 10))} for _ in range(5)] \n",
    "trim_checkpoint_for_prediction(\n",
    "    problem=problem, input_dir=model.params.ckpt_dir,\n",
    "    output_dir=model.params.ckpt_dir+'_pred',\n",
    "    problem_type_dict=problem_type_dict, overwrite=True, fake_input_list=fake_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def eval_bert_multitask(\n",
    "        problem='weibo_ner',\n",
    "        num_gpus=1,\n",
    "        model_dir='',\n",
    "        params=None,\n",
    "        problem_type_dict=None,\n",
    "        processing_fn_dict=None,\n",
    "        model=None):\n",
    "    \"\"\"Evaluate Multi-task Bert model\n",
    "\n",
    "    Available eval_scheme:\n",
    "        ner, cws, acc\n",
    "\n",
    "    Keyword Arguments:\n",
    "        problem {str} -- problems to evaluate (default: {'weibo_ner'})\n",
    "        num_gpus {int} -- number of gpu to use (default: {1})\n",
    "        model_dir {str} -- model dir (default: {''})\n",
    "        eval_scheme {str} -- Evaluation scheme (default: {'ner'})\n",
    "        params {Params} -- params to define model (default: {DynamicBatchSizeParams()})\n",
    "        problem_type_dict {dict} -- Key: problem name, value: problem type (default: {{}})\n",
    "        processing_fn_dict {dict} -- Key: problem name, value: problem data preprocessing fn (default: {{}})\n",
    "    \"\"\"\n",
    "    if not model_dir and params is not None:\n",
    "        model_dir = params.ckpt_dir\n",
    "    params = get_params_ready(problem, num_gpus, model_dir,\n",
    "                              params, problem_type_dict, processing_fn_dict,\n",
    "                              mode='predict', json_path=os.path.join(model_dir, 'params.json'))\n",
    "    eval_dataset = train_eval_input_fn(params, mode=EVAL)\n",
    "    one_batch_data = next(eval_dataset.as_numpy_iterator())\n",
    "    eval_dataset = train_eval_input_fn(params, mode=EVAL)\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    model = create_keras_model(\n",
    "        mirrored_strategy=mirrored_strategy, params=params, mode='eval', inputs_to_build_model=one_batch_data)\n",
    "    eval_dict = model.evaluate(eval_dataset, return_dict=True)\n",
    "    return eval_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\n",
    "Now we can use the trimmed checkpoint to do evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new problem weibo_fake_ner, problem type: seq_tag\n",
      "Adding new problem weibo_cws, problem type: seq_tag\n",
      "Adding new problem weibo_fake_multi_cls, problem type: multi_cls\n",
      "Adding new problem weibo_fake_cls, problem type: cls\n",
      "Adding new problem weibo_masklm, problem type: masklm\n",
      "Adding new problem weibo_pretrain, problem type: pretrain\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "9/9 [==============================] - 3s 339ms/step - loss: 16.5807 - mean_acc: 0.2441 - weibo_fake_cls_acc: 0.4500 - weibo_fake_ner_acc: 0.1834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 16.580690383911133,\n",
       " 'mean_acc': 0.2440515160560608,\n",
       " 'weibo_fake_cls_acc': 0.44999998807907104,\n",
       " 'weibo_fake_ner_acc': 0.18340164422988892}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_bert_multitask(problem=problem, params=params,\n",
    "                    problem_type_dict=problem_type_dict, processing_fn_dict=processing_fn_dict,\n",
    "                    model_dir=model.params.ckpt_dir+'_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def predict_bert_multitask(\n",
    "        inputs,\n",
    "        problem='weibo_ner',\n",
    "        model_dir='',\n",
    "        params: BaseParams = None,\n",
    "        problem_type_dict: Dict[str, str] = None,\n",
    "        processing_fn_dict: Dict[str, Callable] = None,\n",
    "        model: tf.keras.Model = None,\n",
    "        return_model=False):\n",
    "    \"\"\"Evaluate Multi-task Bert model\n",
    "\n",
    "    Available eval_scheme:\n",
    "        ner, cws, acc\n",
    "\n",
    "    Keyword Arguments:\n",
    "        problem {str} -- problems to evaluate (default: {'weibo_ner'})\n",
    "        num_gpus {int} -- number of gpu to use (default: {1})\n",
    "        model_dir {str} -- model dir (default: {''})\n",
    "        eval_scheme {str} -- Evaluation scheme (default: {'ner'})\n",
    "        params {Params} -- params to define model (default: {DynamicBatchSizeParams()})\n",
    "        problem_type_dict {dict} -- Key: problem name, value: problem type (default: {{}})\n",
    "        processing_fn_dict {dict} -- Key: problem name, value: problem data preprocessing fn (default: {{}})\n",
    "    \"\"\"\n",
    "\n",
    "    if params is None:\n",
    "        params = DynamicBatchSizeParams()\n",
    "    if not model_dir and params is not None:\n",
    "        model_dir = params.ckpt_dir\n",
    "    params = get_params_ready(problem, 1, model_dir,\n",
    "                              params, problem_type_dict, processing_fn_dict,\n",
    "                              mode='predict', json_path=os.path.join(model_dir, 'params.json'))\n",
    "\n",
    "    LOGGER.info('Checkpoint dir: %s', params.ckpt_dir)\n",
    "    time.sleep(3)\n",
    "\n",
    "    pred_dataset = predict_input_fn(inputs, params)\n",
    "    one_batch_data = next(pred_dataset.as_numpy_iterator())\n",
    "    pred_dataset = predict_input_fn(inputs, params)\n",
    "\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    if model is None:\n",
    "        model = create_keras_model(\n",
    "            mirrored_strategy=mirrored_strategy, params=params, mode='predict', inputs_to_build_model=one_batch_data)\n",
    "\n",
    "    with mirrored_strategy.scope():\n",
    "        pred = model.predict(pred_dataset)\n",
    "\n",
    "    if return_model:\n",
    "        return pred, model\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "We can do prediction by providing list of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new problem weibo_fake_ner, problem type: seq_tag\n",
      "Adding new problem weibo_cws, problem type: seq_tag\n",
      "Adding new problem weibo_fake_multi_cls, problem type: multi_cls\n",
      "Adding new problem weibo_fake_cls, problem type: cls\n",
      "Adding new problem weibo_masklm, problem type: masklm\n",
      "Adding new problem weibo_pretrain, problem type: pretrain\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "pred, model = predict_bert_multitask(\n",
    "    problem='weibo_fake_ner',\n",
    "    inputs=fake_inputs*20, model_dir=model.params.ckpt_dir,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict, return_model=True,\n",
    "    params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new problem weibo_fake_ner, problem type: seq_tag\nAdding new problem weibo_cws, problem type: seq_tag\nAdding new problem weibo_fake_multi_cls, problem type: multi_cls\nAdding new problem weibo_fake_cls, problem type: cls\nAdding new problem weibo_masklm, problem type: masklm\nAdding new problem weibo_pretrain, problem type: pretrain\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/weibo_fake_cls_weibo_fake_multi_cls_weibo_fake_ner_weibo_masklm_weibo_pretrain_ckpt_pred_pred/params.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a618af8f1348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# hide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m pred, model = predict_bert_multitask(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mproblem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weibo_fake_ner'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfake_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_pred'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-d68c6235c647>\u001b[0m in \u001b[0;36mpredict_bert_multitask\u001b[0;34m(inputs, problem, model_dir, params, problem_type_dict, processing_fn_dict, model, return_model)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     params = get_params_ready(problem, 1, model_dir,\n\u001b[0m\u001b[1;32m     31\u001b[0m                               \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem_type_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_fn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                               mode='predict', json_path=os.path.join(model_dir, 'params.json'))\n",
      "\u001b[0;32m<ipython-input-7-32f502f7b74c>\u001b[0m in \u001b[0;36mget_params_ready\u001b[0;34m(problem, num_gpus, model_dir, params, problem_type_dict, processing_fn_dict, mode, json_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         params.assign_problem(problem, gpu=int(num_gpus),\n\u001b[1;32m     22\u001b[0m                               base_dir=base_dir, dir_name=dir_name)\n",
      "\u001b[0;32m/data/bert-multitask-learning/bert_multitask_learning/params.py\u001b[0m in \u001b[0;36mfrom_json\u001b[0;34m(self, json_path)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0massign_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0mdump_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdump_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/weibo_fake_cls_weibo_fake_multi_cls_weibo_fake_ner_weibo_masklm_weibo_pretrain_ckpt_pred_pred/params.json'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
