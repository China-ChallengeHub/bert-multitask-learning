{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp run_bert_multitask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Bert Multitask Learning\n",
    "\n",
    "Train, eval and predict api for bert multitask learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Callable\n",
    "from shutil import copytree, ignore_patterns, rmtree\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.errors_impl import NotFoundError as TFNotFoundError\n",
    "\n",
    "from bert_multitask_learning.input_fn import predict_input_fn, train_eval_input_fn\n",
    "from bert_multitask_learning.model_fn import BertMultiTask\n",
    "from bert_multitask_learning.params import DynamicBatchSizeParams, BaseParams\n",
    "from bert_multitask_learning.special_tokens import EVAL\n",
    "\n",
    "# Fix duplicate log\n",
    "LOGGER = tf.get_logger()\n",
    "LOGGER.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def create_keras_model(\n",
    "        mirrored_strategy: tf.distribute.MirroredStrategy,\n",
    "        params: BaseParams,\n",
    "        mode='train',\n",
    "        inputs_to_build_model=None,\n",
    "        model=None):\n",
    "    \"\"\"init model in various mode\n",
    "\n",
    "    train: model will be loaded from huggingface\n",
    "    resume: model will be loaded from params.ckpt_dir, if params.ckpt_dir dose not contain valid checkpoint, then load from huggingface\n",
    "    transfer: model will be loaded from params.init_checkpoint, the correspongding path should contain checkpoints saved using bert-multitask-learning\n",
    "    predict: model will be loaded from params.ckpt_dir except optimizers' states\n",
    "    eval: model will be loaded from params.ckpt_dir except optimizers' states, model will be compiled\n",
    "\n",
    "    Args:\n",
    "        mirrored_strategy (tf.distribute.MirroredStrategy): mirrored strategy\n",
    "        params (BaseParams): params\n",
    "        mode (str, optional): Mode, see above explaination. Defaults to 'train'.\n",
    "        inputs_to_build_model (Dict, optional): A batch of data. Defaults to None.\n",
    "        model (Model, optional): Keras model. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        model: loaded model\n",
    "    \"\"\"\n",
    "   \n",
    "    def _get_model_wrapper(params, mode, inputs_to_build_model, model):\n",
    "        if model is None:\n",
    "            model = BertMultiTask(params)\n",
    "            # model.run_eagerly = True\n",
    "        if mode == 'resume':\n",
    "            model.compile()\n",
    "            # build training graph\n",
    "            # model.train_step(inputs_to_build_model)\n",
    "            _ = model(inputs_to_build_model,\n",
    "                      mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            # load ALL vars including optimizers' states\n",
    "            try:\n",
    "                model.load_weights(os.path.join(\n",
    "                    params.ckpt_dir, 'model'), skip_mismatch=False)\n",
    "            except TFNotFoundError:\n",
    "                LOGGER.warn('Not resuming since no mathcing ckpt found')\n",
    "        elif mode == 'transfer':\n",
    "            # build graph without optimizers' states\n",
    "            # calling compile again should reset optimizers' states but we're playing safe here\n",
    "            _ = model(inputs_to_build_model,\n",
    "                      mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            # load weights without loading optimizers' vars\n",
    "            model.load_weights(os.path.join(params.init_checkpoint, 'model'))\n",
    "            # compile again\n",
    "            model.compile()\n",
    "        elif mode == 'predict':\n",
    "            _ = model(inputs_to_build_model,\n",
    "                      mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            # load weights without loading optimizers' vars\n",
    "            model.load_weights(os.path.join(params.ckpt_dir, 'model'))\n",
    "        elif mode == 'eval':\n",
    "            _ = model(inputs_to_build_model,\n",
    "                      mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            # load weights without loading optimizers' vars\n",
    "            model.load_weights(os.path.join(params.ckpt_dir, 'model'))\n",
    "            model.compile()\n",
    "        else:\n",
    "            model.compile()\n",
    "\n",
    "        return model\n",
    "    if mirrored_strategy is not None:\n",
    "         with mirrored_strategy.scope():\n",
    "             model = _get_model_wrapper(params, mode, inputs_to_build_model, model)\n",
    "    else:\n",
    "        model = _get_model_wrapper(params, mode, inputs_to_build_model, model)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _train_bert_multitask_keras_model(train_dataset: tf.data.Dataset,\n",
    "                                      eval_dataset: tf.data.Dataset,\n",
    "                                      model: tf.keras.Model,\n",
    "                                      params: BaseParams,\n",
    "                                      mirrored_strategy: tf.distribute.MirroredStrategy = None):\n",
    "    # can't save whole model with model subclassing api due to tf bug\n",
    "    # see: https://github.com/tensorflow/tensorflow/issues/42741\n",
    "    # https://github.com/tensorflow/tensorflow/issues/40366\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(params.ckpt_dir, 'model'),\n",
    "        save_weights_only=True,\n",
    "        monitor='val_mean_acc',\n",
    "        mode='auto',\n",
    "        save_best_only=False)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=params.ckpt_dir)\n",
    "    if mirrored_strategy is not None:\n",
    "        with mirrored_strategy.scope():\n",
    "            model.fit(\n",
    "                x=train_dataset,\n",
    "                validation_data=eval_dataset,\n",
    "                epochs=params.train_epoch,\n",
    "                callbacks=[model_checkpoint_callback, tensorboard_callback],\n",
    "                steps_per_epoch=params.train_steps_per_epoch\n",
    "            )\n",
    "    else:\n",
    "        model.fit(\n",
    "            x=train_dataset,\n",
    "            validation_data=eval_dataset,\n",
    "            epochs=params.train_epoch,\n",
    "            callbacks=[model_checkpoint_callback, tensorboard_callback],\n",
    "            steps_per_epoch=params.train_steps_per_epoch\n",
    "        )\n",
    "    model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_params_ready(problem, num_gpus, model_dir, params, problem_type_dict, processing_fn_dict, mode='train', json_path=''):\n",
    "    if params is None:\n",
    "        params = DynamicBatchSizeParams()\n",
    "    if not os.path.exists('models'):\n",
    "        os.mkdir('models')\n",
    "    if model_dir:\n",
    "        base_dir, dir_name = os.path.split(model_dir)\n",
    "    else:\n",
    "        base_dir, dir_name = None, None\n",
    "    # add new problem to params if problem_type_dict and processing_fn_dict provided\n",
    "    if problem_type_dict:\n",
    "        params.add_multiple_problems(\n",
    "            problem_type_dict=problem_type_dict, processing_fn_dict=processing_fn_dict)\n",
    "    if mode == 'train':\n",
    "        params.assign_problem(problem, gpu=int(num_gpus),\n",
    "                              base_dir=base_dir, dir_name=dir_name)\n",
    "        params.to_json()\n",
    "    else:\n",
    "        params.from_json(json_path)\n",
    "        params.assign_problem(problem, gpu=int(num_gpus),\n",
    "                              base_dir=base_dir, dir_name=dir_name)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_bert_multitask(\n",
    "        problem='weibo_ner',\n",
    "        num_gpus=1,\n",
    "        num_epochs=10,\n",
    "        model_dir='',\n",
    "        params: BaseParams = None,\n",
    "        problem_type_dict: Dict[str, str] = None,\n",
    "        processing_fn_dict: Dict[str, Callable] = None,\n",
    "        model: tf.keras.Model = None,\n",
    "        create_tf_record_only=False,\n",
    "        steps_per_epoch=None,\n",
    "        warmup_ratio=0.1,\n",
    "        continue_training=False,\n",
    "        mirrored_strategy=None):\n",
    "    \"\"\"Train Multi-task Bert model\n",
    "\n",
    "    About problem:\n",
    "        There are two types of chaining operations can be used to chain problems.\n",
    "            - `&`. If two problems have the same inputs, they can be chained using `&`.\n",
    "                Problems chained by `&` will be trained at the same time.\n",
    "            - `|`. If two problems don't have the same inputs, they need to be chained using `|`.\n",
    "                Problems chained by `|` will be sampled to train at every instance.\n",
    "\n",
    "        For example, `cws|NER|weibo_ner&weibo_cws`, one problem will be sampled at each turn, say `weibo_ner&weibo_cws`, then `weibo_ner` and `weibo_cws` will trained for this turn together. Therefore, in a particular batch, some tasks might not be sampled, and their loss could be 0 in this batch.\n",
    "\n",
    "    About problem_type_dict and processing_fn_dict:\n",
    "        If the problem is not predefined, you need to tell the model what's the new problem's problem_type\n",
    "        and preprocessing function.\n",
    "            For example, a new problem: fake_classification\n",
    "            problem_type_dict = {'fake_classification': 'cls'}\n",
    "            processing_fn_dict = {'fake_classification': lambda: return ...}\n",
    "\n",
    "        Available problem type:\n",
    "            cls: Classification\n",
    "            seq_tag: Sequence Labeling\n",
    "            seq2seq_tag: Sequence to Sequence tag problem\n",
    "            seq2seq_text: Sequence to Sequence text generation problem\n",
    "\n",
    "        Preprocessing function example:\n",
    "        Please refer to https://github.com/JayYip/bert-multitask-learning/blob/master/README.md\n",
    "\n",
    "    Keyword Arguments:\n",
    "        problem {str} -- Problems to train (default: {'weibo_ner'})\n",
    "        num_gpus {int} -- Number of GPU to use (default: {1})\n",
    "        num_epochs {int} -- Number of epochs to train (default: {10})\n",
    "        model_dir {str} -- model dir (default: {''})\n",
    "        params {BaseParams} -- Params to define training and models (default: {DynamicBatchSizeParams()})\n",
    "        problem_type_dict {dict} -- Key: problem name, value: problem type (default: {{}})\n",
    "        processing_fn_dict {dict} -- Key: problem name, value: problem data preprocessing fn (default: {{}})\n",
    "    \"\"\"\n",
    "    params.train_epoch = num_epochs\n",
    "    params = get_params_ready(problem, num_gpus, model_dir,\n",
    "                              params, problem_type_dict, processing_fn_dict)\n",
    "\n",
    "    train_dataset = train_eval_input_fn(params)\n",
    "    eval_dataset = train_eval_input_fn(params, mode=EVAL)\n",
    "    if create_tf_record_only:\n",
    "        return\n",
    "\n",
    "    # get train_steps and update params\n",
    "    if steps_per_epoch is not None:\n",
    "        train_steps = steps_per_epoch\n",
    "    else:\n",
    "        train_steps = 0\n",
    "        for _ in train_dataset:\n",
    "            train_steps += 1\n",
    "    params.update_train_steps(train_steps, warmup_ratio=warmup_ratio)\n",
    "\n",
    "    train_dataset = train_dataset.repeat(params.train_epoch)\n",
    "\n",
    "    one_batch = next(train_dataset.as_numpy_iterator())\n",
    "\n",
    "    if mirrored_strategy is None:\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    elif mirrored_strategy is False:\n",
    "        mirrored_strategy = None\n",
    "\n",
    "    if num_gpus > 1 and mirrored_strategy is not None:\n",
    "        train_dataset = mirrored_strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        eval_dataset = mirrored_strategy.experimental_distribute_dataset(\n",
    "            eval_dataset)\n",
    "\n",
    "    # restore priority: self > transfer > huggingface\n",
    "    if continue_training and tf.train.latest_checkpoint(params.ckpt_dir):\n",
    "        mode = 'resume'\n",
    "    elif tf.train.latest_checkpoint(params.init_checkpoint):\n",
    "        mode = 'transfer'\n",
    "    else:\n",
    "        mode = 'train'\n",
    "\n",
    "    model = create_keras_model(\n",
    "        mirrored_strategy=mirrored_strategy, params=params, mode=mode, inputs_to_build_model=one_batch)\n",
    "\n",
    "    _train_bert_multitask_keras_model(\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        model=model,\n",
    "        params=params,\n",
    "        mirrored_strategy=mirrored_strategy\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multitask\n",
    "\n",
    "Before training, we need to do the following things:\n",
    "- pass transformers corresponding configuration to params, we use `voidful/albert_chinese_tiny` as example here\n",
    "- configure the problems we want to train, which includes\n",
    "    - training problems\n",
    "    - their problem type as a dict\n",
    "    - their preprocessing functions as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from bert_multitask_learning.predefined_problems import *\n",
    "\n",
    "from bert_multitask_learning import DynamicBatchSizeParams\n",
    "import os\n",
    "from bert_multitask_learning import predict_input_fn\n",
    "params = DynamicBatchSizeParams()\n",
    "params.shuffle_buffer = 1000\n",
    "\n",
    "# configure transformers\n",
    "params.transformer_tokenizer_loading = 'BertTokenizer'\n",
    "params.transformer_model_loading = 'AlbertForMaskedLM'\n",
    "params.transformer_config_loading = 'AlbertConfig'\n",
    "params.transformer_model_name = 'voidful/albert_chinese_tiny'\n",
    "params.transformer_config_name = 'voidful/albert_chinese_tiny'\n",
    "params.transformer_tokenizer_name = 'voidful/albert_chinese_tiny'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure problems\n",
    "# problem = 'weibo_fake_ner&weibo_fake_cls|weibo_fake_multi_cls|weibo_masklm'\n",
    "# problem_type_dict = {\n",
    "#     'weibo_fake_ner': 'seq_tag',\n",
    "#     'weibo_cws': 'seq_tag',\n",
    "#     'weibo_fake_multi_cls': 'multi_cls',\n",
    "#     'weibo_fake_cls': 'cls',\n",
    "#     'weibo_masklm': 'masklm'\n",
    "# }\n",
    "\n",
    "# processing_fn_dict = {\n",
    "#     'weibo_fake_ner': get_weibo_fake_ner_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "#     'weibo_cws': get_weibo_cws_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "#     'weibo_fake_cls': get_weibo_fake_cls_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "#     'weibo_fake_multi_cls': get_weibo_fake_multi_cls_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "#     'weibo_masklm': get_weibo_masklm(file_path='/data/bert-multitask-learning/data/ner/weiboNER*')\n",
    "# }\n",
    "problem = 'weibo_fake_ner&weibo_fake_cls|weibo_fake_multi_cls|weibo_masklm|weibo_pretrain'\n",
    "problem_type_dict = {\n",
    "    'weibo_fake_ner': 'seq_tag',\n",
    "    'weibo_cws': 'seq_tag',\n",
    "    'weibo_fake_multi_cls': 'multi_cls',\n",
    "    'weibo_fake_cls': 'cls',\n",
    "    'weibo_masklm': 'masklm',\n",
    "    'weibo_pretrain': 'pretrain'\n",
    "}\n",
    "\n",
    "processing_fn_dict = {\n",
    "    'weibo_fake_ner': get_weibo_fake_ner_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_cws': get_weibo_cws_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_fake_cls': get_weibo_fake_cls_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_fake_multi_cls': get_weibo_fake_multi_cls_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_masklm': get_weibo_masklm(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_pretrain': get_weibo_pretrain_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:bert_config not exists. will load model from huggingface checkpoint.\n",
      "Adding new problem weibo_fake_ner, problem type: seq_tag\n",
      "Adding new problem weibo_cws, problem type: seq_tag\n",
      "Adding new problem weibo_fake_multi_cls, problem type: multi_cls\n",
      "Adding new problem weibo_fake_cls, problem type: cls\n",
      "Adding new problem weibo_masklm, problem type: masklm\n",
      "Adding new problem weibo_pretrain, problem type: pretrain\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.4\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.2\n",
      "INFO:tensorflow:weibo_masklm: 0.2\n",
      "INFO:tensorflow:weibo_pretrain: 0.2\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.4\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.2\n",
      "INFO:tensorflow:weibo_masklm: 0.2\n",
      "INFO:tensorflow:weibo_pretrain: 0.2\n",
      "404 Client Error: Not Found for url: https://huggingface.co/voidful/albert_chinese_tiny/resolve/main/tf_model.h5\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "WARNING:tensorflow:From /data/anaconda3/lib/python3.8/inspect.py:350: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /data/anaconda3/lib/python3.8/inspect.py:350: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "INFO:tensorflow:Initial lr: 2e-05\n",
      "INFO:tensorflow:Train steps: 10\n",
      "INFO:tensorflow:Warmup steps: 1\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b447a4ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f4bf867c2e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f4bf867c2e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      " 1/10 [==>...........................] - ETA: 0s - mean_acc: 0.7912 - weibo_fake_cls_acc: 1.0000 - weibo_fake_ner_acc: 0.5824 - weibo_fake_ner_loss: 1.4255 - weibo_fake_cls_loss: 0.6891 - weibo_fake_multi_cls_loss: 0.6807 - weibo_masklm_loss: 9.9448 - weibo_pretrain_loss: 10.6111WARNING:tensorflow:From /data/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 8/10 [=======================>......] - ETA: 1s - mean_acc: 0.5845 - weibo_fake_cls_acc: 0.3500 - weibo_fake_ner_acc: 0.6160 - weibo_fake_ner_loss: 1.1881 - weibo_fake_cls_loss: 0.8448 - weibo_fake_multi_cls_loss: 0.4695 - weibo_masklm_loss: 4.9826 - weibo_pretrain_loss: 10.5392WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      " 8/10 [=======================>......] - 14s 2s/step - mean_acc: 0.5845 - weibo_fake_cls_acc: 0.3500 - weibo_fake_ner_acc: 0.6160 - weibo_fake_ner_loss: 1.0561 - weibo_fake_cls_loss: 0.7510 - weibo_fake_multi_cls_loss: 0.4173 - weibo_masklm_loss: 4.4290 - weibo_pretrain_loss: 10.5468 - val_loss: 16.3286 - val_mean_acc: 0.5807 - val_weibo_fake_cls_acc: 0.5500 - val_weibo_fake_ner_acc: 0.6732\n",
      "Model: \"BertMultiTask\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "BertMultiTaskBody (BertMulti multiple                  4081928   \n",
      "_________________________________________________________________\n",
      "BertMultiTaskTop (BertMultiT multiple                  13237400  \n",
      "_________________________________________________________________\n",
      "mean_acc (Mean)              multiple                  2         \n",
      "=================================================================\n",
      "Total params: 17,319,330\n",
      "Trainable params: 17,319,324\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = train_bert_multitask(\n",
    "    problem=problem,\n",
    "    num_epochs=1,\n",
    "    params=params,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict,\n",
    "    steps_per_epoch=10,\n",
    "    continue_training=True,\n",
    "    mirrored_strategy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:bert_config not exists. will load model from huggingface checkpoint.\n",
      "Adding new problem weibo_fake_ner, problem type: seq_tag\n",
      "Adding new problem weibo_cws, problem type: seq_tag\n",
      "Adding new problem weibo_fake_multi_cls, problem type: multi_cls\n",
      "Adding new problem weibo_fake_cls, problem type: cls\n",
      "Adding new problem weibo_masklm, problem type: masklm\n",
      "Adding new problem weibo_pretrain, problem type: pretrain\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.4\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.2\n",
      "INFO:tensorflow:weibo_masklm: 0.2\n",
      "INFO:tensorflow:weibo_pretrain: 0.2\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.4\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.2\n",
      "INFO:tensorflow:weibo_masklm: 0.2\n",
      "INFO:tensorflow:weibo_pretrain: 0.2\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "404 Client Error: Not Found for url: https://huggingface.co/voidful/albert_chinese_tiny/resolve/main/tf_model.h5\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "INFO:tensorflow:Initial lr: 2e-05\n",
      "INFO:tensorflow:Train steps: 8\n",
      "INFO:tensorflow:Warmup steps: 0\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b4c1c1e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b4c1c1e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b4c1c1e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b4c1c1e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b4c1c1e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:From /data/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "8/8 [==============================] - ETA: 0s - mean_acc: 0.6509 - weibo_fake_cls_acc: 0.4500 - weibo_fake_ner_acc: 0.5966 - weibo_fake_ner_loss: 1.0246 - weibo_fake_cls_loss: 0.5411 - weibo_fake_multi_cls_loss: 0.4697 - weibo_masklm_loss: 6.2262 - weibo_pretrain_loss: 10.6083WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "8/8 [==============================] - 17s 2s/step - mean_acc: 0.6509 - weibo_fake_cls_acc: 0.4500 - weibo_fake_ner_acc: 0.5966 - weibo_fake_ner_loss: 0.9107 - weibo_fake_cls_loss: 0.4809 - weibo_fake_multi_cls_loss: 0.4175 - weibo_masklm_loss: 5.5344 - weibo_pretrain_loss: 10.6083 - val_loss: 16.3286 - val_mean_acc: 0.5807 - val_weibo_fake_cls_acc: 0.5500 - val_weibo_fake_ner_acc: 0.6732\n",
      "Model: \"BertMultiTask\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "BertMultiTaskBody (BertMulti multiple                  4081928   \n",
      "_________________________________________________________________\n",
      "BertMultiTaskTop (BertMultiT multiple                  13237400  \n",
      "_________________________________________________________________\n",
      "mean_acc (Mean)              multiple                  2         \n",
      "=================================================================\n",
      "Total params: 17,319,330\n",
      "Trainable params: 17,319,324\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "model = train_bert_multitask(\n",
    "    problem=problem,\n",
    "    num_epochs=1,\n",
    "    params=params,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict,\n",
    "    continue_training=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def trim_checkpoint_for_prediction(problem: str,\n",
    "                                   input_dir: str,\n",
    "                                   output_dir: str,\n",
    "                                   problem_type_dict: Dict[str, str] = None,\n",
    "                                   overwrite=True,\n",
    "                                   fake_input_list=None):\n",
    "    \"\"\"Minimize checkpoint size for prediction.\n",
    "\n",
    "    Since the original checkpoint contains optimizer's variable,\n",
    "        for instance, if the use adam, the checkpoint size will \n",
    "        be three times of the size of model weights. This function \n",
    "        will remove those unused variables in prediction to save space.\n",
    "\n",
    "    Note: if the model is a multimodal model, you have to provide fake_input_list that\n",
    "        mimic the structure of real input.\n",
    "\n",
    "    Args:\n",
    "        problem (str): problem\n",
    "        input_dir (str): input dir\n",
    "        output_dir (str): output dir\n",
    "        problem_type_dict (Dict[str, str], optional): problem type dict. Defaults to None.\n",
    "        fake_input_list (List): fake input list to create dummy dataset\n",
    "    \"\"\"\n",
    "    if overwrite and os.path.exists(output_dir):\n",
    "        rmtree(output_dir)\n",
    "    copytree(input_dir, output_dir, ignore=ignore_patterns(\n",
    "        'checkpoint', '*.index', '*.data-000*'))\n",
    "    base_dir, dir_name = os.path.split(output_dir)\n",
    "    params = DynamicBatchSizeParams()\n",
    "    params.add_multiple_problems(problem_type_dict=problem_type_dict)\n",
    "    params.from_json(os.path.join(input_dir, 'params.json'))\n",
    "    params.assign_problem(problem, base_dir=base_dir,\n",
    "                          dir_name=dir_name, predicting=True)\n",
    "\n",
    "    model = BertMultiTask(params)\n",
    "    if fake_input_list is None:\n",
    "        dummy_dataset = predict_input_fn(['fake']*5, params)\n",
    "    else:\n",
    "        dummy_dataset = predict_input_fn(fake_input_list*5, params)\n",
    "    _ = model(next(dummy_dataset.as_numpy_iterator()),\n",
    "              mode=tf.estimator.ModeKeys.PREDICT)\n",
    "    model.load_weights(os.path.join(input_dir, 'model'))\n",
    "    model.save_weights(os.path.join(params.ckpt_dir, 'model'))\n",
    "    params.to_json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim checkpoints\n",
    "\n",
    "The checkpoints contains optimizers' states which is not needed once training is done and it makes the checkpoint size two times larger. We provide an api to trim down the size of checkpoint by removing optimizers' states.\n",
    "\n",
    "Note: in multimodal setting, you need to provide a fake input to build the model correctly. Otherwise modal embeddings will be randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7476147 0.21919394 0.7333961  0.6464561\n",
      "  0.07084714 0.76569125 0.34733689 0.51601925]\n",
      " [0.46935939 0.63930461 0.17819292 0.09252301 0.78559387 0.18203984\n",
      "  0.92411457 0.846\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.30747487 0.74799028 0.53882501 0.1357733  0.65485749 0.75965257\n",
      "  0.36138233 0.87176433 0.90745524 0.56880806]\n",
      " [0.92949239 0.88538084 0.73911505 0.96348876 0.23391701 0.37279293\n",
      "  0.4018709  0.71\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.30747487 0.74799028 0.53882501 0.1357733  0.65485749 0.75965257\n",
      "  0.36138233 0.87176433 0.90745524 0.56880806]\n",
      " [0.92949239 0.88538084 0.73911505 0.96348876 0.23391701 0.37279293\n",
      "  0.4018709  0.71\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.98869474 0.3840935  0.52536801 0.61677152 0.24243326 0.04778689\n",
      "  0.92064246 0.68976173 0.80414354 0.97269612]\n",
      " [0.34679152 0.47654615 0.9979687  0.91148181 0.35448134 0.56014375\n",
      "  0.4565003  0.75\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.98869474 0.3840935  0.52536801 0.61677152 0.24243326 0.04778689\n",
      "  0.92064246 0.68976173 0.80414354 0.97269612]\n",
      " [0.34679152 0.47654615 0.9979687  0.91148181 0.35448134 0.56014375\n",
      "  0.4565003  0.75\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.7540214  0.7869558  0.92594465 0.3590884  0.60797301 0.51984424\n",
      "  0.05035638 0.61129743 0.62097315 0.82380169]\n",
      " [0.0259433  0.09015264 0.60168957 0.44428682 0.03662868 0.24760066\n",
      "  0.58223427 0.87\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.7540214  0.7869558  0.92594465 0.3590884  0.60797301 0.51984424\n",
      "  0.05035638 0.61129743 0.62097315 0.82380169]\n",
      " [0.0259433  0.09015264 0.60168957 0.44428682 0.03662868 0.24760066\n",
      "  0.58223427 0.87\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b0e600ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b0e600ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b0e600ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b0e600ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b0e600ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).loss\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).mean_acc\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).mean_acc.total\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).mean_acc.count\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.multimodal_dense.image.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.multimodal_dense.image.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_ner.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_ner.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_cls.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_cls.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_multi_cls.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_multi_cls.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_masklm.share_embedding_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_masklm.share_embedding_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_pretrain.share_embedding_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_pretrain.share_embedding_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.weight\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.pooler.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.pooler.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_pretrain.nsp.seq_relationship.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_pretrain.nsp.seq_relationship.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.position_embeddings.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.token_type_embeddings.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.embedding_hidden_mapping_in.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.embedding_hidden_mapping_in.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.multimodal_dense.image.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.multimodal_dense.image.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_ner.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_ner.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_cls.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_cls.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_multi_cls.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_multi_cls.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_masklm.share_embedding_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_masklm.share_embedding_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_pretrain.share_embedding_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_pretrain.share_embedding_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.weight\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.pooler.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.pooler.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_pretrain.nsp.seq_relationship.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_pretrain.nsp.seq_relationship.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.position_embeddings.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.token_type_embeddings.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.embedding_hidden_mapping_in.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.embedding_hidden_mapping_in.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.beta\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "# fake inputs\n",
    "import numpy as np\n",
    "fake_inputs = [{'text': 'test', 'image': np.random.uniform(\n",
    "            size=(5, 10))} for _ in range(5)] \n",
    "trim_checkpoint_for_prediction(\n",
    "    problem=problem, input_dir=model.params.ckpt_dir,\n",
    "    output_dir=model.params.ckpt_dir+'_pred',\n",
    "    problem_type_dict=problem_type_dict, overwrite=True, fake_input_list=fake_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def eval_bert_multitask(\n",
    "        problem='weibo_ner',\n",
    "        num_gpus=1,\n",
    "        model_dir='',\n",
    "        params=None,\n",
    "        problem_type_dict=None,\n",
    "        processing_fn_dict=None,\n",
    "        model=None):\n",
    "    \"\"\"Evaluate Multi-task Bert model\n",
    "\n",
    "    Available eval_scheme:\n",
    "        ner, cws, acc\n",
    "\n",
    "    Keyword Arguments:\n",
    "        problem {str} -- problems to evaluate (default: {'weibo_ner'})\n",
    "        num_gpus {int} -- number of gpu to use (default: {1})\n",
    "        model_dir {str} -- model dir (default: {''})\n",
    "        eval_scheme {str} -- Evaluation scheme (default: {'ner'})\n",
    "        params {Params} -- params to define model (default: {DynamicBatchSizeParams()})\n",
    "        problem_type_dict {dict} -- Key: problem name, value: problem type (default: {{}})\n",
    "        processing_fn_dict {dict} -- Key: problem name, value: problem data preprocessing fn (default: {{}})\n",
    "    \"\"\"\n",
    "    if not model_dir and params is not None:\n",
    "        model_dir = params.ckpt_dir\n",
    "    params = get_params_ready(problem, num_gpus, model_dir,\n",
    "                              params, problem_type_dict, processing_fn_dict,\n",
    "                              mode='predict', json_path=os.path.join(model_dir, 'params.json'))\n",
    "    eval_dataset = train_eval_input_fn(params, mode=EVAL)\n",
    "    one_batch_data = next(eval_dataset.as_numpy_iterator())\n",
    "    eval_dataset = train_eval_input_fn(params, mode=EVAL)\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    model = create_keras_model(\n",
    "        mirrored_strategy=mirrored_strategy, params=params, mode='eval', inputs_to_build_model=one_batch_data)\n",
    "    eval_dict = model.evaluate(eval_dataset, return_dict=True)\n",
    "    return eval_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\n",
    "Now we can use the trimmed checkpoint to do evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:bert_config not exists. will load model from huggingface checkpoint.\n",
      "WARNING:root:bert_config not exists. will load model from huggingface checkpoint.\n",
      "Adding new problem weibo_fake_ner, problem type: seq_tag\n",
      "Adding new problem weibo_cws, problem type: seq_tag\n",
      "Adding new problem weibo_fake_multi_cls, problem type: multi_cls\n",
      "Adding new problem weibo_fake_cls, problem type: cls\n",
      "Adding new problem weibo_masklm, problem type: masklm\n",
      "Adding new problem weibo_pretrain, problem type: pretrain\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.4\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.2\n",
      "INFO:tensorflow:weibo_masklm: 0.2\n",
      "INFO:tensorflow:weibo_pretrain: 0.2\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.4\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.2\n",
      "INFO:tensorflow:weibo_masklm: 0.2\n",
      "INFO:tensorflow:weibo_pretrain: 0.2\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "404 Client Error: Not Found for url: https://huggingface.co/voidful/albert_chinese_tiny/resolve/main/tf_model.h5\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b4e81c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b4e81c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b4e81c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b4e81c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f4b4e81c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INFO:tensorflow:Initial lr: 2e-05\n",
      "INFO:tensorflow:Train steps: 31\n",
      "INFO:tensorflow:Warmup steps: 3\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "9/9 [==============================] - 3s 315ms/step - loss: 16.3286 - mean_acc: 0.5807 - weibo_fake_cls_acc: 0.5500 - weibo_fake_ner_acc: 0.6732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 16.328594207763672,\n",
       " 'mean_acc': 0.5807175636291504,\n",
       " 'weibo_fake_cls_acc': 0.550000011920929,\n",
       " 'weibo_fake_ner_acc': 0.6731557250022888}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_bert_multitask(problem=problem, params=params,\n",
    "                    problem_type_dict=problem_type_dict, processing_fn_dict=processing_fn_dict,\n",
    "                    model_dir=model.params.ckpt_dir+'_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def predict_bert_multitask(\n",
    "        inputs,\n",
    "        problem='weibo_ner',\n",
    "        model_dir='',\n",
    "        params: BaseParams = None,\n",
    "        problem_type_dict: Dict[str, str] = None,\n",
    "        processing_fn_dict: Dict[str, Callable] = None,\n",
    "        model: tf.keras.Model = None,\n",
    "        return_model=False):\n",
    "    \"\"\"Evaluate Multi-task Bert model\n",
    "\n",
    "    Available eval_scheme:\n",
    "        ner, cws, acc\n",
    "\n",
    "    Keyword Arguments:\n",
    "        problem {str} -- problems to evaluate (default: {'weibo_ner'})\n",
    "        num_gpus {int} -- number of gpu to use (default: {1})\n",
    "        model_dir {str} -- model dir (default: {''})\n",
    "        eval_scheme {str} -- Evaluation scheme (default: {'ner'})\n",
    "        params {Params} -- params to define model (default: {DynamicBatchSizeParams()})\n",
    "        problem_type_dict {dict} -- Key: problem name, value: problem type (default: {{}})\n",
    "        processing_fn_dict {dict} -- Key: problem name, value: problem data preprocessing fn (default: {{}})\n",
    "    \"\"\"\n",
    "\n",
    "    if params is None:\n",
    "        params = DynamicBatchSizeParams()\n",
    "    if not model_dir and params is not None:\n",
    "        model_dir = params.ckpt_dir\n",
    "    params = get_params_ready(problem, 1, model_dir,\n",
    "                              params, problem_type_dict, processing_fn_dict,\n",
    "                              mode='predict', json_path=os.path.join(model_dir, 'params.json'))\n",
    "\n",
    "    LOGGER.info('Checkpoint dir: %s', params.ckpt_dir)\n",
    "    time.sleep(3)\n",
    "\n",
    "    pred_dataset = predict_input_fn(inputs, params)\n",
    "    one_batch_data = next(pred_dataset.as_numpy_iterator())\n",
    "    pred_dataset = predict_input_fn(inputs, params)\n",
    "\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    if model is None:\n",
    "        model = create_keras_model(\n",
    "            mirrored_strategy=mirrored_strategy, params=params, mode='predict', inputs_to_build_model=one_batch_data)\n",
    "\n",
    "    with mirrored_strategy.scope():\n",
    "        pred = model.predict(pred_dataset)\n",
    "\n",
    "    if return_model:\n",
    "        return pred, model\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "We can do prediction by providing list of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:bert_config not exists. will load model from huggingface checkpoint.\n",
      "WARNING:root:bert_config not exists. will load model from huggingface checkpoint.\n",
      "Adding new problem weibo_fake_ner, problem type: seq_tag\n",
      "Adding new problem weibo_cws, problem type: seq_tag\n",
      "Adding new problem weibo_fake_multi_cls, problem type: multi_cls\n",
      "Adding new problem weibo_fake_cls, problem type: cls\n",
      "Adding new problem weibo_masklm, problem type: masklm\n",
      "Adding new problem weibo_pretrain, problem type: pretrain\n",
      "INFO:tensorflow:Checkpoint dir: models/weibo_fake_cls_weibo_fake_multi_cls_weibo_fake_ner_weibo_masklm_weibo_pretrain_ckpt_pred\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.07982977 0.77702639 0.47476147 0.21919394 0.7333961  0.6464561\n",
      "  0.07084714 0.76569125 0.34733689 0.51601925]\n",
      " [0.46935939 0.63930461 0.17819292 0.09252301 0.78559387 0.18203984\n",
      "  0.92411457 0.846\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.07982977 0.77702639 0.47476147 0.21919394 0.7333961  0.6464561\n",
      "  0.07084714 0.76569125 0.34733689 0.51601925]\n",
      " [0.46935939 0.63930461 0.17819292 0.09252301 0.78559387 0.18203984\n",
      "  0.92411457 0.846\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.94716838 0.68045356 0.32109917 0.78018467 0.95048029 0.29380353\n",
      "  0.84834849 0.26848989 0.48442955 0.87187287]\n",
      " [0.79924958 0.34709816 0.74639461 0.70582787 0.67913735 0.97978512\n",
      "  0.86461004 0.42\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.94716838 0.68045356 0.32109917 0.78018467 0.95048029 0.29380353\n",
      "  0.84834849 0.26848989 0.48442955 0.87187287]\n",
      " [0.79924958 0.34709816 0.74639461 0.70582787 0.67913735 0.97978512\n",
      "  0.86461004 0.42\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.07982977 0.77702639 0.47476147 0.21919394 0.7333961  0.6464561\n",
      "  0.07084714 0.76569125 0.34733689 0.51601925]\n",
      " [0.46935939 0.63930461 0.17819292 0.09252301 0.78559387 0.18203984\n",
      "  0.92411457 0.846\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.07982977 0.77702639 0.47476147 0.21919394 0.7333961  0.6464561\n",
      "  0.07084714 0.76569125 0.34733689 0.51601925]\n",
      " [0.46935939 0.63930461 0.17819292 0.09252301 0.78559387 0.18203984\n",
      "  0.92411457 0.846\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.30747487 0.74799028 0.53882501 0.1357733  0.65485749 0.75965257\n",
      "  0.36138233 0.87176433 0.90745524 0.56880806]\n",
      " [0.92949239 0.88538084 0.73911505 0.96348876 0.23391701 0.37279293\n",
      "  0.4018709  0.71\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.30747487 0.74799028 0.53882501 0.1357733  0.65485749 0.75965257\n",
      "  0.36138233 0.87176433 0.90745524 0.56880806]\n",
      " [0.92949239 0.88538084 0.73911505 0.96348876 0.23391701 0.37279293\n",
      "  0.4018709  0.71\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.98869474 0.3840935  0.52536801 0.61677152 0.24243326 0.04778689\n",
      "  0.92064246 0.68976173 0.80414354 0.97269612]\n",
      " [0.34679152 0.47654615 0.9979687  0.91148181 0.35448134 0.56014375\n",
      "  0.4565003  0.75\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.98869474 0.3840935  0.52536801 0.61677152 0.24243326 0.04778689\n",
      "  0.92064246 0.68976173 0.80414354 0.97269612]\n",
      " [0.34679152 0.47654615 0.9979687  0.91148181 0.35448134 0.56014375\n",
      "  0.4565003  0.75\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.7540214  0.7869558  0.92594465 0.3590884  0.60797301 0.51984424\n",
      "  0.05035638 0.61129743 0.62097315 0.82380169]\n",
      " [0.0259433  0.09015264 0.60168957 0.44428682 0.03662868 0.24760066\n",
      "  0.58223427 0.87\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.7540214  0.7869558  0.92594465 0.3590884  0.60797301 0.51984424\n",
      "  0.05035638 0.61129743 0.62097315 0.82380169]\n",
      " [0.0259433  0.09015264 0.60168957 0.44428682 0.03662868 0.24760066\n",
      "  0.58223427 0.87\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.07982977 0.77702639 0.47476147 0.21919394 0.7333961  0.6464561\n",
      "  0.07084714 0.76569125 0.34733689 0.51601925]\n",
      " [0.46935939 0.63930461 0.17819292 0.09252301 0.78559387 0.18203984\n",
      "  0.92411457 0.846\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.07982977 0.77702639 0.47476147 0.21919394 0.7333961  0.6464561\n",
      "  0.07084714 0.76569125 0.34733689 0.51601925]\n",
      " [0.46935939 0.63930461 0.17819292 0.09252301 0.78559387 0.18203984\n",
      "  0.92411457 0.846\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "404 Client Error: Not Found for url: https://huggingface.co/voidful/albert_chinese_tiny/resolve/main/tf_model.h5\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.94716838 0.68045356 0.32109917 0.78018467 0.95048029 0.29380353\n",
      "  0.84834849 0.26848989 0.48442955 0.87187287]\n",
      " [0.79924958 0.34709816 0.74639461 0.70582787 0.67913735 0.97978512\n",
      "  0.86461004 0.42\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.94716838 0.68045356 0.32109917 0.78018467 0.95048029 0.29380353\n",
      "  0.84834849 0.26848989 0.48442955 0.87187287]\n",
      " [0.79924958 0.34709816 0.74639461 0.70582787 0.67913735 0.97978512\n",
      "  0.86461004 0.42\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.07982977 0.77702639 0.47476147 0.21919394 0.7333961  0.6464561\n",
      "  0.07084714 0.76569125 0.34733689 0.51601925]\n",
      " [0.46935939 0.63930461 0.17819292 0.09252301 0.78559387 0.18203984\n",
      "  0.92411457 0.846\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.07982977 0.77702639 0.47476147 0.21919394 0.7333961  0.6464561\n",
      "  0.07084714 0.76569125 0.34733689 0.51601925]\n",
      " [0.46935939 0.63930461 0.17819292 0.09252301 0.78559387 0.18203984\n",
      "  0.92411457 0.846\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.30747487 0.74799028 0.53882501 0.1357733  0.65485749 0.75965257\n",
      "  0.36138233 0.87176433 0.90745524 0.56880806]\n",
      " [0.92949239 0.88538084 0.73911505 0.96348876 0.23391701 0.37279293\n",
      "  0.4018709  0.71\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.30747487 0.74799028 0.53882501 0.1357733  0.65485749 0.75965257\n",
      "  0.36138233 0.87176433 0.90745524 0.56880806]\n",
      " [0.92949239 0.88538084 0.73911505 0.96348876 0.23391701 0.37279293\n",
      "  0.4018709  0.71\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.98869474 0.3840935  0.52536801 0.61677152 0.24243326 0.04778689\n",
      "  0.92064246 0.68976173 0.80414354 0.97269612]\n",
      " [0.34679152 0.47654615 0.9979687  0.91148181 0.35448134 0.56014375\n",
      "  0.4565003  0.75\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.98869474 0.3840935  0.52536801 0.61677152 0.24243326 0.04778689\n",
      "  0.92064246 0.68976173 0.80414354 0.97269612]\n",
      " [0.34679152 0.47654615 0.9979687  0.91148181 0.35448134 0.56014375\n",
      "  0.4565003  0.75\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n",
      "INFO:tensorflow:text: test\n",
      "INFO:tensorflow:image: [[0.7540214  0.7869558  0.92594465 0.3590884  0.60797301 0.51984424\n",
      "  0.05035638 0.61129743 0.62097315 0.82380169]\n",
      " [0.0259433  0.09015264 0.60168957 0.44428682 0.03662868 0.24760066\n",
      "  0.58223427 0.87\n",
      "INFO:tensorflow:input_ids: [101, 10060, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0]\n",
      "INFO:tensorflow:image_input: [[0.7540214  0.7869558  0.92594465 0.3590884  0.60797301 0.51984424\n",
      "  0.05035638 0.61129743 0.62097315 0.82380169]\n",
      " [0.0259433  0.09015264 0.60168957 0.44428682 0.03662868 0.24760066\n",
      "  0.58223427 0.87\n",
      "INFO:tensorflow:image_mask: [1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:image_segment_ids: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "pred, model = predict_bert_multitask(\n",
    "    problem='weibo_fake_ner',\n",
    "    inputs=fake_inputs, model_dir=model.params.ckpt_dir,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict, return_model=True,\n",
    "    params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
