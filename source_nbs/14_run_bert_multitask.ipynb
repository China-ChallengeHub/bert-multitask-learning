{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp run_bert_multitask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Bert Multitask Learning\n",
    "\n",
    "Train, eval and predict api for bert multitask learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Callable\n",
    "from shutil import copytree, ignore_patterns, rmtree\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.errors_impl import NotFoundError as TFNotFoundError\n",
    "\n",
    "from bert_multitask_learning.input_fn import predict_input_fn, train_eval_input_fn\n",
    "from bert_multitask_learning.model_fn import BertMultiTask\n",
    "from bert_multitask_learning.params import DynamicBatchSizeParams, BaseParams\n",
    "from bert_multitask_learning.special_tokens import EVAL\n",
    "\n",
    "# Fix duplicate log\n",
    "LOGGER = tf.get_logger()\n",
    "LOGGER.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def create_keras_model(\n",
    "        mirrored_strategy: tf.distribute.MirroredStrategy,\n",
    "        params: BaseParams,\n",
    "        mode='train',\n",
    "        inputs_to_build_model=None,\n",
    "        model=None):\n",
    "    \"\"\"init model in various mode\n",
    "\n",
    "    train: model will be loaded from huggingface\n",
    "    resume: model will be loaded from params.ckpt_dir, if params.ckpt_dir dose not contain valid checkpoint, then load from huggingface\n",
    "    transfer: model will be loaded from params.init_checkpoint, the correspongding path should contain checkpoints saved using bert-multitask-learning\n",
    "    predict: model will be loaded from params.ckpt_dir except optimizers' states\n",
    "    eval: model will be loaded from params.ckpt_dir except optimizers' states, model will be compiled\n",
    "\n",
    "    Args:\n",
    "        mirrored_strategy (tf.distribute.MirroredStrategy): mirrored strategy\n",
    "        params (BaseParams): params\n",
    "        mode (str, optional): Mode, see above explaination. Defaults to 'train'.\n",
    "        inputs_to_build_model (Dict, optional): A batch of data. Defaults to None.\n",
    "        model (Model, optional): Keras model. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        model: loaded model\n",
    "    \"\"\"\n",
    "   \n",
    "    def _get_model_wrapper(params, mode, inputs_to_build_model, model):\n",
    "        if model is None:\n",
    "            model = BertMultiTask(params)\n",
    "            # model.run_eagerly = True\n",
    "        if mode == 'resume':\n",
    "            model.compile()\n",
    "            # build training graph\n",
    "            # model.train_step(inputs_to_build_model)\n",
    "            _ = model(inputs_to_build_model,\n",
    "                      mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            # load ALL vars including optimizers' states\n",
    "            try:\n",
    "                model.load_weights(os.path.join(\n",
    "                    params.ckpt_dir, 'model'), skip_mismatch=False)\n",
    "            except TFNotFoundError:\n",
    "                LOGGER.warn('Not resuming since no mathcing ckpt found')\n",
    "        elif mode == 'transfer':\n",
    "            # build graph without optimizers' states\n",
    "            # calling compile again should reset optimizers' states but we're playing safe here\n",
    "            _ = model(inputs_to_build_model,\n",
    "                      mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            # load weights without loading optimizers' vars\n",
    "            model.load_weights(os.path.join(params.init_checkpoint, 'model'))\n",
    "            # compile again\n",
    "            model.compile()\n",
    "        elif mode == 'predict':\n",
    "            _ = model(inputs_to_build_model,\n",
    "                      mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            # load weights without loading optimizers' vars\n",
    "            model.load_weights(os.path.join(params.ckpt_dir, 'model'))\n",
    "        elif mode == 'eval':\n",
    "            _ = model(inputs_to_build_model,\n",
    "                      mode=tf.estimator.ModeKeys.PREDICT)\n",
    "            # load weights without loading optimizers' vars\n",
    "            model.load_weights(os.path.join(params.ckpt_dir, 'model'))\n",
    "            model.compile()\n",
    "        else:\n",
    "            model.compile()\n",
    "\n",
    "        return model\n",
    "    if mirrored_strategy is not None:\n",
    "         with mirrored_strategy.scope():\n",
    "             model = _get_model_wrapper(params, mode, inputs_to_build_model, model)\n",
    "    else:\n",
    "        model = _get_model_wrapper(params, mode, inputs_to_build_model, model)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _train_bert_multitask_keras_model(train_dataset: tf.data.Dataset,\n",
    "                                      eval_dataset: tf.data.Dataset,\n",
    "                                      model: tf.keras.Model,\n",
    "                                      params: BaseParams,\n",
    "                                      mirrored_strategy: tf.distribute.MirroredStrategy = None):\n",
    "    # can't save whole model with model subclassing api due to tf bug\n",
    "    # see: https://github.com/tensorflow/tensorflow/issues/42741\n",
    "    # https://github.com/tensorflow/tensorflow/issues/40366\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(params.ckpt_dir, 'model'),\n",
    "        save_weights_only=True,\n",
    "        monitor='val_mean_acc',\n",
    "        mode='auto',\n",
    "        save_best_only=False)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=params.ckpt_dir)\n",
    "    if mirrored_strategy is not None:\n",
    "        with mirrored_strategy.scope():\n",
    "            model.fit(\n",
    "                x=train_dataset,\n",
    "                validation_data=eval_dataset,\n",
    "                epochs=params.train_epoch,\n",
    "                callbacks=[model_checkpoint_callback, tensorboard_callback],\n",
    "                steps_per_epoch=params.train_steps_per_epoch\n",
    "            )\n",
    "    else:\n",
    "        model.fit(\n",
    "            x=train_dataset,\n",
    "            validation_data=eval_dataset,\n",
    "            epochs=params.train_epoch,\n",
    "            callbacks=[model_checkpoint_callback, tensorboard_callback],\n",
    "            steps_per_epoch=params.train_steps_per_epoch\n",
    "        )\n",
    "    model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_params_ready(problem, num_gpus, model_dir, params, problem_type_dict, processing_fn_dict, mode='train', json_path=''):\n",
    "    if params is None:\n",
    "        params = DynamicBatchSizeParams()\n",
    "    if not os.path.exists('models'):\n",
    "        os.mkdir('models')\n",
    "    if model_dir:\n",
    "        base_dir, dir_name = os.path.split(model_dir)\n",
    "    else:\n",
    "        base_dir, dir_name = None, None\n",
    "    # add new problem to params if problem_type_dict and processing_fn_dict provided\n",
    "    if problem_type_dict:\n",
    "        params.add_multiple_problems(\n",
    "            problem_type_dict=problem_type_dict, processing_fn_dict=processing_fn_dict)\n",
    "    if mode == 'train':\n",
    "        params.assign_problem(problem, gpu=int(num_gpus),\n",
    "                              base_dir=base_dir, dir_name=dir_name)\n",
    "        params.to_json()\n",
    "    else:\n",
    "        params.from_json(json_path)\n",
    "        params.assign_problem(problem, gpu=int(num_gpus),\n",
    "                              base_dir=base_dir, dir_name=dir_name)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_bert_multitask(\n",
    "        problem='weibo_ner',\n",
    "        num_gpus=1,\n",
    "        num_epochs=10,\n",
    "        model_dir='',\n",
    "        params: BaseParams = None,\n",
    "        problem_type_dict: Dict[str, str] = None,\n",
    "        processing_fn_dict: Dict[str, Callable] = None,\n",
    "        model: tf.keras.Model = None,\n",
    "        create_tf_record_only=False,\n",
    "        steps_per_epoch=None,\n",
    "        warmup_ratio=0.1,\n",
    "        continue_training=False,\n",
    "        mirrored_strategy=None):\n",
    "    \"\"\"Train Multi-task Bert model\n",
    "\n",
    "    About problem:\n",
    "        There are two types of chaining operations can be used to chain problems.\n",
    "            - `&`. If two problems have the same inputs, they can be chained using `&`.\n",
    "                Problems chained by `&` will be trained at the same time.\n",
    "            - `|`. If two problems don't have the same inputs, they need to be chained using `|`.\n",
    "                Problems chained by `|` will be sampled to train at every instance.\n",
    "\n",
    "        For example, `cws|NER|weibo_ner&weibo_cws`, one problem will be sampled at each turn, say `weibo_ner&weibo_cws`, then `weibo_ner` and `weibo_cws` will trained for this turn together. Therefore, in a particular batch, some tasks might not be sampled, and their loss could be 0 in this batch.\n",
    "\n",
    "    About problem_type_dict and processing_fn_dict:\n",
    "        If the problem is not predefined, you need to tell the model what's the new problem's problem_type\n",
    "        and preprocessing function.\n",
    "            For example, a new problem: fake_classification\n",
    "            problem_type_dict = {'fake_classification': 'cls'}\n",
    "            processing_fn_dict = {'fake_classification': lambda: return ...}\n",
    "\n",
    "        Available problem type:\n",
    "            cls: Classification\n",
    "            seq_tag: Sequence Labeling\n",
    "            seq2seq_tag: Sequence to Sequence tag problem\n",
    "            seq2seq_text: Sequence to Sequence text generation problem\n",
    "\n",
    "        Preprocessing function example:\n",
    "        Please refer to https://github.com/JayYip/bert-multitask-learning/blob/master/README.md\n",
    "\n",
    "    Keyword Arguments:\n",
    "        problem {str} -- Problems to train (default: {'weibo_ner'})\n",
    "        num_gpus {int} -- Number of GPU to use (default: {1})\n",
    "        num_epochs {int} -- Number of epochs to train (default: {10})\n",
    "        model_dir {str} -- model dir (default: {''})\n",
    "        params {BaseParams} -- Params to define training and models (default: {DynamicBatchSizeParams()})\n",
    "        problem_type_dict {dict} -- Key: problem name, value: problem type (default: {{}})\n",
    "        processing_fn_dict {dict} -- Key: problem name, value: problem data preprocessing fn (default: {{}})\n",
    "    \"\"\"\n",
    "    \n",
    "    params = get_params_ready(problem, num_gpus, model_dir,\n",
    "                              params, problem_type_dict, processing_fn_dict)\n",
    "    params.train_epoch = num_epochs\n",
    "\n",
    "    train_dataset = train_eval_input_fn(params)\n",
    "    eval_dataset = train_eval_input_fn(params, mode=EVAL)\n",
    "    if create_tf_record_only:\n",
    "        return\n",
    "\n",
    "    # get train_steps and update params\n",
    "    if steps_per_epoch is not None:\n",
    "        train_steps = steps_per_epoch\n",
    "    else:\n",
    "        train_steps = 0\n",
    "        for _ in train_dataset:\n",
    "            train_steps += 1\n",
    "    params.update_train_steps(train_steps, warmup_ratio=warmup_ratio)\n",
    "    \n",
    "    train_dataset = train_eval_input_fn(params)\n",
    "    train_dataset = train_dataset.repeat(10)\n",
    "\n",
    "    one_batch = next(train_dataset.as_numpy_iterator())\n",
    "\n",
    "    if mirrored_strategy is None:\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    elif mirrored_strategy is False:\n",
    "        mirrored_strategy = None\n",
    "\n",
    "    if num_gpus > 1 and mirrored_strategy is not False:\n",
    "        train_dataset = mirrored_strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        eval_dataset = mirrored_strategy.experimental_distribute_dataset(\n",
    "            eval_dataset)\n",
    "\n",
    "    # restore priority: self > transfer > huggingface\n",
    "    if continue_training and tf.train.latest_checkpoint(params.ckpt_dir):\n",
    "        mode = 'resume'\n",
    "    elif tf.train.latest_checkpoint(params.init_checkpoint):\n",
    "        mode = 'transfer'\n",
    "    else:\n",
    "        mode = 'train'\n",
    "\n",
    "    model = create_keras_model(\n",
    "        mirrored_strategy=mirrored_strategy, params=params, mode=mode, inputs_to_build_model=one_batch)\n",
    "\n",
    "    _train_bert_multitask_keras_model(\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        model=model,\n",
    "        params=params,\n",
    "        mirrored_strategy=mirrored_strategy\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multitask\n",
    "\n",
    "Before training, we need to do the following things:\n",
    "- pass transformers corresponding configuration to params, we use `voidful/albert_chinese_tiny` as example here\n",
    "- configure the problems we want to train, which includes\n",
    "    - training problems\n",
    "    - their problem type as a dict\n",
    "    - their preprocessing functions as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from bert_multitask_learning.predefined_problems import *\n",
    "\n",
    "from bert_multitask_learning import DynamicBatchSizeParams\n",
    "import os\n",
    "from bert_multitask_learning import predict_input_fn\n",
    "params = DynamicBatchSizeParams()\n",
    "params.shuffle_buffer = 1000\n",
    "\n",
    "# configure transformers\n",
    "params.transformer_tokenizer_loading = 'BertTokenizer'\n",
    "params.transformer_model_loading = 'AlbertForMaskedLM'\n",
    "params.transformer_config_loading = 'AlbertConfig'\n",
    "params.transformer_model_name = 'voidful/albert_chinese_tiny'\n",
    "params.transformer_config_name = 'voidful/albert_chinese_tiny'\n",
    "params.transformer_tokenizer_name = 'voidful/albert_chinese_tiny'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import tempfile\n",
    "params.tmp_file_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "problem = 'weibo_fake_ner&weibo_fake_cls|weibo_fake_multi_cls|weibo_masklm|weibo_pretrain'\n",
    "problem_type_dict = {\n",
    "    'weibo_fake_ner': 'seq_tag',\n",
    "    'weibo_cws': 'seq_tag',\n",
    "    'weibo_fake_multi_cls': 'multi_cls',\n",
    "    'weibo_fake_cls': 'cls',\n",
    "    'weibo_masklm': 'masklm',\n",
    "    'weibo_pretrain': 'pretrain'\n",
    "}\n",
    "\n",
    "processing_fn_dict = {\n",
    "    'weibo_fake_ner': get_weibo_fake_ner_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_cws': get_weibo_cws_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_fake_cls': get_weibo_fake_cls_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_fake_multi_cls': get_weibo_fake_multi_cls_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_masklm': get_weibo_masklm(file_path='/data/bert-multitask-learning/data/ner/weiboNER*'),\n",
    "    'weibo_pretrain': get_weibo_pretrain_fn(file_path='/data/bert-multitask-learning/data/ner/weiboNER*')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "101, 2458, 1297, 749, 8024, 103, 1157, 103, 103, 103, 1378, 103, 2769, 103, 2207, 5101, 2797, 3322, 1728, 711, 800, 3221, 103, 791, 103, 103, 3297, 2571, 103, 2207, 5101, 2797,\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [1, 9, 11, 12, 13, 15, 17, 26, 28, 29, 32, 39, 52, 58, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1420 1157 7564 5276  749  511 4263 6812  711 3632 4638  704 5101 5276\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['最', '热', '时', '尚', '榜', '女', '人', '不', '坏', '，', '男', '人', '不', '爱', '，', '一', '个', '男', '女', '必', '看', '的', '微', '博', '花', '心']\n",
      "INFO:tensorflow:input_ids: [101, 3297, 4178, 3198, 2213, 103, 1957, 782, 679, 1776, 8024, 4511, 782, 679, 4263, 8024, 671, 702, 4511, 1957, 2553, 4692, 103, 2544, 1300, 5709, 2552, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [5, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3528 4638    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['回', '复', '支', '持', '，', '赞', '成', '，', '哈', '哈', '米', '八', '吴', '够', '历', '史', '要', '的', '陈', '小', '奥', '丁', '丁', '我', '爱', '小', '肥', '肥', '一', '族', '大', '头', '仔', '大', '家', '团', '结', '一', '致', '，',\n",
      "INFO:tensorflow:input_ids: [101, 1726, 1908, 3118, 103, 103, 6614, 2768, 8024, 103, 1506, 5101, 103, 1426, 1916, 1325, 1380, 6206, 4638, 7357, 2207, 1952, 672, 672, 103, 4263, 103, 5503, 103, 671, 3184, 1920, 103, 798, 1920, 21\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [4, 5, 9, 12, 24, 26, 28, 32, 44, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [2898 8024 1506 1061 2769 2207 5503 1928 1378 7030    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['剑', '网', '乱', '世', '长', '安', '公', '测', '盛', '典', '今', '日', '开', '启', '，', '海', '量', '豪', '礼', '火', '爆', '开', '送', '精', '美', '挂', '件', '、', '听', '雨', '·', '汉', '服', '娃', '娃', '、', '诙', '谐', '双', '骑',\n",
      "INFO:tensorflow:input_ids: [101, 1187, 5381, 744, 686, 7270, 2128, 1062, 3844, 103, 1073, 791, 3189, 2458, 1423, 8024, 3862, 7030, 6498, 103, 4125, 103, 2458, 103, 5125, 5401, 103, 816, 510, 1420, 7433, 185, 3727, 103, 2015, 20\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [9, 19, 21, 23, 26, 33, 42, 51, 54, 76, 82, 86, 87, 89, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [4670 4851 4255 6843 2899 3302 6762 1283 5101 1346 2544 2145 2787 1218\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['在', '街', '上', '听', '见', '音', '乐', '我', '舞', '动', '起', '来', '很', '丢', '人', '？', '真', '的', '很', '丢', '人', '吗', '？']\n",
      "INFO:tensorflow:input_ids: [101, 1762, 6125, 103, 1420, 6224, 103, 727, 2769, 5659, 1220, 6629, 3341, 2523, 696, 782, 103, 4696, 103, 2523, 696, 782, 103, 8043, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [3, 6, 16, 18, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [ 677 7509 8043 4638 1408    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['三', '毛', '说', '我', '唯', '一', '锲', '而', '不', '舍', '，', '愿', '意', '以', '自', '己', '的', '生', '命', '去', '努', '力', '的', '，', '只', '不', '过', '是', '保', '守', '我', '个', '人', '的', '心', '怀', '意', '念', '，', '在',\n",
      "INFO:tensorflow:input_ids: [101, 676, 3688, 6432, 2769, 1546, 671, 7244, 5445, 679, 103, 8024, 2703, 2692, 809, 103, 2346, 4638, 4495, 1462, 1343, 1222, 1213, 4638, 8024, 1372, 103, 6814, 3221, 924, 2127, 2769, 702, 782, 4638, \n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [10, 15, 26, 37, 58, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [5650 5632  679 2692 2190 1762    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['星', '期', '天', '的', '早', '晨', '七', '点', '学', '车', '，', '驾', '校', '太', '给', '力', '。', '我', '的', '头', '发', '都', '没', '有', '洗']\n",
      "INFO:tensorflow:input_ids: [101, 3215, 3309, 1921, 4638, 103, 3247, 673, 103, 103, 6756, 8024, 103, 3413, 1922, 5314, 1213, 511, 2769, 4638, 1928, 1355, 103, 3766, 3300, 3819, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [5, 8, 9, 12, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [3193 4157 2110 7730 6963    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['发', '表', '了', '博', '文', '小', '学', '美', '术', '新', '课', '标', '的', '反', '思', '学', '习', '新', '学', '期', '开', '学', '有', '两', '个', '多', '月', '了', '，', '在', '这', '段', '时', '间', '的', '教', '学', '计', '划', '、',\n",
      "INFO:tensorflow:input_ids: [101, 1355, 6134, 749, 103, 3152, 103, 2110, 5401, 3318, 3173, 6440, 3403, 4638, 1353, 2590, 2110, 103, 3173, 2110, 3309, 2458, 2110, 3300, 697, 702, 1914, 3299, 103, 8024, 1762, 6821, 3667, 103, 7313\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [4, 6, 17, 28, 33, 37, 56, 62, 68, 73, 77, 81, 85, 88, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1300 2207  739  749 3198 2110 3318 3300 6371 4385 4638 3418 2552 4157\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['哈', '哈', '哈', '哈', '哈', '哈', '镰', '刀', '刮', '腋', '毛', '哈', '哈', '哈', '哈', '哈', '哈', '我', '的', '朋', '友', '是', '个', '呆', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '哈', '十', '万', '个',\n",
      "INFO:tensorflow:input_ids: [101, 1506, 1506, 1506, 1506, 1506, 103, 7266, 1143, 1167, 5573, 3688, 1506, 1506, 1506, 103, 1506, 1506, 103, 4638, 3301, 1351, 3221, 702, 1438, 1506, 1506, 1506, 1506, 103, 1506, 1506, 1506, 1506, 1\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "INFO:tensorflow:masked_lm_positions: [6, 15, 18, 29, 53, 69, 73, 87, 93, 97, 98, 105, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [1506 1506 2769 1506 1506 3683 4331 1541 1506 3392 7448 1506    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['�', '�', '就', '爱', '这', '个', '牌', '子', '的', '糖', '果']\n",
      "INFO:tensorflow:input_ids: [101, 2218, 4263, 6821, 702, 103, 2094, 4638, 5131, 103, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [5, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [4277 3362    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['延', '参', '法', '师', '品', '味', '人', '生', '如', '同', '走', '进', '一', '片', '山', '水', '，', '静', '静', '的', '呼', '吸', '，', '安', '静', '的', '欣', '赏', '，', '这', '就', '是', '生', '活', '。']\n",
      "INFO:tensorflow:input_ids: [101, 2454, 1346, 3791, 2360, 1501, 1456, 103, 4495, 1963, 1398, 6624, 103, 671, 4275, 2255, 3717, 8024, 7474, 7474, 4638, 1461, 1429, 8024, 2128, 7474, 4638, 3615, 6605, 103, 6821, 2218, 3221, 103, 3\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [7, 12, 29, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [ 782 6822 8024 4495    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:['就', '感', '恩', '吧', '越', '来', '越', '没', '战', '斗', '力', '了', '。']\n",
      "INFO:tensorflow:input_ids: [101, 2218, 103, 2617, 1416, 6632, 3341, 6632, 3766, 2773, 3159, 1213, 749, 511, 102]\n",
      "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_positions: [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "INFO:tensorflow:masked_lm_ids: [2697    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "INFO:tensorflow:masked_lm_weights: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.4\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.2\n",
      "INFO:tensorflow:weibo_masklm: 0.2\n",
      "INFO:tensorflow:weibo_pretrain: 0.2\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.4\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.2\n",
      "INFO:tensorflow:weibo_masklm: 0.2\n",
      "INFO:tensorflow:weibo_pretrain: 0.2\n",
      "INFO:tensorflow:sampling weights: \n",
      "INFO:tensorflow:weibo_fake_cls_weibo_fake_ner: 0.4\n",
      "INFO:tensorflow:weibo_fake_multi_cls: 0.2\n",
      "INFO:tensorflow:weibo_masklm: 0.2\n",
      "INFO:tensorflow:weibo_pretrain: 0.2\n",
      "404 Client Error: Not Found for url: https://huggingface.co/voidful/albert_chinese_tiny/resolve/main/tf_model.h5\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "WARNING:tensorflow:From /data/anaconda3/lib/python3.8/inspect.py:350: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /data/anaconda3/lib/python3.8/inspect.py:350: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "INFO:tensorflow:Initial lr: 2e-05\n",
      "INFO:tensorflow:Train steps: 1\n",
      "INFO:tensorflow:Warmup steps: 0\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f3e95b1bee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f3f401db2e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f3f401db2e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1/1 [==============================] - ETA: 0s - mean_acc: 0.2095 - weibo_fake_cls_acc: 0.0000e+00 - weibo_fake_ner_acc: 0.4189 - weibo_fake_ner_loss: 1.6564 - weibo_fake_cls_loss: 1.4976 - weibo_fake_multi_cls_loss: 1.3982 - weibo_masklm_loss: 9.9856 - weibo_pretrain_loss: 10.6494WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1/1 [==============================] - 7s 7s/step - mean_acc: 0.2095 - weibo_fake_cls_acc: 0.0000e+00 - weibo_fake_ner_acc: 0.4189 - weibo_fake_ner_loss: 1.6564 - weibo_fake_cls_loss: 1.4976 - weibo_fake_multi_cls_loss: 1.3982 - weibo_masklm_loss: 9.9856 - weibo_pretrain_loss: 10.6494 - val_loss: 16.8679 - val_mean_acc: 0.4195 - val_weibo_fake_cls_acc: 0.5500 - val_weibo_fake_ner_acc: 0.3088\n",
      "Model: \"BertMultiTask\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "BertMultiTaskBody (BertMulti multiple                  4081928   \n",
      "_________________________________________________________________\n",
      "BertMultiTaskTop (BertMultiT multiple                  13231453  \n",
      "_________________________________________________________________\n",
      "mean_acc (Mean)              multiple                  2         \n",
      "=================================================================\n",
      "Total params: 17,313,383\n",
      "Trainable params: 17,313,377\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = train_bert_multitask(\n",
    "    problem=problem,\n",
    "    num_epochs=1,\n",
    "    params=params,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict,\n",
    "    steps_per_epoch=1,\n",
    "    continue_training=True,\n",
    "    mirrored_strategy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "th different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f3e342a1d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f3e342a1d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function BertMultiTaskBody.get_features_for_problem at 0x7f3e342a1d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INFO:tensorflow:Initial lr: 2e-05\n",
      "INFO:tensorflow:Train steps: 1\n",
      "INFO:tensorflow:Warmup steps: 0\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.multimodal_dense.image.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.multimodal_dense.image.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_ner.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_ner.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_cls.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_cls.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_multi_cls.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_fake_multi_cls.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_masklm.share_embedding_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_masklm.share_embedding_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_pretrain.share_embedding_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_pretrain.share_embedding_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.weight\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.pooler.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.pooler.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_pretrain.nsp.seq_relationship.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).top.top_layer_dict.weibo_pretrain.nsp.seq_relationship.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.position_embeddings.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.token_type_embeddings.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.embedding_hidden_mapping_in.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.embedding_hidden_mapping_in.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.multimodal_dense.image.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.multimodal_dense.image.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_ner.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_ner.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_cls.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_cls.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_multi_cls.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_fake_multi_cls.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_masklm.share_embedding_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_masklm.share_embedding_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_pretrain.share_embedding_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_pretrain.share_embedding_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.weight\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.pooler.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.pooler.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_pretrain.nsp.seq_relationship.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).top.top_layer_dict.weibo_pretrain.nsp.seq_relationship.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.position_embeddings.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.token_type_embeddings.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.LayerNorm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.embeddings.LayerNorm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.embedding_hidden_mapping_in.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.embedding_hidden_mapping_in.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).body.bert.bert_model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.beta\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1/1 [==============================] - ETA: 0s - mean_acc: 0.3333 - weibo_fake_cls_acc: 0.0000e+00 - weibo_fake_ner_acc: 0.6667 - weibo_fake_ner_loss: 2.0548 - weibo_fake_cls_loss: 0.6933 - weibo_fake_multi_cls_loss: 0.6051 - weibo_masklm_loss: 9.9221 - weibo_pretrain_loss: 10.7459WARNING:tensorflow:Seems there's a multimodal inputs but params.enable_modal_type is not set to be True.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1/1 [==============================] - 7s 7s/step - mean_acc: 0.3333 - weibo_fake_cls_acc: 0.0000e+00 - weibo_fake_ner_acc: 0.6667 - weibo_fake_ner_loss: 2.0548 - weibo_fake_cls_loss: 0.6933 - weibo_fake_multi_cls_loss: 0.6051 - weibo_masklm_loss: 9.9221 - weibo_pretrain_loss: 10.7459 - val_loss: 16.5940 - val_mean_acc: 0.6559 - val_weibo_fake_cls_acc: 0.6000 - val_weibo_fake_ner_acc: 0.6994\n",
      "Model: \"BertMultiTask\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "BertMultiTaskBody (BertMulti multiple                  4081928   \n",
      "_________________________________________________________________\n",
      "BertMultiTaskTop (BertMultiT multiple                  13231453  \n",
      "_________________________________________________________________\n",
      "mean_acc (Mean)              multiple                  2         \n",
      "=================================================================\n",
      "Total params: 17,313,383\n",
      "Trainable params: 17,313,377\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "model = train_bert_multitask(\n",
    "    problem=problem,\n",
    "    num_epochs=1,\n",
    "    params=params,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict,\n",
    "    continue_training=True\n",
    ")\n",
    "\n",
    "# fresh train\n",
    "_ = train_bert_multitask(\n",
    "    problem=problem,\n",
    "    num_epochs=1,\n",
    "    params=params,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict,\n",
    "    steps_per_epoch=1,\n",
    "    continue_training=False,\n",
    "    mirrored_strategy=False,\n",
    "    model_dir='./models/fresh_train'\n",
    ")\n",
    "\n",
    "# transfer train\n",
    "params.init_checkpoint = './models/fresh_train'\n",
    "_ = train_bert_multitask(\n",
    "    problem=problem,\n",
    "    num_epochs=1,\n",
    "    params=params,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict,\n",
    "    steps_per_epoch=1,\n",
    "    continue_training=False,\n",
    "    mirrored_strategy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def trim_checkpoint_for_prediction(problem: str,\n",
    "                                   input_dir: str,\n",
    "                                   output_dir: str,\n",
    "                                   problem_type_dict: Dict[str, str] = None,\n",
    "                                   overwrite=True,\n",
    "                                   fake_input_list=None,\n",
    "                                   params=None):\n",
    "    \"\"\"Minimize checkpoint size for prediction.\n",
    "\n",
    "    Since the original checkpoint contains optimizer's variable,\n",
    "        for instance, if the use adam, the checkpoint size will \n",
    "        be three times of the size of model weights. This function \n",
    "        will remove those unused variables in prediction to save space.\n",
    "\n",
    "    Note: if the model is a multimodal model, you have to provide fake_input_list that\n",
    "        mimic the structure of real input.\n",
    "\n",
    "    Args:\n",
    "        problem (str): problem\n",
    "        input_dir (str): input dir\n",
    "        output_dir (str): output dir\n",
    "        problem_type_dict (Dict[str, str], optional): problem type dict. Defaults to None.\n",
    "        fake_input_list (List): fake input list to create dummy dataset\n",
    "    \"\"\"\n",
    "    if overwrite and os.path.exists(output_dir):\n",
    "        rmtree(output_dir)\n",
    "    copytree(input_dir, output_dir, ignore=ignore_patterns(\n",
    "        'checkpoint', '*.index', '*.data-000*'))\n",
    "    base_dir, dir_name = os.path.split(output_dir)\n",
    "    if params is None:\n",
    "        params = DynamicBatchSizeParams()\n",
    "    params.add_multiple_problems(problem_type_dict=problem_type_dict)\n",
    "    params.from_json(os.path.join(input_dir, 'params.json'))\n",
    "    params.assign_problem(problem, base_dir=base_dir,\n",
    "                          dir_name=dir_name, predicting=True)\n",
    "\n",
    "    model = BertMultiTask(params)\n",
    "    if fake_input_list is None:\n",
    "        dummy_dataset = predict_input_fn(['fake']*5, params)\n",
    "    else:\n",
    "        dummy_dataset = predict_input_fn(fake_input_list*5, params)\n",
    "    _ = model(next(dummy_dataset.as_numpy_iterator()),\n",
    "              mode=tf.estimator.ModeKeys.PREDICT)\n",
    "    model.load_weights(os.path.join(input_dir, 'model'))\n",
    "    model.save_weights(os.path.join(params.ckpt_dir, 'model'))\n",
    "    params.to_json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim checkpoints\n",
    "\n",
    "The checkpoints contains optimizers' states which is not needed once training is done and it makes the checkpoint size two times larger. We provide an api to trim down the size of checkpoint by removing optimizers' states.\n",
    "\n",
    "Note: in multimodal setting, you need to provide a fake input to build the model correctly. Otherwise modal embeddings will be randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Adding new problem weibo_fake_ner, problem type: seq_tag\n",
      "Adding new problem weibo_cws, problem type: seq_tag\n",
      "Adding new problem weibo_fake_multi_cls, problem type: multi_cls\n",
      "Adding new problem weibo_fake_cls, problem type: cls\n",
      "Adding new problem weibo_masklm, problem type: masklm\n",
      "Adding new problem weibo_pretrain, problem type: pretrain\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n"
     ]
    }
   ],
   "source": [
    "# fake inputs\n",
    "import numpy as np\n",
    "fake_inputs = [{'text': 'test', 'image': np.random.uniform(\n",
    "            size=(5, 10))} for _ in range(5)] \n",
    "trim_checkpoint_for_prediction(\n",
    "    problem=problem, input_dir=model.params.ckpt_dir,\n",
    "    output_dir=model.params.ckpt_dir+'_pred',\n",
    "    problem_type_dict=problem_type_dict, overwrite=True, fake_input_list=fake_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def eval_bert_multitask(\n",
    "        problem='weibo_ner',\n",
    "        num_gpus=1,\n",
    "        model_dir='',\n",
    "        params=None,\n",
    "        problem_type_dict=None,\n",
    "        processing_fn_dict=None,\n",
    "        model=None):\n",
    "    \"\"\"Evaluate Multi-task Bert model\n",
    "\n",
    "    Available eval_scheme:\n",
    "        ner, cws, acc\n",
    "\n",
    "    Keyword Arguments:\n",
    "        problem {str} -- problems to evaluate (default: {'weibo_ner'})\n",
    "        num_gpus {int} -- number of gpu to use (default: {1})\n",
    "        model_dir {str} -- model dir (default: {''})\n",
    "        eval_scheme {str} -- Evaluation scheme (default: {'ner'})\n",
    "        params {Params} -- params to define model (default: {DynamicBatchSizeParams()})\n",
    "        problem_type_dict {dict} -- Key: problem name, value: problem type (default: {{}})\n",
    "        processing_fn_dict {dict} -- Key: problem name, value: problem data preprocessing fn (default: {{}})\n",
    "    \"\"\"\n",
    "    if not model_dir and params is not None:\n",
    "        model_dir = params.ckpt_dir\n",
    "    params = get_params_ready(problem, num_gpus, model_dir,\n",
    "                              params, problem_type_dict, processing_fn_dict,\n",
    "                              mode='predict', json_path=os.path.join(model_dir, 'params.json'))\n",
    "    eval_dataset = train_eval_input_fn(params, mode=EVAL)\n",
    "    one_batch_data = next(eval_dataset.as_numpy_iterator())\n",
    "    eval_dataset = train_eval_input_fn(params, mode=EVAL)\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    model = create_keras_model(\n",
    "        mirrored_strategy=mirrored_strategy, params=params, mode='eval', inputs_to_build_model=one_batch_data)\n",
    "    eval_dict = model.evaluate(eval_dataset, return_dict=True)\n",
    "    return eval_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\n",
    "Now we can use the trimmed checkpoint to do evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Adding new problem weibo_fake_ner, problem type: seq_tag\n",
      "Adding new problem weibo_cws, problem type: seq_tag\n",
      "Adding new problem weibo_fake_multi_cls, problem type: multi_cls\n",
      "Adding new problem weibo_fake_cls, problem type: cls\n",
      "Adding new problem weibo_masklm, problem type: masklm\n",
      "Adding new problem weibo_pretrain, problem type: pretrain\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 16.5940 - mean_acc: 0.6559 - weibo_fake_cls_acc: 0.6000 - weibo_fake_ner_acc: 0.6994\n",
      "Adding new problem weibo_fake_ner, problem type: seq_tag\n",
      "Adding new problem weibo_cws, problem type: seq_tag\n",
      "Adding new problem weibo_fake_multi_cls, problem type: multi_cls\n",
      "Adding new problem weibo_fake_cls, problem type: cls\n",
      "Adding new problem weibo_masklm, problem type: masklm\n",
      "Adding new problem weibo_pretrain, problem type: pretrain\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "WARNING:root:Share embedding is enabled but hidden_size != embedding_size\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "9/9 [==============================] - 3s 344ms/step - loss: 16.5940 - mean_acc: 0.6559 - weibo_fake_cls_acc: 0.6000 - weibo_fake_ner_acc: 0.6994\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'loss': 16.59403419494629,\n",
       " 'mean_acc': 0.6558765769004822,\n",
       " 'weibo_fake_cls_acc': 0.6000000238418579,\n",
       " 'weibo_fake_ner_acc': 0.699386477470398}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "eval_bert_multitask(problem=problem, params=params,\n",
    "                    problem_type_dict=problem_type_dict, processing_fn_dict=processing_fn_dict,\n",
    "                    model_dir=model.params.ckpt_dir+'_pred')\n",
    "\n",
    "# provide model instead of dir\n",
    "eval_bert_multitask(problem=problem, params=params,\n",
    "                    problem_type_dict=problem_type_dict, processing_fn_dict=processing_fn_dict,\n",
    "                    model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def predict_bert_multitask(\n",
    "        inputs,\n",
    "        problem='weibo_ner',\n",
    "        model_dir='',\n",
    "        params: BaseParams = None,\n",
    "        problem_type_dict: Dict[str, str] = None,\n",
    "        processing_fn_dict: Dict[str, Callable] = None,\n",
    "        model: tf.keras.Model = None,\n",
    "        return_model=False):\n",
    "    \"\"\"Evaluate Multi-task Bert model\n",
    "\n",
    "    Available eval_scheme:\n",
    "        ner, cws, acc\n",
    "\n",
    "    Keyword Arguments:\n",
    "        problem {str} -- problems to evaluate (default: {'weibo_ner'})\n",
    "        num_gpus {int} -- number of gpu to use (default: {1})\n",
    "        model_dir {str} -- model dir (default: {''})\n",
    "        eval_scheme {str} -- Evaluation scheme (default: {'ner'})\n",
    "        params {Params} -- params to define model (default: {DynamicBatchSizeParams()})\n",
    "        problem_type_dict {dict} -- Key: problem name, value: problem type (default: {{}})\n",
    "        processing_fn_dict {dict} -- Key: problem name, value: problem data preprocessing fn (default: {{}})\n",
    "    \"\"\"\n",
    "\n",
    "    if params is None:\n",
    "        params = DynamicBatchSizeParams()\n",
    "    if not model_dir and params is not None:\n",
    "        model_dir = params.ckpt_dir\n",
    "    params = get_params_ready(problem, 1, model_dir,\n",
    "                              params, problem_type_dict, processing_fn_dict,\n",
    "                              mode='predict', json_path=os.path.join(model_dir, 'params.json'))\n",
    "\n",
    "    LOGGER.info('Checkpoint dir: %s', params.ckpt_dir)\n",
    "    time.sleep(3)\n",
    "\n",
    "    pred_dataset = predict_input_fn(inputs, params)\n",
    "    one_batch_data = next(pred_dataset.as_numpy_iterator())\n",
    "    pred_dataset = predict_input_fn(inputs, params)\n",
    "\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    if model is None:\n",
    "        model = create_keras_model(\n",
    "            mirrored_strategy=mirrored_strategy, params=params, mode='predict', inputs_to_build_model=one_batch_data)\n",
    "\n",
    "    with mirrored_strategy.scope():\n",
    "        pred = model.predict(pred_dataset)\n",
    "\n",
    "    if return_model:\n",
    "        return pred, model\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "We can do prediction by providing list of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Adding new problem weibo_fake_ner, problem type: seq_tag\n",
      "Adding new problem weibo_cws, problem type: seq_tag\n",
      "Adding new problem weibo_fake_multi_cls, problem type: multi_cls\n",
      "Adding new problem weibo_fake_cls, problem type: cls\n",
      "Adding new problem weibo_masklm, problem type: masklm\n",
      "Adding new problem weibo_pretrain, problem type: pretrain\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "pred, model = predict_bert_multitask(\n",
    "    problem='weibo_fake_ner',\n",
    "    inputs=fake_inputs*20, model_dir=model.params.ckpt_dir,\n",
    "    problem_type_dict=problem_type_dict,\n",
    "    processing_fn_dict=processing_fn_dict, return_model=True,\n",
    "    params=params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}