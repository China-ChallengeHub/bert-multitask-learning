{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp bert_preprocessing.create_bert_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bert Features\n",
    "\n",
    "Module to convert preprocessing function outputs to model input, including\n",
    "\n",
    "- convert texts to token id using transformers tokenizers\n",
    "- convert labels to label ids / token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from bert_multitask_learning.special_tokens import PREDICT\n",
    "from bert_multitask_learning.bert_preprocessing.bert_utils import (create_instances_from_document)\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "LOGGER = tf.get_logger()\n",
    "\n",
    "def seq_tag_label_handling(tokenized_dict, target, pad_token):\n",
    "    special_token_mask = tokenized_dict['special_tokens_mask']\n",
    "    del tokenized_dict['special_tokens_mask']\n",
    "\n",
    "    # handle truncation\n",
    "    if tokenized_dict.get('num_truncated_tokens', 0) > 0:\n",
    "        target = target[:len(target) - tokenized_dict['num_truncated_tokens']]\n",
    "\n",
    "    processed_target = []\n",
    "    for m in special_token_mask:\n",
    "        # 0 is special tokens, 1 is tokens\n",
    "        if m == 1:\n",
    "            # add pad\n",
    "            processed_target.append(pad_token)\n",
    "        else:\n",
    "            processed_target.append(target.pop(0))\n",
    "    return processed_target, tokenized_dict\n",
    "\n",
    "\n",
    "def pad_wrapper(inp, target_len=90):\n",
    "    if len(inp) >= target_len:\n",
    "        return inp[:target_len]\n",
    "    else:\n",
    "        return inp + [0]*(target_len - len(inp))\n",
    "\n",
    "\n",
    "def convert_labels_to_ids(target, problem_type, label_encoder, tokenizer=None, decoding_length=None, custom_label_handling_fn=None):\n",
    "    label_mask = None\n",
    "    if custom_label_handling_fn is not None:\n",
    "        return custom_label_handling_fn(target, label_encoder, tokenizer, decoding_length)\n",
    "    if isinstance(target, list):\n",
    "        if problem_type == 'seq2seq_text':\n",
    "\n",
    "            target = [label_encoder.bos_token] + \\\n",
    "                target + [label_encoder.eos_token]\n",
    "            label_dict = label_encoder(\n",
    "                target, add_special_tokens=False, is_split_into_words=True)\n",
    "            label_id = label_dict['input_ids']\n",
    "            label_mask = label_dict['attention_mask']\n",
    "            label_id = pad_wrapper(label_id, decoding_length)\n",
    "            label_mask = pad_wrapper(label_mask, decoding_length)\n",
    "\n",
    "        elif problem_type == 'multi_cls':\n",
    "            label_id = label_encoder.transform([target])[0]\n",
    "        elif problem_type == 'seq2seq_tag':\n",
    "            # seq2seq_tag\n",
    "            target = [label_encoder.bos_token] + \\\n",
    "                target + [label_encoder.eos_token]\n",
    "            label_dict = tokenizer(\n",
    "                target, is_split_into_words=True, add_special_tokens=False)\n",
    "            label_mask = label_dict['attention_mask']\n",
    "            label_id = label_encoder.transform(target).tolist()\n",
    "            label_id = [np.int32(i) for i in label_id]\n",
    "        else:\n",
    "            label_id = label_encoder.transform(target).tolist()\n",
    "            label_id = [np.int32(i) for i in label_id]\n",
    "    else:\n",
    "        if problem_type == 'seq2seq_text':\n",
    "            target = label_encoder.bos_token + target + label_encoder.eos_token\n",
    "            label_dict = label_encoder(\n",
    "                target, add_special_tokens=False, is_split_into_words=False)\n",
    "            label_id = label_dict['input_ids']\n",
    "            label_mask = label_dict['attention_mask']\n",
    "            label_id = pad_wrapper(label_id, decoding_length)\n",
    "            label_mask = pad_wrapper(label_mask, decoding_length)\n",
    "        else:\n",
    "            label_id = label_encoder.transform([target]).tolist()[0]\n",
    "            label_id = np.int32(label_id)\n",
    "    return label_id, label_mask\n",
    "\n",
    "\n",
    "def _create_bert_features(problem,\n",
    "                          example_list,\n",
    "                          label_encoder,\n",
    "                          params,\n",
    "                          tokenizer: PreTrainedTokenizer,\n",
    "                          mode,\n",
    "                          problem_type,\n",
    "                          is_seq):\n",
    "    is_mask_lm = problem_type == 'masklm'\n",
    "\n",
    "    for example_id, example in enumerate(example_list):\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            raw_inputs, raw_target = example\n",
    "        else:\n",
    "            raw_inputs = example\n",
    "            raw_target = None\n",
    "\n",
    "        # # tokenize inputs, now the length is fixed, target == raw_target\n",
    "        if isinstance(raw_inputs, dict):\n",
    "            tokens_a = raw_inputs['a']\n",
    "            tokens_b = raw_inputs['b']\n",
    "        else:\n",
    "            tokens_a = raw_inputs\n",
    "            tokens_b = None\n",
    "\n",
    "        target = raw_target\n",
    "\n",
    "        if is_mask_lm:\n",
    "            tokenized_dict, mlm_feature_dict = mask_inputs_for_mask_lm(\n",
    "                tokens_a, tokenizer, mask_prob=params.masked_lm_prob,\n",
    "                max_length=params.max_seq_len, max_predictions_per_seq=params.max_predictions_per_seq)\n",
    "            if tokenized_dict is None:\n",
    "                # hacky approach to continue outer loop\n",
    "                continue\n",
    "        else:\n",
    "            tokenized_dict = tokenizer.encode_plus(\n",
    "                tokens_a, tokens_b,\n",
    "                truncation=True,\n",
    "                max_length=params.max_seq_len,\n",
    "                is_split_into_words=False,\n",
    "                padding=False,\n",
    "                return_special_tokens_mask=is_seq,\n",
    "                add_special_tokens=True,\n",
    "                return_overflowing_tokens=True)\n",
    "\n",
    "        # check whether tokenization changed the length\n",
    "        if is_seq:\n",
    "            target, tokenized_dict = seq_tag_label_handling(\n",
    "                tokenized_dict, target, '[PAD]')\n",
    "\n",
    "            if len(target) != len(tokenized_dict['input_ids']):\n",
    "                raise ValueError(\n",
    "                    'Length is different for seq tag problem, inputs: {}'.format(tokenizer.decode(tokenized_dict['input_ids'])))\n",
    "\n",
    "        if mode != PREDICT and not is_mask_lm:\n",
    "\n",
    "            custom_label_handling_fn = params.label_handling_fn.get(\n",
    "                problem_type, None)\n",
    "            label_id, label_mask = convert_labels_to_ids(\n",
    "                target, problem_type, label_encoder,\n",
    "                tokenizer, params.decode_max_seq_len, custom_label_handling_fn=custom_label_handling_fn)\n",
    "\n",
    "        input_ids = tokenized_dict['input_ids']\n",
    "        segment_ids = tokenized_dict['token_type_ids']\n",
    "        input_mask = tokenized_dict['attention_mask']\n",
    "        return_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'input_mask': input_mask,\n",
    "            'segment_ids': segment_ids\n",
    "        }\n",
    "        # create return dict\n",
    "        if mode != PREDICT:\n",
    "            if is_mask_lm:\n",
    "                return_dict.update(mlm_feature_dict)\n",
    "            else:\n",
    "                return_dict['%s_label_ids' % problem] = label_id\n",
    "\n",
    "        if problem_type in ['seq2seq_tag', 'seq2seq_text']:\n",
    "            return_dict['%s_mask' % problem] = label_mask\n",
    "\n",
    "        if example_id < 10:\n",
    "            if isinstance(raw_inputs, dict):\n",
    "                for raw_input_name, raw_input in raw_inputs.items():\n",
    "                    LOGGER.info('{}: {}'.format(\n",
    "                        raw_input_name, str(raw_input)[:200]))\n",
    "            else:\n",
    "                LOGGER.info(str(raw_inputs)[:200])\n",
    "            for return_key, return_item in return_dict.items():\n",
    "                LOGGER.info('{}: {}'.format(\n",
    "                    return_key, str(return_item)[:200]))\n",
    "        yield return_dict\n",
    "\n",
    "\n",
    "def create_bert_features(problem,\n",
    "                         example_list,\n",
    "                         label_encoder,\n",
    "                         params,\n",
    "                         tokenizer,\n",
    "                         mode,\n",
    "                         problem_type,\n",
    "                         is_seq):\n",
    "    if problem_type == 'pretrain':\n",
    "        return create_bert_pretraining(\n",
    "            problem=problem,\n",
    "            inputs_list=example_list,\n",
    "            label_encoder=label_encoder,\n",
    "            params=params,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "    gen = _create_bert_features(problem,\n",
    "                                example_list,\n",
    "                                label_encoder,\n",
    "                                params,\n",
    "                                tokenizer,\n",
    "                                mode,\n",
    "                                problem_type,\n",
    "                                is_seq)\n",
    "    return_dict_list = [d for d in gen]\n",
    "    return return_dict_list\n",
    "\n",
    "\n",
    "def create_bert_pretraining(problem,\n",
    "                            inputs_list,\n",
    "                            label_encoder,\n",
    "                            params,\n",
    "                            tokenizer\n",
    "                            ):\n",
    "    \"\"\"Slight modification of original code\n",
    "\n",
    "    Raises:\n",
    "        ValueError -- Input format not right\n",
    "    \"\"\"\n",
    "    if not isinstance(inputs_list[0][0], list):\n",
    "        raise ValueError('inputs is expected to be list of list of list.')\n",
    "\n",
    "    all_documents = []\n",
    "    for document in inputs_list:\n",
    "        all_documents.append([])\n",
    "        for sentence in document:\n",
    "            all_documents[-1].append(tokenizer.tokenize('\\t'.join(sentence)))\n",
    "\n",
    "    all_documents = [d for d in all_documents if d]\n",
    "    rng = random.Random()\n",
    "    rng.shuffle(all_documents)\n",
    "\n",
    "    vocab_words = list(tokenizer.vocab.keys())\n",
    "    instances = []\n",
    "\n",
    "    print_count = 0\n",
    "    return_list = []\n",
    "    for _ in range(params.dupe_factor):\n",
    "        for document_index in range(len(all_documents)):\n",
    "            instances = create_instances_from_document(\n",
    "                all_documents,\n",
    "                document_index,\n",
    "                params.max_seq_len,\n",
    "                params.short_seq_prob,\n",
    "                params.masked_lm_prob,\n",
    "                params.max_predictions_per_seq,\n",
    "                vocab_words, rng)\n",
    "            for instance in instances:\n",
    "                tokens = instance.tokens\n",
    "                segment_ids = list(instance.segment_ids)\n",
    "\n",
    "                masked_lm_positions = list(instance.masked_lm_positions)\n",
    "\n",
    "                next_sentence_label = 1 if instance.is_random_next else 0\n",
    "\n",
    "                mask_lm_dict = tokenizer(instance.masked_lm_labels,\n",
    "                                         truncation=False,\n",
    "                                         is_split_into_words=True,\n",
    "                                         padding='max_length',\n",
    "                                         max_length=params.max_predictions_per_seq,\n",
    "                                         return_special_tokens_mask=False,\n",
    "                                         add_special_tokens=False)\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                input_mask = [1 for _ in input_ids]\n",
    "                masked_lm_ids = mask_lm_dict['input_ids']\n",
    "                masked_lm_weights = mask_lm_dict['attention_mask']\n",
    "                masked_lm_positions = masked_lm_positions + \\\n",
    "                    masked_lm_ids[len(masked_lm_positions):]\n",
    "\n",
    "                assert len(input_ids) == len(\n",
    "                    segment_ids), (len(input_ids), len(segment_ids))\n",
    "                assert len(masked_lm_ids) == len(masked_lm_positions), (len(\n",
    "                    masked_lm_ids), len(masked_lm_positions))\n",
    "\n",
    "                yield_dict = {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"input_mask\": input_mask,\n",
    "                    \"segment_ids\": segment_ids,\n",
    "                    \"masked_lm_positions\": masked_lm_positions,\n",
    "                    \"masked_lm_ids\": masked_lm_ids,\n",
    "                    \"masked_lm_weights\": masked_lm_weights,\n",
    "                    \"next_sentence_label_ids\": next_sentence_label\n",
    "                }\n",
    "\n",
    "                if print_count < 3:\n",
    "                    tf.compat.v1.logging.debug('%s : %s' %\n",
    "                                               ('tokens', ' '.join([str(x) for x in tokens])))\n",
    "                    for k, v in yield_dict.items():\n",
    "                        if not isinstance(v, int):\n",
    "                            tf.compat.v1.logging.debug('%s : %s' %\n",
    "                                                       (k, ' '.join([str(x) for x in v])))\n",
    "                    print_count += 1\n",
    "\n",
    "                return_list.append(yield_dict)\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def mask_inputs_for_mask_lm(inp_text: str, tokenizer: PreTrainedTokenizer, mask_prob=0.1, max_length=128, max_predictions_per_seq=20) -> str:\n",
    "    if not inp_text:\n",
    "        return None, None\n",
    "    inp_text = list(inp_text)\n",
    "    mask_idx = [i for i in range(min(len(inp_text), max_length))\n",
    "                if random.uniform(0, 1) <= mask_prob]\n",
    "    if not mask_idx:\n",
    "        return None, None\n",
    "    masked_text = [inp_text[i] for i in mask_idx]\n",
    "    inp_text = [t if i not in mask_idx else '[MASK]' for i,\n",
    "                t in enumerate(inp_text)]\n",
    "\n",
    "    tokenized_dict = tokenizer(\n",
    "        inp_text, None,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        is_split_into_words=True,\n",
    "        padding=False,\n",
    "        return_special_tokens_mask=False,\n",
    "        add_special_tokens=True,\n",
    "        return_overflowing_tokens=True)\n",
    "\n",
    "    # create mask lm features\n",
    "    mask_lm_dict = tokenizer(masked_text,\n",
    "                             truncation=True,\n",
    "                             is_split_into_words=True,\n",
    "                             padding='max_length',\n",
    "                             max_length=max_predictions_per_seq,\n",
    "                             return_special_tokens_mask=False,\n",
    "                             add_special_tokens=False,)\n",
    "\n",
    "    mask_token_id = tokenizer(\n",
    "        '[MASK]', add_special_tokens=False, is_split_into_words=False)['input_ids'][0]\n",
    "    masked_lm_positions = [i for i, input_id in enumerate(\n",
    "        tokenized_dict['input_ids']) if input_id == mask_token_id]\n",
    "    # pad masked_lm_positions to max_predictions_per_seq\n",
    "    if len(masked_lm_positions) < max_predictions_per_seq:\n",
    "        masked_lm_positions = masked_lm_positions + \\\n",
    "            [0 for _ in range(max_predictions_per_seq -\n",
    "                              len(masked_lm_positions))]\n",
    "    masked_lm_positions = masked_lm_positions[:max_predictions_per_seq]\n",
    "    masked_lm_ids = np.array(mask_lm_dict['input_ids'], dtype='int32')\n",
    "    masked_lm_weights = np.array(mask_lm_dict['attention_mask'], dtype='int32')\n",
    "    mask_lm_dict = {'masked_lm_positions': masked_lm_positions,\n",
    "                    'masked_lm_ids': masked_lm_ids,\n",
    "                    'masked_lm_weights': masked_lm_weights}\n",
    "\n",
    "    return tokenized_dict, mask_lm_dict\n",
    "\n",
    "\n",
    "def _create_multimodal_bert_features(problem,\n",
    "                                     example_list,\n",
    "                                     label_encoder,\n",
    "                                     params,\n",
    "                                     tokenizer,\n",
    "                                     mode,\n",
    "                                     problem_type,\n",
    "                                     is_seq):\n",
    "    if problem_type == 'pretrain':\n",
    "        raise NotImplementedError('Multimodal Pretraining is not implemented')\n",
    "\n",
    "    is_mask_lm = problem_type == 'masklm'\n",
    "\n",
    "    for example_id, example in enumerate(example_list):\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            raw_inputs, raw_target = example\n",
    "        else:\n",
    "            raw_inputs = example\n",
    "            raw_target = None\n",
    "\n",
    "        if problem_type == 'seq_tag' and not isinstance(raw_target, dict):\n",
    "            raise ValueError(\n",
    "                'Label of multimodal sequence tagging must be a dictionary')\n",
    "\n",
    "        if not isinstance(raw_inputs, dict):\n",
    "            raise ValueError(\n",
    "                'Multimodal inputs is supposed to be a dictionary')\n",
    "\n",
    "        if isinstance(raw_target, dict):\n",
    "            target_by_modal = True\n",
    "        else:\n",
    "            target_by_modal = False\n",
    "\n",
    "        modal_name_list = ['text', 'image', 'others']\n",
    "\n",
    "        return_dict = {}\n",
    "        try:\n",
    "            for modal_name in modal_name_list:\n",
    "                if modal_name not in raw_inputs:\n",
    "                    continue\n",
    "\n",
    "                modal_inputs = raw_inputs[modal_name]\n",
    "\n",
    "                if target_by_modal:\n",
    "                    modal_target = raw_target[modal_name]\n",
    "                else:\n",
    "                    modal_target = raw_target\n",
    "\n",
    "                if modal_name == 'text':\n",
    "                    # tokenize inputs, now the length is fixed, target == raw_target\n",
    "                    if isinstance(modal_inputs, dict):\n",
    "                        tokens_a = modal_inputs['a']\n",
    "                        tokens_b = modal_inputs['b']\n",
    "                    else:\n",
    "                        tokens_a = modal_inputs\n",
    "                        tokens_b = None\n",
    "                    target = modal_target\n",
    "                    if is_mask_lm:\n",
    "                        tokenized_dict, mlm_feature_dict = mask_inputs_for_mask_lm(\n",
    "                            tokens_a, tokenizer, mask_prob=params.masked_lm_prob,\n",
    "                            max_length=params.max_seq_len, max_predictions_per_seq=params.max_predictions_per_seq)\n",
    "                        if tokenized_dict is None:\n",
    "                            # hacky approach to continue outer loop\n",
    "                            raise NotImplementedError\n",
    "                    else:\n",
    "                        mlm_feature_dict = {}\n",
    "\n",
    "                        if isinstance(tokens_a, list):\n",
    "                            is_split_into_words = True\n",
    "                        else:\n",
    "                            is_split_into_words = False\n",
    "                        tokenized_dict = tokenizer.encode_plus(\n",
    "                            tokens_a, tokens_b,\n",
    "                            truncation=True,\n",
    "                            max_length=params.max_seq_len,\n",
    "                            is_split_into_words=False,\n",
    "                            padding=False,\n",
    "                            return_special_tokens_mask=is_seq,\n",
    "                            add_special_tokens=True,\n",
    "                            return_overflowing_tokens=True)\n",
    "\n",
    "                    if is_seq:\n",
    "                        target, tokenized_dict = seq_tag_label_handling(\n",
    "                            tokenized_dict, target, tokenizer.pad_token)\n",
    "\n",
    "                        if len(target) != len(tokenized_dict['input_ids']):\n",
    "                            raise ValueError(\n",
    "                                'Length is different for seq tag problem, inputs: {}'.format(tokenizer.decode(tokenized_dict['input_ids'])))\n",
    "\n",
    "                    input_ids = tokenized_dict['input_ids']\n",
    "                    segment_ids = tokenized_dict['token_type_ids']\n",
    "                    input_mask = tokenized_dict['attention_mask']\n",
    "\n",
    "                    modal_feature_dict = {\n",
    "                        'input_ids': input_ids,\n",
    "                        'input_mask': input_mask,\n",
    "                        'segment_ids': segment_ids\n",
    "                    }\n",
    "                    modal_feature_dict.update(mlm_feature_dict)\n",
    "\n",
    "                else:\n",
    "                    modal_inputs = np.array(modal_inputs)\n",
    "                    if len(modal_inputs.shape) == 1:\n",
    "                        modal_inputs = np.expand_dims(modal_inputs, axis=0)\n",
    "                    target = modal_target\n",
    "                    segment_ids = np.zeros(\n",
    "                        modal_inputs.shape[0], dtype=np.int32) + params.modal_segment_id[modal_name]\n",
    "                    input_mask = [1]*len(modal_inputs)\n",
    "                    modal_feature_dict = {\n",
    "                        '{}_input'.format(modal_name): modal_inputs,\n",
    "                        '{}_mask'.format(modal_name): input_mask,\n",
    "                        '{}_segment_ids'.format(modal_name): segment_ids}\n",
    "\n",
    "                # encode labels\n",
    "                if mode != PREDICT:\n",
    "                    if not is_mask_lm:\n",
    "                        custom_label_handling_fn = params.label_handling_fn.get(\n",
    "                            problem_type, None)\n",
    "                        label_id, label_mask = convert_labels_to_ids(\n",
    "                            target, problem_type, label_encoder, tokenizer, params.decode_max_seq_len, custom_label_handling_fn=custom_label_handling_fn)\n",
    "\n",
    "                        if target_by_modal:\n",
    "                            modal_feature_dict['{}_{}_label_ids'.format(\n",
    "                                problem, modal_name)] = label_id\n",
    "                        else:\n",
    "                            modal_feature_dict['{}_label_ids'.format(\n",
    "                                problem)] = label_id\n",
    "                return_dict.update(modal_feature_dict)\n",
    "\n",
    "        except NotImplementedError:\n",
    "            continue\n",
    "\n",
    "        if problem_type in ['seq2seq_tag', 'seq2seq_text']:\n",
    "            return_dict['%s_mask' % problem] = label_mask\n",
    "\n",
    "        if example_id < 10:\n",
    "            if isinstance(raw_inputs, dict):\n",
    "                for raw_input_name, raw_input in raw_inputs.items():\n",
    "                    LOGGER.info('{}: {}'.format(\n",
    "                        raw_input_name, str(raw_input)[:200]))\n",
    "            else:\n",
    "                LOGGER.info(str(raw_inputs)[:200])\n",
    "            for return_key, return_item in return_dict.items():\n",
    "                LOGGER.info('{}: {}'.format(\n",
    "                    return_key, str(return_item)[:200]))\n",
    "        yield return_dict\n",
    "\n",
    "\n",
    "def create_multimodal_bert_features(problem,\n",
    "                                    example_list,\n",
    "                                    label_encoder,\n",
    "                                    params,\n",
    "                                    tokenizer,\n",
    "                                    mode,\n",
    "                                    problem_type,\n",
    "                                    is_seq):\n",
    "    if problem_type == 'pretrain':\n",
    "        raise NotImplementedError(\"Multimodal pretraining is not implemented\")\n",
    "    gen = _create_multimodal_bert_features(problem,\n",
    "                                           example_list,\n",
    "                                           label_encoder,\n",
    "                                           params,\n",
    "                                           tokenizer,\n",
    "                                           mode,\n",
    "                                           problem_type,\n",
    "                                           is_seq)\n",
    "    return_dict_list = [d for d in gen]\n",
    "    return return_dict_list\n",
    "\n",
    "\n",
    "def create_bert_features_generator(problem,\n",
    "                                   example_list,\n",
    "                                   label_encoder,\n",
    "                                   params,\n",
    "                                   tokenizer,\n",
    "                                   mode,\n",
    "                                   problem_type,\n",
    "                                   is_seq):\n",
    "    if problem_type == 'pretrain':\n",
    "        raise ValueError('pretraining does not support generator')\n",
    "    gen = _create_bert_features(problem,\n",
    "                                example_list,\n",
    "                                label_encoder,\n",
    "                                params,\n",
    "                                tokenizer,\n",
    "                                mode,\n",
    "                                problem_type,\n",
    "                                is_seq)\n",
    "    return gen\n",
    "\n",
    "\n",
    "def create_multimodal_bert_features_generator(problem,\n",
    "                                              example_list,\n",
    "                                              label_encoder,\n",
    "                                              params,\n",
    "                                              tokenizer,\n",
    "                                              mode,\n",
    "                                              problem_type,\n",
    "                                              is_seq):\n",
    "    if problem_type == 'pretrain':\n",
    "        raise ValueError('pretraining does not support generator')\n",
    "    gen = _create_multimodal_bert_features(problem,\n",
    "                                           example_list,\n",
    "                                           label_encoder,\n",
    "                                           params,\n",
    "                                           tokenizer,\n",
    "                                           mode,\n",
    "                                           problem_type,\n",
    "                                           is_seq)\n",
    "    return gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}