{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params\n",
    "\n",
    "Params is the major object to control the whole modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import logging\n",
    "from typing import Callable, List, Tuple, Dict, Union\n",
    "from collections import defaultdict\n",
    "from distutils.dir_util import copy_tree\n",
    "import tensorflow as tf\n",
    "\n",
    "from bert_multitask_learning.utils import create_path, load_transformer_tokenizer, load_transformer_config\n",
    "from bert_multitask_learning.special_tokens import BOS_TOKEN, EOS_TOKEN\n",
    "\n",
    "\n",
    "class BaseParams():\n",
    "    # pylint: disable=attribute-defined-outside-init\n",
    "    def __init__(self):\n",
    "        self.run_problem_list = []\n",
    "\n",
    "        self.problem_type = {\n",
    "        }\n",
    "\n",
    "        # transformers params\n",
    "        self.transformer_model_name = 'bert-base-chinese'\n",
    "        self.transformer_tokenizer_name = 'bert-base-chinese'\n",
    "        self.transformer_config_name = 'bert-base-chinese'\n",
    "        self.transformer_model_loading = 'TFAutoModel'\n",
    "        self.transformer_config_loading = 'AutoConfig'\n",
    "        self.transformer_tokenizer_loading = 'AutoTokenizer'\n",
    "        self.transformer_decoder_model_name = None\n",
    "        self.transformer_decoder_config_name = None\n",
    "        self.transformer_decoder_tokenizer_name = None\n",
    "        # self.transformer_decoder_model_name = \"hfl/chinese-xlnet-base\"\n",
    "        # self.transformer_decoder_config_name = \"hfl/chinese-xlnet-base\"\n",
    "        # self.transformer_decoder_tokenizer_name = \"hfl/chinese-xlnet-base\"\n",
    "        self.transformer_decoder_model_loading = 'TFAutoModel'\n",
    "        self.transformer_decoder_config_loading = 'AutoConfig'\n",
    "        self.transformer_decoder_tokenizer_loading = 'AutoTokenizer'\n",
    "\n",
    "        # multimodal params\n",
    "        self.modal_segment_id = {\n",
    "            'text': 0,\n",
    "            'image': 0,\n",
    "            'others': 0\n",
    "        }\n",
    "        self.modal_type_id = {\n",
    "            'text': 0,\n",
    "            'image': 1,\n",
    "            'others': 2\n",
    "        }\n",
    "        self.enable_modal_type = False\n",
    "        # bert config\n",
    "        self.init_checkpoint = ''\n",
    "\n",
    "        # specify this will make key reuse values top\n",
    "        # that it, weibo_ner problem will use NER's top\n",
    "        self.share_top = {\n",
    "        }\n",
    "        for p in self.problem_type:\n",
    "            if p not in self.share_top:\n",
    "                self.share_top[p] = p\n",
    "\n",
    "        self.multitask_balance_type = 'data_balanced'\n",
    "        self.problem_type_list = ['cls', 'seq_tag', 'seq2seq_tag',\n",
    "                                  'seq2seq_text', 'multi_cls', 'pretrain', 'masklm']\n",
    "        self.predefined_problem_type = ['cls', 'seq_tag', 'seq2seq_tag',\n",
    "                                        'seq2seq_text', 'multi_cls', 'pretrain', 'masklm']\n",
    "        self.get_or_make_label_encoder_fn_dict: Dict[str, Callable] = {}\n",
    "        self.label_handling_fn: Dict[str, Callable] = {}\n",
    "        self.top_layer = {}\n",
    "        self.num_classes = {}\n",
    "        # self.multitask_balance_type = 'problem_balanced'\n",
    "\n",
    "        # logging control\n",
    "        self.log_every_n_steps = 100\n",
    "        self.detail_log = True\n",
    "\n",
    "        self.multiprocess = True\n",
    "        self.num_cpus = 4\n",
    "        self.per_cpu_buffer = 3000\n",
    "        self.decode_vocab_file = None\n",
    "        self.eval_throttle_secs = 600\n",
    "\n",
    "        # training\n",
    "        self.init_lr = 2e-5\n",
    "        self.batch_size = 32\n",
    "        self.train_epoch = 15\n",
    "        self.freeze_step = 0\n",
    "        self.prefetch = 5000\n",
    "        self.dynamic_padding = True\n",
    "        self.bucket_batch_sizes = [32, 32, 32, 16]\n",
    "        self.bucket_boundaries = [30, 64, 128]\n",
    "        self.shuffle_buffer = 200000\n",
    "\n",
    "        # hparm\n",
    "        self.dropout_keep_prob = 0.9\n",
    "        self.max_seq_len = 256\n",
    "        self.use_one_hot_embeddings = True\n",
    "        self.label_smoothing = 0.0\n",
    "        self.crf = False\n",
    "        self.bert_num_hidden_layer = 12\n",
    "        self.hidden_dense = False\n",
    "        # threshold to calculate metrics for multi_cls\n",
    "        self.multi_cls_threshold = 0.5\n",
    "        self.multi_cls_positive_weight = 1.0\n",
    "        self.custom_pooled_hidden_size = 0\n",
    "        self.share_embedding = True\n",
    "\n",
    "        # seq2seq\n",
    "        self.decoder_num_hidden_layers = 3\n",
    "        self.beam_size = 10\n",
    "        self.init_decoder_from_encoder = False\n",
    "        self.beam_search_alpha = 0.6\n",
    "        self.decode_max_seq_len = 90\n",
    "\n",
    "        # experimental multitask approach\n",
    "        self.label_transfer = False\n",
    "        # train mask lm and downstream task at the same time\n",
    "        self.augument_mask_lm = False\n",
    "        self.augument_rate = 0.5\n",
    "        # NOT implemented\n",
    "        self.distillation = False\n",
    "        # Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\n",
    "        # ref: https://arxiv.org/abs/1705.07115\n",
    "        self.uncertain_weight_loss = False\n",
    "        # dep since not good\n",
    "        # self.mutual_prediction = False\n",
    "\n",
    "        # add an extra attention for each task\n",
    "        #   with BERT layers as encoder output, task logits as decoder inputs\n",
    "        self.grid_transformer = False\n",
    "\n",
    "        # add an extra attention for each task\n",
    "        #   with other tasks' logits as encoder output, task logits asn decoder inputs\n",
    "        self.task_transformer = False\n",
    "\n",
    "        # do a mean for gradients of BERT layers instead of sum\n",
    "        self.mean_gradients = False\n",
    "\n",
    "        # random replace punctuation by some prob to\n",
    "        # ease the punctuation sensitive problem\n",
    "        self.punc_replace_prob = 0.0\n",
    "        self.punc_list = list(',.!?！。？，、')\n",
    "        self.hidden_gru = False\n",
    "        self.label_transfer_gru = False\n",
    "        # if None, we will use the same hidden_size as inputs\n",
    "        # e.g. # of labels\n",
    "        self.label_transfer_gru_hidden_size = None\n",
    "\n",
    "        # pretrain hparm\n",
    "        self.dupe_factor = 10\n",
    "        self.short_seq_prob = 0.1\n",
    "        self.masked_lm_prob = 0.15\n",
    "        self.max_predictions_per_seq = 20\n",
    "        self.mask_lm_hidden_size = 768\n",
    "        self.mask_lm_hidden_act = 'gelu'\n",
    "        self.mask_lm_initializer_range = 0.02\n",
    "\n",
    "        self.train_problem = None\n",
    "        self.tmp_file_dir = 'tmp'\n",
    "        self.cache_dir = 'models/transformers_cache'\n",
    "        # get generator function for each problem\n",
    "        self.read_data_fn = {}\n",
    "        self.problem_assigned = False\n",
    "\n",
    "    def add_problem(self, problem_name: str, problem_type='cls', processing_fn: Callable = None):\n",
    "        \"\"\"Add problems.\n",
    "\n",
    "        Args:\n",
    "            problem_name (str): problem name. \n",
    "            problem_type (str, optional): One of the following problem types:\n",
    "                ['cls', 'seq_tag', 'seq2seq_tag', 'seq2seq_text', 'multi_cls', 'pretrain']. \n",
    "                Defaults to 'cls'.\n",
    "            processing_fn (Callable, optional): preprocessing function. Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: unexpected problem_type\n",
    "        \"\"\"\n",
    "\n",
    "        if problem_type not in self.problem_type_list:\n",
    "            raise ValueError('Provided problem type not valid, expect {0}, got {1}'.format(\n",
    "                self.problem_type_list,\n",
    "                problem_type))\n",
    "\n",
    "        self.problem_type[problem_name] = problem_type\n",
    "        self.read_data_fn[problem_name] = processing_fn\n",
    "\n",
    "    def add_multiple_problems(self, problem_type_dict: Dict[str, str], processing_fn_dict: Dict[str, Callable] = None):\n",
    "        \"\"\"add multiple problems.\n",
    "        processing_fn_dict is optional, if it's not provided, processing fn will be set as None.\n",
    "\n",
    "        Args:\n",
    "            problem_type_dict (Dict[str, str]): problem type dict\n",
    "            processing_fn_dict (Dict[str, Callable], optional): problem type fn. Defaults to None.\n",
    "        \"\"\"\n",
    "        # add new problem to params if problem_type_dict and processing_fn_dict provided\n",
    "        for new_problem, problem_type in problem_type_dict.items():\n",
    "            print('Adding new problem {0}, problem type: {1}'.format(\n",
    "                new_problem, problem_type_dict[new_problem]))\n",
    "            if processing_fn_dict:\n",
    "                new_problem_processing_fn = processing_fn_dict[new_problem]\n",
    "            else:\n",
    "                new_problem_processing_fn = None\n",
    "            self.add_problem(\n",
    "                problem_name=new_problem, problem_type=problem_type, processing_fn=new_problem_processing_fn)\n",
    "\n",
    "    def assign_problem(self,\n",
    "                       flag_string: str,\n",
    "                       gpu=2,\n",
    "                       base_dir: str = None,\n",
    "                       dir_name: str = None,\n",
    "                       predicting=False):\n",
    "        \"\"\"Assign the actual run problem to param. This function will\n",
    "        do the following things:\n",
    "\n",
    "        1. parse the flag string to form the run_problem_list\n",
    "        2. create checkpoint saving path\n",
    "        3. calculate total number of training data and training steps\n",
    "        4. scale learning rate with the number of gpu linearly\n",
    "\n",
    "        Arguments:\n",
    "            flag_string {str} -- run problem string\n",
    "            example: cws|POS|weibo_ner&weibo_cws\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gpu {int} -- number of gpu use for training, this\n",
    "                will affect the training steps and learning rate (default: {2})\n",
    "            base_dir {str} -- base dir for ckpt, if None,\n",
    "                then \"models\" is assigned (default: {None})\n",
    "            dir_name {str} -- dir name for ckpt, if None,\n",
    "                will be created automatically (default: {None})\n",
    "        \"\"\"\n",
    "        self.assigned_details = (\n",
    "            flag_string, gpu, base_dir, dir_name, predicting)\n",
    "        self.problem_assigned = True\n",
    "        self.predicting = predicting\n",
    "\n",
    "        self.problem_list, self.problem_chunk = self.parse_problem_string(\n",
    "            flag_string)\n",
    "\n",
    "        # create dir and get vocab, config\n",
    "        self.prepare_dir(base_dir, dir_name, self.problem_list)\n",
    "\n",
    "        self.get_data_info(self.problem_list, self.ckpt_dir)\n",
    "\n",
    "        self.set_data_sampling_strategy()\n",
    "\n",
    "        if not predicting:\n",
    "            for problem in self.problem_list:\n",
    "                if self.problem_type[problem] == 'pretrain':\n",
    "                    dup_fac = self.dupe_factor\n",
    "                    break\n",
    "                else:\n",
    "                    dup_fac = 1\n",
    "            self.train_steps = int((\n",
    "                self.data_num * self.train_epoch * dup_fac) / (self.batch_size*max(1, gpu)))\n",
    "            self.train_steps_per_epoch = int(\n",
    "                self.train_steps / self.train_epoch)\n",
    "            self.num_warmup_steps = int(0.1 * self.train_steps)\n",
    "\n",
    "            # linear scale learing rate\n",
    "            self.lr = self.init_lr * gpu\n",
    "\n",
    "    def to_json(self):\n",
    "        \"\"\"Save the params as json files. Please note that processing_fn is not saved.\n",
    "        \"\"\"\n",
    "        dump_dict = {}\n",
    "        for att_name, att in vars(self).items():\n",
    "            try:\n",
    "                json.dumps(att)\n",
    "                dump_dict[att_name] = att\n",
    "            except TypeError:\n",
    "                pass\n",
    "\n",
    "        with open(self.params_path, 'w', encoding='utf8') as f:\n",
    "            json.dump(dump_dict, f)\n",
    "\n",
    "    def from_json(self, json_path: str = None):\n",
    "        \"\"\"Load json file as params. \n",
    "\n",
    "        json_path could not be None if the problem is not assigned to params\n",
    "\n",
    "        Args:\n",
    "            json_path (str, optional): Path to json file. Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "            AttributeError\n",
    "        \"\"\"\n",
    "        try:\n",
    "            params_path = json_path if json_path is not None else self.params_path\n",
    "        except AttributeError:\n",
    "            raise AttributeError(\n",
    "                'Either json_path should not be None or problem is assigned.')\n",
    "        if self.problem_assigned:\n",
    "            assign_details = self.assigned_details\n",
    "        else:\n",
    "            assign_details = None\n",
    "\n",
    "        with open(params_path, 'r', encoding='utf8') as f:\n",
    "            dump_dict = json.load(f)\n",
    "        for att in dump_dict:\n",
    "            setattr(self, att, dump_dict[att])\n",
    "        self.bert_config = load_transformer_config(\n",
    "            self.bert_config_dict, self.transformer_config_loading)\n",
    "        if hasattr(self, 'bert_decoder_config_dict'):\n",
    "            self.bert_decoder_config = load_transformer_config(\n",
    "                self.bert_decoder_config_dict, self.transformer_decoder_config_loading\n",
    "            )\n",
    "        if assign_details:\n",
    "            self.assign_problem(*assign_details)\n",
    "\n",
    "    def get_data_info(self, problem_list: List[str], base: str):\n",
    "        '''Get number of data, number of classes of data and eos_id of data.\n",
    "\n",
    "        Arguments:\n",
    "            problem_list {list} -- problem list\n",
    "            base {str} -- path to store data_info.json\n",
    "        '''\n",
    "\n",
    "        json_path = os.path.join(base, 'data_info.json')\n",
    "        if os.path.exists(json_path):\n",
    "            data_info = json.load(open(json_path, 'r', encoding='utf8'))\n",
    "            self.data_num_dict = data_info['data_num']\n",
    "            self.num_classes = data_info['num_classes']\n",
    "        elif self.predicting:\n",
    "            data_info = {\n",
    "                'data_num': self.data_num_dict,\n",
    "                'num_classes': self.num_classes,\n",
    "            }\n",
    "            return json.dump(data_info, open(json_path, 'w', encoding='utf8'))\n",
    "        else:\n",
    "            if not hasattr(self, 'data_num_dict'):\n",
    "                self.data_num_dict = {}\n",
    "            if not hasattr(self, 'num_classes'):\n",
    "                self.num_classes = {}\n",
    "\n",
    "        if not self.predicting:\n",
    "            # update data_num and train_steps\n",
    "            self.data_num = 0\n",
    "            for problem in problem_list:\n",
    "                if problem not in self.data_num_dict:\n",
    "\n",
    "                    self.data_num_dict[problem], _ = self.read_data_fn[problem](\n",
    "                        self, 'train', get_data_num=True)\n",
    "                    self.data_num += self.data_num_dict[problem]\n",
    "                else:\n",
    "                    self.data_num += self.data_num_dict[problem]\n",
    "\n",
    "            data_info = {\n",
    "                'data_num': self.data_num_dict,\n",
    "                'num_classes': self.num_classes,\n",
    "            }\n",
    "\n",
    "            json.dump(data_info, open(json_path, 'w', encoding='utf8'))\n",
    "        return json_path\n",
    "\n",
    "    def parse_problem_string(self, flag_string: str) -> Tuple[List[str], List[List[str]]]:\n",
    "        '''Parse problem string\n",
    "        Example:\n",
    "            cws|POS|weibo_ner&weibo_cws\n",
    "\n",
    "            self.run_problem_list = [{cws:seq_tag}, {POS:seq_tag}, {weibo_ner:seq_tag, weibo_cws:seq_tag}]\n",
    "            problem_list = [cws, POS, weibo_ner, weibo_cws]\n",
    "            problem_chunk = [[cws], [POS], [weibo_ner, weibo_cws]]\n",
    "\n",
    "        Arguments:\n",
    "            flag_string {str} -- problem string\n",
    "\n",
    "        Returns:\n",
    "            list -- problem list\n",
    "        '''\n",
    "\n",
    "        self.problem_str = flag_string\n",
    "        # Parse problem string\n",
    "        self.run_problem_list = []\n",
    "        problem_chunk = []\n",
    "        for flag_chunk in flag_string.split('|'):\n",
    "\n",
    "            if '&' not in flag_chunk:\n",
    "                problem_type = {}\n",
    "                problem_type[flag_chunk] = self.problem_type[flag_chunk]\n",
    "                self.run_problem_list.append(problem_type)\n",
    "                problem_chunk.append([flag_chunk])\n",
    "            else:\n",
    "                problem_type = {}\n",
    "                problem_chunk.append([])\n",
    "                for problem in flag_chunk.split('&'):\n",
    "                    problem_type[problem] = self.problem_type[problem]\n",
    "                    problem_chunk[-1].append(problem)\n",
    "                self.run_problem_list.append(problem_type)\n",
    "        # if (self.label_transfer or self.mutual_prediction) and self.train_problem is None:\n",
    "        if self.train_problem is None:\n",
    "            self.train_problem = [p for p in self.run_problem_list]\n",
    "\n",
    "        problem_list = sorted(re.split(r'[&|]', flag_string))\n",
    "        return problem_list, problem_chunk\n",
    "\n",
    "    def prepare_dir(self, base_dir: str, dir_name: str, problem_list: List[str]):\n",
    "        \"\"\"prepare model checkpoint dir. this function will copy or save transformers' configs\n",
    "        and tokenizers to params.ckpt_dir\n",
    "\n",
    "        Args:\n",
    "            base_dir (str): base_dir of params.ckpt_dir. same as os.path.dirname(params.ckpt_dir). bad naming\n",
    "            dir_name (str): dir_name, same as os.path.basename(params.ckpt_dir). bad naming\n",
    "            problem_list (List[str]): [description]\n",
    "        \"\"\"\n",
    "        base = base_dir if base_dir is not None else 'models'\n",
    "\n",
    "        dir_name = dir_name if dir_name is not None else '_'.join(\n",
    "            problem_list)+'_ckpt'\n",
    "        self.ckpt_dir = os.path.join(base, dir_name)\n",
    "\n",
    "        # we need to make sure all configs, tokenizers are in ckpt_dir\n",
    "        # configs\n",
    "        from_config_path = os.path.join(self.init_checkpoint,\n",
    "                                        'bert_config')\n",
    "        from_decoder_config_path = os.path.join(self.init_checkpoint,\n",
    "                                                'bert_decoder_config')\n",
    "        to_config_path = os.path.join(self.ckpt_dir, 'bert_config')\n",
    "        to_decoder_config_path = os.path.join(\n",
    "            self.ckpt_dir, 'bert_decoder_config')\n",
    "\n",
    "        # tokenizers\n",
    "        from_tokenizer_path = os.path.join(self.init_checkpoint, 'tokenizer')\n",
    "        to_tokenizer_path = os.path.join(self.ckpt_dir, 'tokenizer')\n",
    "\n",
    "        from_decoder_tokenizer_path = os.path.join(\n",
    "            self.init_checkpoint, 'decoder_tokenizer')\n",
    "        to_decoder_tokenizer_path = os.path.join(\n",
    "            self.ckpt_dir, 'decoder_tokenizer')\n",
    "\n",
    "        self.params_path = os.path.join(self.ckpt_dir, 'params.json')\n",
    "\n",
    "        if not self.predicting:\n",
    "            create_path(self.ckpt_dir)\n",
    "\n",
    "            # two ways to init model\n",
    "            # 1. init from TF checkpoint dir created by bert-multitask-learning.\n",
    "            # 2. init from huggingface checkpoint.\n",
    "\n",
    "            # bert config exists, init from existing config\n",
    "            if os.path.exists(from_config_path):\n",
    "                # copy config\n",
    "                copy_tree(from_config_path, to_config_path)\n",
    "                self.bert_config = load_transformer_config(\n",
    "                    to_config_path, self.transformer_config_loading)\n",
    "\n",
    "                # copy tokenizer\n",
    "                copy_tree(from_tokenizer_path, to_tokenizer_path)\n",
    "\n",
    "                # copy decoder config\n",
    "                if os.path.exists(from_decoder_config_path):\n",
    "                    copy_tree(from_decoder_config_path,\n",
    "                              to_decoder_config_path)\n",
    "                    self.bert_decoder_config = load_transformer_config(\n",
    "                        from_decoder_config_path, self.transformer_decoder_config_loading\n",
    "                    )\n",
    "                    self.bert_decoder_config_dict = self.bert_decoder_config.to_dict()\n",
    "                # copy decoder tokenizer\n",
    "                if os.path.exists(from_decoder_tokenizer_path):\n",
    "                    copy_tree(from_decoder_tokenizer_path,\n",
    "                              to_decoder_tokenizer_path)\n",
    "\n",
    "                self.init_weight_from_huggingface = False\n",
    "            else:\n",
    "                # load config from huggingface\n",
    "                logging.warning(\n",
    "                    '%s not exists. will load model from huggingface checkpoint.', from_config_path)\n",
    "                # get or download config\n",
    "                self.init_weight_from_huggingface = True\n",
    "                self.bert_config = load_transformer_config(\n",
    "                    self.transformer_config_name, self.transformer_config_loading)\n",
    "                self.bert_config.save_pretrained(to_config_path)\n",
    "\n",
    "                # save tokenizer\n",
    "                tokenizer = load_transformer_tokenizer(\n",
    "                    self.transformer_tokenizer_name, self.transformer_tokenizer_loading)\n",
    "                tokenizer.save_pretrained(to_tokenizer_path)\n",
    "                # save_pretrained method of tokenizer saves the config as tokenizer_config.json, which will cause\n",
    "                # OSError if use tokenizer.from_pretrained directly. we need to manually rename the json file\n",
    "                try:\n",
    "                    os.rename(os.path.join(to_tokenizer_path, 'tokenizer_config.json'), os.path.join(\n",
    "                        to_tokenizer_path, 'config.json'))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                # if decoder is specified\n",
    "                if self.transformer_decoder_model_name:\n",
    "                    self.bert_decoder_config = load_transformer_config(\n",
    "                        self.transformer_decoder_config_name, self.transformer_decoder_config_loading\n",
    "                    )\n",
    "                    self.bert_decoder_config_dict = self.bert_decoder_config.to_dict()\n",
    "                    self.bert_decoder_config.save_pretrained(\n",
    "                        to_decoder_config_path)\n",
    "                    decoder_tokenizer = load_transformer_tokenizer(\n",
    "                        self.transformer_decoder_tokenizer_name, self.transformer_decoder_tokenizer_loading)\n",
    "                    decoder_tokenizer.save_pretrained(\n",
    "                        to_decoder_tokenizer_path)\n",
    "                    try:\n",
    "                        os.rename(os.path.join(to_decoder_tokenizer_path, 'tokenizer_config.json'), os.path.join(\n",
    "                            to_decoder_tokenizer_path, 'config.json'))\n",
    "                    except:\n",
    "                        pass\n",
    "        else:\n",
    "            self.bert_config = load_transformer_config(to_config_path)\n",
    "            if os.path.exists(to_decoder_config_path):\n",
    "                self.bert_decoder_config = load_transformer_config(\n",
    "                    to_decoder_config_path)\n",
    "            self.init_weight_from_huggingface = False\n",
    "\n",
    "        self.transformer_config_name = to_config_path\n",
    "        # set value if and only if decoder is assigned\n",
    "        self.transformer_decoder_config_name = to_decoder_config_path if self.transformer_decoder_config_name is not None else None\n",
    "        self.transformer_tokenizer_name = to_tokenizer_path\n",
    "        # set value if and only if decoder is assigned\n",
    "        self.transformer_decoder_tokenizer_name = to_decoder_tokenizer_path if self.transformer_decoder_tokenizer_name is not None else None\n",
    "\n",
    "        self.bert_config_dict = self.bert_config.to_dict()\n",
    "\n",
    "        tokenizer = load_transformer_tokenizer(\n",
    "            self.transformer_tokenizer_name, self.transformer_tokenizer_loading)\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        if self.transformer_decoder_tokenizer_name:\n",
    "            decoder_tokenizer = load_transformer_tokenizer(\n",
    "                self.transformer_decoder_tokenizer_name,\n",
    "                self.transformer_decoder_tokenizer_loading\n",
    "            )\n",
    "\n",
    "            # if set bos and eos\n",
    "            if decoder_tokenizer.bos_token is None:\n",
    "                decoder_tokenizer.add_special_tokens({'bos_token': BOS_TOKEN})\n",
    "\n",
    "            if decoder_tokenizer.eos_token is None:\n",
    "                decoder_tokenizer.add_special_tokens({'eos_token': EOS_TOKEN})\n",
    "\n",
    "            # overwrite tokenizer\n",
    "            decoder_tokenizer.save_pretrained(to_decoder_tokenizer_path)\n",
    "\n",
    "            self.decoder_vocab_size = decoder_tokenizer.vocab_size\n",
    "            self.bos_id = decoder_tokenizer.bos_token_id\n",
    "            self.eos_id = decoder_tokenizer.eos_token_id\n",
    "\n",
    "    def get_problem_type(self, problem: str) -> str:\n",
    "        return self.problem_type[problem]\n",
    "\n",
    "    def update_train_steps(self, train_steps_per_epoch: int, epoch: int = None, warmup_ratio=0.1) -> None:\n",
    "        \"\"\"If the batch_size is dynamic, we have to loop through the tf.data.Dataset\n",
    "        to get the accurate number of training steps. In this case, we need a function to\n",
    "        update the train_steps which will be used to calculate learning rate schedule.\n",
    "\n",
    "        WARNING: updating should be called before the model is compiled! \n",
    "\n",
    "        Args:\n",
    "            train_steps (int): new number of train_steps\n",
    "        \"\"\"\n",
    "        if epoch:\n",
    "            train_steps = train_steps_per_epoch * epoch\n",
    "        else:\n",
    "            train_steps = train_steps_per_epoch * self.train_epoch\n",
    "\n",
    "        logging.info('Updating train_steps from {0} to {1}'.format(\n",
    "            self.train_steps, train_steps))\n",
    "\n",
    "        self.train_steps = train_steps\n",
    "        self.train_steps_per_epoch = train_steps_per_epoch\n",
    "        self.num_warmup_steps = int(self.train_steps * warmup_ratio)\n",
    "\n",
    "    def get_problem_chunk(self, as_str=True) -> Union[List[str], List[List[str]]]:\n",
    "\n",
    "        if as_str:\n",
    "            res_list = []\n",
    "            for problem_list in self.problem_chunk:\n",
    "                res_list.append('_'.join(sorted(problem_list)))\n",
    "            return res_list\n",
    "        else:\n",
    "            return self.problem_chunk\n",
    "\n",
    "    def set_data_sampling_strategy(self,\n",
    "                                   sampling_strategy='data_balanced',\n",
    "                                   sampling_strategy_fn: Callable = None) -> Dict[str, float]:\n",
    "        \"\"\"Set data sampling strategy for multi-task learning.\n",
    "\n",
    "        'data_balanced' and 'problem_balanced' is implemented by default.\n",
    "        data_balanced: sampling weight equals to number of rows of that problem chunk.\n",
    "        problem_balanced: sampling weight equals to 1 for every problem chunk.\n",
    "\n",
    "        Args:\n",
    "            sampling_strategy (str, optional): sampling strategy. Defaults to 'data_balanced'.\n",
    "            sampling_strategy_fn (Callable, optional): function to create weight dict. Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: sampling_strategy_fn is not implemented yet\n",
    "            ValueError: invalid sampling_strategy provided\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: sampling weight for each problem_chunk\n",
    "        \"\"\"\n",
    "        if sampling_strategy_fn:\n",
    "            logging.info(\n",
    "                'sampling_strategy_fn is provided, sampling_strategy arg will be ignored.')\n",
    "            raise NotImplementedError\n",
    "\n",
    "        problem_chunk_data_num = defaultdict(float)\n",
    "        if sampling_strategy == 'data_balanced':\n",
    "            problem_chunk = self.get_problem_chunk(as_str=False)\n",
    "            for problem_list in problem_chunk:\n",
    "                str_per_chunk = '_'.join(sorted(problem_list))\n",
    "                for problem in problem_list:\n",
    "                    problem_chunk_data_num[str_per_chunk] += self.data_num_dict[problem]\n",
    "        elif sampling_strategy == 'problem_balanced':\n",
    "            problem_chunk = self.get_problem_chunk(as_str=True)\n",
    "            for str_per_chunk in problem_chunk:\n",
    "                problem_chunk_data_num[str_per_chunk] = 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'sampling strategy {} is not implemented by default. '\n",
    "                'please provide sampling_strategy_fn.'.format(sampling_strategy))\n",
    "\n",
    "        # devided by sum to get sampling prob\n",
    "        sum_across_problems = sum(\n",
    "            [v for _, v in problem_chunk_data_num.items()])\n",
    "        self.problem_sampling_weight_dict = {\n",
    "            k: v / sum_across_problems for k, v in problem_chunk_data_num.items()}\n",
    "        return self.problem_sampling_weight_dict\n",
    "\n",
    "    def register_problem_type(self,\n",
    "                              problem_type: str,\n",
    "                              top_layer: tf.keras.Model,\n",
    "                              label_handling_fn: Callable = None,\n",
    "                              get_or_make_label_encoder_fn: Callable = None):\n",
    "        self.problem_type_list.append(problem_type)\n",
    "        self.get_or_make_label_encoder_fn_dict[problem_type] = get_or_make_label_encoder_fn\n",
    "        self.top_layer[problem_type] = top_layer\n",
    "        self.label_handling_fn[problem_type] = label_handling_fn\n",
    "\n",
    "\n",
    "class CRFParams(BaseParams):\n",
    "    def __init__(self):\n",
    "        super(CRFParams, self).__init__()\n",
    "        self.crf = True\n",
    "\n",
    "\n",
    "class StaticBatchParams(BaseParams):\n",
    "    def __init__(self):\n",
    "        super(StaticBatchParams, self).__init__()\n",
    "        self.dynamic_padding = False\n",
    "\n",
    "\n",
    "class DynamicBatchSizeParams(BaseParams):\n",
    "    def __init__(self):\n",
    "        super(DynamicBatchSizeParams, self).__init__()\n",
    "        self.bucket_batch_sizes = [128, 64, 32, 16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
