{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp params\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params\n",
    "\n",
    "`BaseParams` is the major object to control the whole modeling process. It is supposed to be accessable anywhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import logging\n",
    "from typing import Callable, List, Tuple, Dict, Union\n",
    "from collections import defaultdict\n",
    "from distutils.dir_util import copy_tree\n",
    "import tensorflow as tf\n",
    "\n",
    "from bert_multitask_learning.utils import create_path, load_transformer_tokenizer, load_transformer_config\n",
    "from bert_multitask_learning.special_tokens import BOS_TOKEN, EOS_TOKEN\n",
    "\n",
    "\n",
    "class BaseParams():\n",
    "    # pylint: disable=attribute-defined-outside-init\n",
    "    def __init__(self):\n",
    "        self.run_problem_list = []\n",
    "\n",
    "        self.problem_type = {\n",
    "        }\n",
    "\n",
    "        # transformers params\n",
    "        self.transformer_model_name = 'bert-base-chinese'\n",
    "        self.transformer_tokenizer_name = 'bert-base-chinese'\n",
    "        self.transformer_config_name = 'bert-base-chinese'\n",
    "        self.transformer_model_loading = 'TFBertModel'\n",
    "        self.transformer_config_loading = 'BertConfig'\n",
    "        self.transformer_tokenizer_loading = 'BertTokenizer'\n",
    "        self.transformer_decoder_model_name = None\n",
    "        self.transformer_decoder_config_name = None\n",
    "        self.transformer_decoder_tokenizer_name = None\n",
    "        # self.transformer_decoder_model_name = \"hfl/chinese-xlnet-base\"\n",
    "        # self.transformer_decoder_config_name = \"hfl/chinese-xlnet-base\"\n",
    "        # self.transformer_decoder_tokenizer_name = \"hfl/chinese-xlnet-base\"\n",
    "        self.transformer_decoder_model_loading = 'TFAutoModel'\n",
    "        self.transformer_decoder_config_loading = 'AutoConfig'\n",
    "        self.transformer_decoder_tokenizer_loading = 'AutoTokenizer'\n",
    "\n",
    "        # multimodal params\n",
    "        self.modal_segment_id = {\n",
    "            'text': 0,\n",
    "            'image': 0,\n",
    "            'others': 0\n",
    "        }\n",
    "        self.modal_type_id = {\n",
    "            'text': 0,\n",
    "            'image': 1,\n",
    "            'others': 2\n",
    "        }\n",
    "        self.enable_modal_type = False\n",
    "        # bert config\n",
    "        self.init_checkpoint = ''\n",
    "\n",
    "        # specify this will make key reuse values top\n",
    "        # that it, weibo_ner problem will use NER's top\n",
    "        self.share_top = {\n",
    "        }\n",
    "        for p in self.problem_type:\n",
    "            if p not in self.share_top:\n",
    "                self.share_top[p] = p\n",
    "\n",
    "        self.multitask_balance_type = 'data_balanced'\n",
    "        self.problem_type_list = ['cls', 'seq_tag', 'seq2seq_tag',\n",
    "                                  'seq2seq_text', 'multi_cls', 'pretrain', 'masklm']\n",
    "        self.predefined_problem_type = ['cls', 'seq_tag', 'seq2seq_tag',\n",
    "                                        'seq2seq_text', 'multi_cls', 'pretrain', 'masklm']\n",
    "        self.get_or_make_label_encoder_fn_dict: Dict[str, Callable] = {}\n",
    "        self.label_handling_fn: Dict[str, Callable] = {}\n",
    "        self.top_layer = {}\n",
    "        self.num_classes = {}\n",
    "        # self.multitask_balance_type = 'problem_balanced'\n",
    "\n",
    "        # logging control\n",
    "        self.log_every_n_steps = 100\n",
    "        self.detail_log = True\n",
    "\n",
    "        self.multiprocess = True\n",
    "        self.num_cpus = 4\n",
    "        self.per_cpu_buffer = 3000\n",
    "        self.decode_vocab_file = None\n",
    "        self.eval_throttle_secs = 600\n",
    "\n",
    "        # training\n",
    "        self.init_lr = 2e-5\n",
    "        self.batch_size = 32\n",
    "        self.train_epoch = 15\n",
    "        self.freeze_step = 0\n",
    "        self.prefetch = 5000\n",
    "        self.dynamic_padding = True\n",
    "        self.bucket_batch_sizes = [32, 32, 32, 16]\n",
    "        self.bucket_boundaries = [30, 64, 128]\n",
    "        self.shuffle_buffer = 200000\n",
    "\n",
    "        # hparm\n",
    "        self.dropout_keep_prob = 0.9\n",
    "        self.max_seq_len = 256\n",
    "        self.use_one_hot_embeddings = True\n",
    "        self.label_smoothing = 0.0\n",
    "        self.crf = False\n",
    "        self.bert_num_hidden_layer = 12\n",
    "        self.hidden_dense = False\n",
    "        # threshold to calculate metrics for multi_cls\n",
    "        self.multi_cls_threshold = 0.5\n",
    "        self.multi_cls_positive_weight = 1.0\n",
    "        self.custom_pooled_hidden_size = 0\n",
    "        self.share_embedding = True\n",
    "\n",
    "        # seq2seq\n",
    "        self.decoder_num_hidden_layers = 3\n",
    "        self.beam_size = 10\n",
    "        self.init_decoder_from_encoder = False\n",
    "        self.beam_search_alpha = 0.6\n",
    "        self.decode_max_seq_len = 90\n",
    "\n",
    "        # experimental multitask approach\n",
    "        self.label_transfer = False\n",
    "        # train mask lm and downstream task at the same time\n",
    "        self.augument_mask_lm = False\n",
    "        self.augument_rate = 0.5\n",
    "        # NOT implemented\n",
    "        self.distillation = False\n",
    "        # Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\n",
    "        # ref: https://arxiv.org/abs/1705.07115\n",
    "        self.uncertain_weight_loss = False\n",
    "        # dep since not good\n",
    "        # self.mutual_prediction = False\n",
    "\n",
    "        # add an extra attention for each task\n",
    "        #   with BERT layers as encoder output, task logits as decoder inputs\n",
    "        self.grid_transformer = False\n",
    "\n",
    "        # add an extra attention for each task\n",
    "        #   with other tasks' logits as encoder output, task logits asn decoder inputs\n",
    "        self.task_transformer = False\n",
    "\n",
    "        # do a mean for gradients of BERT layers instead of sum\n",
    "        self.mean_gradients = False\n",
    "\n",
    "        # random replace punctuation by some prob to\n",
    "        # ease the punctuation sensitive problem\n",
    "        self.punc_replace_prob = 0.0\n",
    "        self.punc_list = list(',.!?！。？，、')\n",
    "        self.hidden_gru = False\n",
    "        self.label_transfer_gru = False\n",
    "        # if None, we will use the same hidden_size as inputs\n",
    "        # e.g. # of labels\n",
    "        self.label_transfer_gru_hidden_size = None\n",
    "\n",
    "        # pretrain hparm\n",
    "        self.dupe_factor = 10\n",
    "        self.short_seq_prob = 0.1\n",
    "        self.masked_lm_prob = 0.15\n",
    "        self.max_predictions_per_seq = 20\n",
    "        self.mask_lm_hidden_size = 768\n",
    "        self.mask_lm_hidden_act = 'gelu'\n",
    "        self.mask_lm_initializer_range = 0.02\n",
    "\n",
    "        self.train_problem = None\n",
    "        self.tmp_file_dir = 'tmp'\n",
    "        self.cache_dir = 'models/transformers_cache'\n",
    "        # get generator function for each problem\n",
    "        self.read_data_fn = {}\n",
    "        self.problem_assigned = False\n",
    "\n",
    "    def add_problem(self, problem_name: str, problem_type='cls', processing_fn: Callable = None):\n",
    "\n",
    "        if problem_type not in self.problem_type_list:\n",
    "            raise ValueError('Provided problem type not valid, expect {0}, got {1}'.format(\n",
    "                self.problem_type_list,\n",
    "                problem_type))\n",
    "\n",
    "        self.problem_type[problem_name] = problem_type\n",
    "        self.read_data_fn[problem_name] = processing_fn\n",
    "\n",
    "    def add_multiple_problems(self, problem_type_dict: Dict[str, str], processing_fn_dict: Dict[str, Callable] = None):\n",
    "        # add new problem to params if problem_type_dict and processing_fn_dict provided\n",
    "        for new_problem, problem_type in problem_type_dict.items():\n",
    "            print('Adding new problem {0}, problem type: {1}'.format(\n",
    "                new_problem, problem_type_dict[new_problem]))\n",
    "            if processing_fn_dict:\n",
    "                new_problem_processing_fn = processing_fn_dict[new_problem]\n",
    "            else:\n",
    "                new_problem_processing_fn = None\n",
    "            self.add_problem(\n",
    "                problem_name=new_problem, problem_type=problem_type, processing_fn=new_problem_processing_fn)\n",
    "\n",
    "    def assign_problem(self,\n",
    "                       flag_string: str,\n",
    "                       gpu=2,\n",
    "                       base_dir: str = None,\n",
    "                       dir_name: str = None,\n",
    "                       predicting=False):\n",
    "        self.assigned_details = (\n",
    "            flag_string, gpu, base_dir, dir_name, predicting)\n",
    "        self.problem_assigned = True\n",
    "        self.predicting = predicting\n",
    "\n",
    "        self.problem_list, self.problem_chunk = self.parse_problem_string(\n",
    "            flag_string)\n",
    "\n",
    "        # create dir and get vocab, config\n",
    "        self.prepare_dir(base_dir, dir_name, self.problem_list)\n",
    "\n",
    "        self.get_data_info(self.problem_list, self.ckpt_dir)\n",
    "\n",
    "        self.set_data_sampling_strategy()\n",
    "\n",
    "        if not predicting:\n",
    "            for problem in self.problem_list:\n",
    "                if self.problem_type[problem] == 'pretrain':\n",
    "                    dup_fac = self.dupe_factor\n",
    "                    break\n",
    "                else:\n",
    "                    dup_fac = 1\n",
    "            self.train_steps = int((\n",
    "                self.data_num * self.train_epoch * dup_fac) / (self.batch_size*max(1, gpu)))\n",
    "            self.train_steps_per_epoch = int(\n",
    "                self.train_steps / self.train_epoch)\n",
    "            self.num_warmup_steps = int(0.1 * self.train_steps)\n",
    "\n",
    "            # linear scale learing rate\n",
    "            self.lr = self.init_lr * gpu\n",
    "\n",
    "    def to_json(self):\n",
    "        \"\"\"Save the params as json files. Please note that processing_fn is not saved.\n",
    "        \"\"\"\n",
    "        dump_dict = {}\n",
    "        for att_name, att in vars(self).items():\n",
    "            try:\n",
    "                json.dumps(att)\n",
    "                dump_dict[att_name] = att\n",
    "            except TypeError:\n",
    "                pass\n",
    "\n",
    "        with open(self.params_path, 'w', encoding='utf8') as f:\n",
    "            json.dump(dump_dict, f)\n",
    "\n",
    "    def from_json(self, json_path: str = None):\n",
    "        \"\"\"Load json file as params. \n",
    "\n",
    "        json_path could not be None if the problem is not assigned to params\n",
    "\n",
    "        Args:\n",
    "            json_path (str, optional): Path to json file. Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "            AttributeError\n",
    "        \"\"\"\n",
    "        try:\n",
    "            params_path = json_path if json_path is not None else self.params_path\n",
    "        except AttributeError:\n",
    "            raise AttributeError(\n",
    "                'Either json_path should not be None or problem is assigned.')\n",
    "        if self.problem_assigned:\n",
    "            assign_details = self.assigned_details\n",
    "        else:\n",
    "            assign_details = None\n",
    "\n",
    "        with open(params_path, 'r', encoding='utf8') as f:\n",
    "            dump_dict = json.load(f)\n",
    "        for att in dump_dict:\n",
    "            setattr(self, att, dump_dict[att])\n",
    "        self.bert_config = load_transformer_config(\n",
    "            self.bert_config_dict, self.transformer_config_loading)\n",
    "        if hasattr(self, 'bert_decoder_config_dict'):\n",
    "            self.bert_decoder_config = load_transformer_config(\n",
    "                self.bert_decoder_config_dict, self.transformer_decoder_config_loading\n",
    "            )\n",
    "        if assign_details:\n",
    "            self.assign_problem(*assign_details)\n",
    "\n",
    "    def get_data_info(self, problem_list: List[str], base: str):\n",
    "\n",
    "        json_path = os.path.join(base, 'data_info.json')\n",
    "        if os.path.exists(json_path):\n",
    "            data_info = json.load(open(json_path, 'r', encoding='utf8'))\n",
    "            self.data_num_dict = data_info['data_num']\n",
    "            self.num_classes = data_info['num_classes']\n",
    "        elif self.predicting:\n",
    "            data_info = {\n",
    "                'data_num': self.data_num_dict,\n",
    "                'num_classes': self.num_classes,\n",
    "            }\n",
    "            return json.dump(data_info, open(json_path, 'w', encoding='utf8'))\n",
    "        else:\n",
    "            if not hasattr(self, 'data_num_dict'):\n",
    "                self.data_num_dict = {}\n",
    "            if not hasattr(self, 'num_classes'):\n",
    "                self.num_classes = {}\n",
    "\n",
    "        if not self.predicting:\n",
    "            # update data_num and train_steps\n",
    "            self.data_num = 0\n",
    "            for problem in problem_list:\n",
    "                if problem not in self.data_num_dict:\n",
    "\n",
    "                    self.data_num_dict[problem], _ = self.read_data_fn[problem](\n",
    "                        self, 'train', get_data_num=True)\n",
    "                    self.data_num += self.data_num_dict[problem]\n",
    "                else:\n",
    "                    self.data_num += self.data_num_dict[problem]\n",
    "\n",
    "            data_info = {\n",
    "                'data_num': self.data_num_dict,\n",
    "                'num_classes': self.num_classes,\n",
    "            }\n",
    "\n",
    "            json.dump(data_info, open(json_path, 'w', encoding='utf8'))\n",
    "        return json_path\n",
    "\n",
    "    def parse_problem_string(self, flag_string: str) -> Tuple[List[str], List[List[str]]]:\n",
    "\n",
    "        self.problem_str = flag_string\n",
    "        # Parse problem string\n",
    "        self.run_problem_list = []\n",
    "        problem_chunk = []\n",
    "        for flag_chunk in flag_string.split('|'):\n",
    "\n",
    "            if '&' not in flag_chunk:\n",
    "                problem_type = {}\n",
    "                problem_type[flag_chunk] = self.problem_type[flag_chunk]\n",
    "                self.run_problem_list.append(problem_type)\n",
    "                problem_chunk.append([flag_chunk])\n",
    "            else:\n",
    "                problem_type = {}\n",
    "                problem_chunk.append([])\n",
    "                for problem in flag_chunk.split('&'):\n",
    "                    problem_type[problem] = self.problem_type[problem]\n",
    "                    problem_chunk[-1].append(problem)\n",
    "                self.run_problem_list.append(problem_type)\n",
    "        # if (self.label_transfer or self.mutual_prediction) and self.train_problem is None:\n",
    "        if self.train_problem is None:\n",
    "            self.train_problem = [p for p in self.run_problem_list]\n",
    "\n",
    "        problem_list = sorted(re.split(r'[&|]', flag_string))\n",
    "        return problem_list, problem_chunk\n",
    "\n",
    "    def prepare_dir(self, base_dir: str, dir_name: str, problem_list: List[str]):\n",
    "        \"\"\"prepare model checkpoint dir. this function will copy or save transformers' configs\n",
    "        and tokenizers to params.ckpt_dir\n",
    "\n",
    "        Args:\n",
    "            base_dir (str): base_dir of params.ckpt_dir. same as os.path.dirname(params.ckpt_dir). bad naming\n",
    "            dir_name (str): dir_name, same as os.path.basename(params.ckpt_dir). bad naming\n",
    "            problem_list (List[str]): [description]\n",
    "        \"\"\"\n",
    "        base = base_dir if base_dir is not None else 'models'\n",
    "\n",
    "        dir_name = dir_name if dir_name is not None else '_'.join(\n",
    "            problem_list)+'_ckpt'\n",
    "        self.ckpt_dir = os.path.join(base, dir_name)\n",
    "\n",
    "        # we need to make sure all configs, tokenizers are in ckpt_dir\n",
    "        # configs\n",
    "        from_config_path = os.path.join(self.init_checkpoint,\n",
    "                                        'bert_config')\n",
    "        from_decoder_config_path = os.path.join(self.init_checkpoint,\n",
    "                                                'bert_decoder_config')\n",
    "        to_config_path = os.path.join(self.ckpt_dir, 'bert_config')\n",
    "        to_decoder_config_path = os.path.join(\n",
    "            self.ckpt_dir, 'bert_decoder_config')\n",
    "\n",
    "        # tokenizers\n",
    "        from_tokenizer_path = os.path.join(self.init_checkpoint, 'tokenizer')\n",
    "        to_tokenizer_path = os.path.join(self.ckpt_dir, 'tokenizer')\n",
    "\n",
    "        from_decoder_tokenizer_path = os.path.join(\n",
    "            self.init_checkpoint, 'decoder_tokenizer')\n",
    "        to_decoder_tokenizer_path = os.path.join(\n",
    "            self.ckpt_dir, 'decoder_tokenizer')\n",
    "\n",
    "        self.params_path = os.path.join(self.ckpt_dir, 'params.json')\n",
    "\n",
    "        if not self.predicting:\n",
    "            create_path(self.ckpt_dir)\n",
    "\n",
    "            # two ways to init model\n",
    "            # 1. init from TF checkpoint dir created by bert-multitask-learning.\n",
    "            # 2. init from huggingface checkpoint.\n",
    "\n",
    "            # bert config exists, init from existing config\n",
    "            if os.path.exists(from_config_path):\n",
    "                # copy config\n",
    "                copy_tree(from_config_path, to_config_path)\n",
    "                self.bert_config = load_transformer_config(\n",
    "                    to_config_path, self.transformer_config_loading)\n",
    "\n",
    "                # copy tokenizer\n",
    "                copy_tree(from_tokenizer_path, to_tokenizer_path)\n",
    "\n",
    "                # copy decoder config\n",
    "                if os.path.exists(from_decoder_config_path):\n",
    "                    copy_tree(from_decoder_config_path,\n",
    "                              to_decoder_config_path)\n",
    "                    self.bert_decoder_config = load_transformer_config(\n",
    "                        from_decoder_config_path, self.transformer_decoder_config_loading\n",
    "                    )\n",
    "                    self.bert_decoder_config_dict = self.bert_decoder_config.to_dict()\n",
    "                # copy decoder tokenizer\n",
    "                if os.path.exists(from_decoder_tokenizer_path):\n",
    "                    copy_tree(from_decoder_tokenizer_path,\n",
    "                              to_decoder_tokenizer_path)\n",
    "\n",
    "                self.init_weight_from_huggingface = False\n",
    "            else:\n",
    "                # load config from huggingface\n",
    "                logging.warning(\n",
    "                    '%s not exists. will load model from huggingface checkpoint.', from_config_path)\n",
    "                # get or download config\n",
    "                self.init_weight_from_huggingface = True\n",
    "                self.bert_config = load_transformer_config(\n",
    "                    self.transformer_config_name, self.transformer_config_loading)\n",
    "                self.bert_config.save_pretrained(to_config_path)\n",
    "\n",
    "                # save tokenizer\n",
    "                tokenizer = load_transformer_tokenizer(\n",
    "                    self.transformer_tokenizer_name, self.transformer_tokenizer_loading)\n",
    "                tokenizer.save_pretrained(to_tokenizer_path)\n",
    "                # save_pretrained method of tokenizer saves the config as tokenizer_config.json, which will cause\n",
    "                # OSError if use tokenizer.from_pretrained directly. we need to manually rename the json file\n",
    "                try:\n",
    "                    os.rename(os.path.join(to_tokenizer_path, 'tokenizer_config.json'), os.path.join(\n",
    "                        to_tokenizer_path, 'config.json'))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                # if decoder is specified\n",
    "                if self.transformer_decoder_model_name:\n",
    "                    self.bert_decoder_config = load_transformer_config(\n",
    "                        self.transformer_decoder_config_name, self.transformer_decoder_config_loading\n",
    "                    )\n",
    "                    self.bert_decoder_config_dict = self.bert_decoder_config.to_dict()\n",
    "                    self.bert_decoder_config.save_pretrained(\n",
    "                        to_decoder_config_path)\n",
    "                    decoder_tokenizer = load_transformer_tokenizer(\n",
    "                        self.transformer_decoder_tokenizer_name, self.transformer_decoder_tokenizer_loading)\n",
    "                    decoder_tokenizer.save_pretrained(\n",
    "                        to_decoder_tokenizer_path)\n",
    "                    try:\n",
    "                        os.rename(os.path.join(to_decoder_tokenizer_path, 'tokenizer_config.json'), os.path.join(\n",
    "                            to_decoder_tokenizer_path, 'config.json'))\n",
    "                    except:\n",
    "                        pass\n",
    "        else:\n",
    "            self.bert_config = load_transformer_config(to_config_path)\n",
    "            if os.path.exists(to_decoder_config_path):\n",
    "                self.bert_decoder_config = load_transformer_config(\n",
    "                    to_decoder_config_path)\n",
    "            self.init_weight_from_huggingface = False\n",
    "\n",
    "        self.transformer_config_name = to_config_path\n",
    "        # set value if and only if decoder is assigned\n",
    "        self.transformer_decoder_config_name = to_decoder_config_path if self.transformer_decoder_config_name is not None else None\n",
    "        self.transformer_tokenizer_name = to_tokenizer_path\n",
    "        # set value if and only if decoder is assigned\n",
    "        self.transformer_decoder_tokenizer_name = to_decoder_tokenizer_path if self.transformer_decoder_tokenizer_name is not None else None\n",
    "\n",
    "        self.bert_config_dict = self.bert_config.to_dict()\n",
    "\n",
    "        tokenizer = load_transformer_tokenizer(\n",
    "            self.transformer_tokenizer_name, self.transformer_tokenizer_loading)\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        if self.transformer_decoder_tokenizer_name:\n",
    "            decoder_tokenizer = load_transformer_tokenizer(\n",
    "                self.transformer_decoder_tokenizer_name,\n",
    "                self.transformer_decoder_tokenizer_loading\n",
    "            )\n",
    "\n",
    "            # if set bos and eos\n",
    "            if decoder_tokenizer.bos_token is None:\n",
    "                decoder_tokenizer.add_special_tokens({'bos_token': BOS_TOKEN})\n",
    "\n",
    "            if decoder_tokenizer.eos_token is None:\n",
    "                decoder_tokenizer.add_special_tokens({'eos_token': EOS_TOKEN})\n",
    "\n",
    "            # overwrite tokenizer\n",
    "            decoder_tokenizer.save_pretrained(to_decoder_tokenizer_path)\n",
    "\n",
    "            self.decoder_vocab_size = decoder_tokenizer.vocab_size\n",
    "            self.bos_id = decoder_tokenizer.bos_token_id\n",
    "            self.eos_id = decoder_tokenizer.eos_token_id\n",
    "\n",
    "    def get_problem_type(self, problem: str) -> str:\n",
    "        return self.problem_type[problem]\n",
    "\n",
    "    def update_train_steps(self, train_steps_per_epoch: int, epoch: int = None, warmup_ratio=0.1) -> None:\n",
    "        \"\"\"If the batch_size is dynamic, we have to loop through the tf.data.Dataset\n",
    "        to get the accurate number of training steps. In this case, we need a function to\n",
    "        update the train_steps which will be used to calculate learning rate schedule.\n",
    "\n",
    "        WARNING: updating should be called before the model is compiled! \n",
    "\n",
    "        Args:\n",
    "            train_steps (int): new number of train_steps\n",
    "        \"\"\"\n",
    "        if epoch:\n",
    "            train_steps = train_steps_per_epoch * epoch\n",
    "        else:\n",
    "            train_steps = train_steps_per_epoch * self.train_epoch\n",
    "\n",
    "        logging.info('Updating train_steps from {0} to {1}'.format(\n",
    "            self.train_steps, train_steps))\n",
    "\n",
    "        self.train_steps = train_steps\n",
    "        self.train_steps_per_epoch = train_steps_per_epoch\n",
    "        self.num_warmup_steps = int(self.train_steps * warmup_ratio)\n",
    "\n",
    "    def get_problem_chunk(self, as_str=True) -> Union[List[str], List[List[str]]]:\n",
    "\n",
    "        if as_str:\n",
    "            res_list = []\n",
    "            for problem_list in self.problem_chunk:\n",
    "                res_list.append('_'.join(sorted(problem_list)))\n",
    "            return res_list\n",
    "        else:\n",
    "            return self.problem_chunk\n",
    "\n",
    "    def set_data_sampling_strategy(self,\n",
    "                                   sampling_strategy='data_balanced',\n",
    "                                   sampling_strategy_fn: Callable = None) -> Dict[str, float]:\n",
    "        if sampling_strategy_fn:\n",
    "            logging.info(\n",
    "                'sampling_strategy_fn is provided, sampling_strategy arg will be ignored.')\n",
    "            raise NotImplementedError\n",
    "\n",
    "        problem_chunk_data_num = defaultdict(float)\n",
    "        if sampling_strategy == 'data_balanced':\n",
    "            problem_chunk = self.get_problem_chunk(as_str=False)\n",
    "            for problem_list in problem_chunk:\n",
    "                str_per_chunk = '_'.join(sorted(problem_list))\n",
    "                for problem in problem_list:\n",
    "                    problem_chunk_data_num[str_per_chunk] += self.data_num_dict[problem]\n",
    "        elif sampling_strategy == 'problem_balanced':\n",
    "            problem_chunk = self.get_problem_chunk(as_str=True)\n",
    "            for str_per_chunk in problem_chunk:\n",
    "                problem_chunk_data_num[str_per_chunk] = 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'sampling strategy {} is not implemented by default. '\n",
    "                'please provide sampling_strategy_fn.'.format(sampling_strategy))\n",
    "\n",
    "        # devided by sum to get sampling prob\n",
    "        sum_across_problems = sum(\n",
    "            [v for _, v in problem_chunk_data_num.items()])\n",
    "        self.problem_sampling_weight_dict = {\n",
    "            k: v / sum_across_problems for k, v in problem_chunk_data_num.items()}\n",
    "        return self.problem_sampling_weight_dict\n",
    "\n",
    "    def register_problem_type(self,\n",
    "                              problem_type: str,\n",
    "                              top_layer: tf.keras.Model,\n",
    "                              label_handling_fn: Callable = None,\n",
    "                              get_or_make_label_encoder_fn: Callable = None):\n",
    "        self.problem_type_list.append(problem_type)\n",
    "        self.get_or_make_label_encoder_fn_dict[problem_type] = get_or_make_label_encoder_fn\n",
    "        self.top_layer[problem_type] = top_layer\n",
    "        self.label_handling_fn[problem_type] = label_handling_fn\n",
    "\n",
    "\n",
    "class CRFParams(BaseParams):\n",
    "    def __init__(self):\n",
    "        super(CRFParams, self).__init__()\n",
    "        self.crf = True\n",
    "\n",
    "\n",
    "class StaticBatchParams(BaseParams):\n",
    "    def __init__(self):\n",
    "        super(StaticBatchParams, self).__init__()\n",
    "        self.dynamic_padding = False\n",
    "\n",
    "\n",
    "class DynamicBatchSizeParams(BaseParams):\n",
    "    def __init__(self):\n",
    "        super(DynamicBatchSizeParams, self).__init__()\n",
    "        self.bucket_batch_sizes = [128, 64, 32, 16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "params = BaseParams()\n",
    "assert params.problem_assigned == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# define a simple preprocessing function\n",
    "import bert_multitask_learning\n",
    "from bert_multitask_learning import preprocessing_fn\n",
    "@preprocessing_fn\n",
    "def toy_cls(params: BaseParams, mode: str):\n",
    "    \"Simple example to demonstrate singe modal tuple of list return\"\n",
    "    if mode == bert_multitask_learning.TRAIN:\n",
    "        toy_input = ['this is a toy input' for _ in range(10)]\n",
    "        toy_target = ['a' for _ in range(10)]\n",
    "    else:\n",
    "        toy_input = ['this is a toy input for test' for _ in range(10)]\n",
    "        toy_target = ['a' for _ in range(10)]\n",
    "    return toy_input, toy_target\n",
    "\n",
    "@preprocessing_fn\n",
    "def toy_seq_tag(params: BaseParams, mode: str):\n",
    "    \"Simple example to demonstrate singe modal tuple of list return\"\n",
    "    if mode == bert_multitask_learning.TRAIN:\n",
    "        toy_input = ['this is a toy input'.split(' ') for _ in range(10)]\n",
    "        toy_target = [['a', 'b', 'c', 'd', 'e'] for _ in range(10)]\n",
    "    else:\n",
    "        toy_input = ['this is a toy input for test'.split(' ') for _ in range(10)]\n",
    "        toy_target = [['a', 'b', 'c', 'd', 'e', 'e', 'e'] for _ in range(10)]\n",
    "    return toy_input, toy_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"BaseParams.add_problem\" class=\"doc_header\"><code>BaseParams.add_problem</code><a href=\"__main__.py#L168\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>BaseParams.add_problem</code>(**`problem_name`**:`str`, **`problem_type`**=*`'cls'`*, **`processing_fn`**:`Callable`=*`None`*)\n\n",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseParams.add_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add problems.\n",
    "\n",
    "Args:\n",
    "- problem_name (str): problem name.\n",
    "- problem_type (str, optional): One of the following problem types:\n",
    "['cls', 'seq_tag', 'seq2seq_tag', 'seq2seq_text', 'multi_cls', 'pretrain'].\n",
    "Defaults to 'cls'.\n",
    "- processing_fn (Callable, optional): preprocessing function. Defaults to None.\n",
    "\n",
    "Raises:\n",
    "- ValueError: unexpected problem_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.add_problem(problem_name='toy_cls', problem_type='cls', processing_fn=toy_cls)\n",
    "params.add_problem(problem_name='toy_seq_tag', problem_type='seq_tag', processing_fn=toy_seq_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"BaseParams.add_multiple_problems\" class=\"doc_header\"><code>BaseParams.add_multiple_problems</code><a href=\"__main__.py#L178\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>BaseParams.add_multiple_problems</code>(**`problem_type_dict`**:`Dict`\\[`str`, `str`\\], **`processing_fn_dict`**:`Dict`\\[`str`, `Callable`\\]=*`None`*)\n\n",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseParams.add_multiple_problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add multiple problems.\n",
    "\n",
    "processing_fn_dict is optional, if it's not provided, processing fn will be set as None.\n",
    "\n",
    "Args:\n",
    "- problem_type_dict (Dict[str, str]): problem type dict\n",
    "- processing_fn_dict (Dict[str, Callable], optional): problem type fn. Defaults to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new problem toy_cls, problem type: cls\n",
      "Adding new problem toy_seq_tag, problem type: seq_tag\n"
     ]
    }
   ],
   "source": [
    "# make dict and add problems to params\n",
    "problem_type_dict = {'toy_cls': 'cls', 'toy_seq_tag': 'seq_tag'}\n",
    "processing_fn_dict = {'toy_cls': toy_cls, 'toy_seq_tag': toy_seq_tag}\n",
    "params.add_multiple_problems(problem_type_dict=problem_type_dict, processing_fn_dict=processing_fn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"BaseParams.assign_problem\" class=\"doc_header\"><code>BaseParams.assign_problem</code><a href=\"__main__.py#L190\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>BaseParams.assign_problem</code>(**`flag_string`**:`str`, **`gpu`**=*`2`*, **`base_dir`**:`str`=*`None`*, **`dir_name`**:`str`=*`None`*, **`predicting`**=*`False`*)\n\n",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseParams.assign_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the actual run problem to param. This function will\n",
    "do the following things:\n",
    "\n",
    "1. parse the flag string to form the run_problem_list\n",
    "2. create checkpoint saving path\n",
    "3. calculate total number of training data and training steps\n",
    "4. scale learning rate with the number of gpu linearly\n",
    "\n",
    "Arguments:\n",
    "- flag_string {str} -- run problem string\n",
    "- example: cws|POS|weibo_ner&weibo_cws\n",
    "\n",
    "Keyword Arguments:\n",
    "- gpu {int} -- number of gpu use for training, this will affect the training steps and learning rate (default: {2})\n",
    "- base_dir {str} -- base dir for ckpt, if None, then \"models\" is assigned (default: {None})\n",
    "- dir_name {str} -- dir name for ckpt, if None, will be created automatically (default: {None})\n",
    "- predicting {bool} -- whether is predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:bert_config not exists. will load model from huggingface checkpoint.\n"
     ]
    }
   ],
   "source": [
    "params.assign_problem(flag_string='toy_seq_tag|toy_cls')\n",
    "assert params.problem_assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After problem assigned, the model path should be created with tokenizers, label encoder files in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# assert os.listdir(params.ckpt_dir) == ['data_info.json',\n",
    "#  'tokenizer',\n",
    "#  'toy_cls_label_encoder.pkl',\n",
    "#  'toy_seq_tag_label_encoder.pkl',\n",
    "#  'bert_config']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register new problem type\n",
    "\n",
    "You can also implement your own problem type. Essentially, a problem type has:\n",
    "- name\n",
    "- top layer\n",
    "- label handling function\n",
    "- label encoder creating function\n",
    "\n",
    "Here we register a vector fitting(vector annealing) problem type as an example.\n",
    "\n",
    "Note: This is originally designed as an internal API for development. So it's not user-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"BaseParams.register_problem_type\" class=\"doc_header\"><code>BaseParams.register_problem_type</code><a href=\"__main__.py#L552\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>BaseParams.register_problem_type</code>(**`problem_type`**:`str`, **`top_layer`**:`Model`, **`label_handling_fn`**:`Callable`=*`None`*, **`get_or_make_label_encoder_fn`**:`Callable`=*`None`*)\n\n",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseParams.register_problem_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API to register a new problem type\n",
    "\n",
    "Args:\n",
    "- problem_type: string, problem type name\n",
    "- top_layer: a keras model with some specific reqirements\n",
    "- label_handling_fn: function to convert labels to label ids\n",
    "- get_or_make_label_encoder_fn: function to create label encoder, num_classes has to be specified here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_multitask_learning.top import BaseTop\n",
    "# top layer\n",
    "class VectorFit(BaseTop):\n",
    "    def __init__(self, params: BaseParams, problem_name: str) -> None:\n",
    "        super(VectorFit, self).__init__(\n",
    "            params=params, problem_name=problem_name)\n",
    "        self.num_classes = self.params.num_classes[problem_name]\n",
    "        self.dense = tf.keras.layers.Dense(self.num_classes)\n",
    "\n",
    "    def call(self, inputs: Tuple[Dict], mode: str):\n",
    "        feature, hidden_feature = inputs\n",
    "        pooled_hidden = hidden_feature['pooled']\n",
    "\n",
    "        logits = self.dense(pooled_hidden)\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            # this is the same as the label_id returned by vector_fit_label_handling_fn\n",
    "            label = feature['{}_label_ids'.format(self.problem_name)]\n",
    "\n",
    "            loss = empty_tensor_handling_loss(label, logits, cosine_wrapper)\n",
    "            loss = nan_loss_handling(loss)\n",
    "            self.add_loss(loss)\n",
    "\n",
    "            self.add_metric(tf.math.negative(\n",
    "                loss), name='{}_cos_sim'.format(self.problem_name), aggregation='mean')\n",
    "        return logits\n",
    "\n",
    "# label handling fn\n",
    "def vector_fit_label_handling_fn(target, label_encoder=None, tokenizer=None, decoding_length=None):\n",
    "    # don't need to encoder labels, return array directly\n",
    "    # return label_id and label mask\n",
    "    label_id = np.array(target, dtype='float32')\n",
    "    return label_id, None\n",
    "\n",
    "# make label encoder\n",
    "def vector_fit_get_or_make_label_encoder_fn(params: BaseParams, problem, mode, label_list):\n",
    "    # don't need to make label encoder here\n",
    "    # set params num_classes for this problem\n",
    "    label_array = np.array(label_list)\n",
    "    params.num_classes[problem] = label_array.shape[-1]\n",
    "    return None\n",
    "\n",
    "params.register_problem_type(problem_type='vectorfit', top_layer=VectorFit, label_handling_fn=vector_fit_label_handling_fn, get_or_make_label_encoder_fn=vector_fit_get_or_make_label_encoder_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"BaseParams.from_json\" class=\"doc_header\"><code>BaseParams.from_json</code><a href=\"__main__.py#L241\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>BaseParams.from_json</code>(**`json_path`**:`str`=*`None`*)\n\nLoad json file as params. \n\njson_path could not be None if the problem is not assigned to params\n\nArgs:\n    json_path (str, optional): Path to json file. Defaults to None.\n\nRaises:\n    AttributeError",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseParams.from_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"BaseParams.to_json\" class=\"doc_header\"><code>BaseParams.to_json</code><a href=\"__main__.py#L227\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>BaseParams.to_json</code>()\n\nSave the params as json files. Please note that processing_fn is not saved.\n        ",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseParams.to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"BaseParams.parse_problem_string\" class=\"doc_header\"><code>BaseParams.parse_problem_string</code><a href=\"__main__.py#L314\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>BaseParams.parse_problem_string</code>(**`flag_string`**:`str`)\n\n",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseParams.parse_problem_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse problem string\n",
    "\n",
    "Arguments: flag_string {str} -- problem string\n",
    "\n",
    "Returns: list -- problem list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chained with |:  (['toy_cls', 'toy_seq_tag'], [['toy_seq_tag'], ['toy_cls']])\n",
      "chained with &:  (['toy_cls', 'toy_seq_tag'], [['toy_seq_tag', 'toy_cls']])\n"
     ]
    }
   ],
   "source": [
    "print('chained with |: ', params.parse_problem_string('toy_seq_tag|toy_cls'))\n",
    "print('chained with &: ', params.parse_problem_string('toy_seq_tag&toy_cls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"BaseParams.get_data_info\" class=\"doc_header\"><code>BaseParams.get_data_info</code><a href=\"__main__.py#L275\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>BaseParams.get_data_info</code>(**`problem_list`**:`List`\\[`str`\\], **`base`**:`str`)\n\n",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseParams.get_data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get number of data, number of classes of data and eos_id of data.\n",
    "\n",
    "Arguments:\n",
    "- problem_list {list} -- problem list\n",
    "- base {str} -- path to store data_info.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toy_cls': 10, 'toy_seq_tag': 10} {'toy_cls': 2, 'toy_seq_tag': 5}\n"
     ]
    }
   ],
   "source": [
    "params.get_data_info(params.problem_list, params.ckpt_dir)\n",
    "print(params.data_num_dict, params.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"BaseParams.get_problem_type\" class=\"doc_header\"><code>BaseParams.get_problem_type</code><a href=\"__main__.py#L486\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>BaseParams.get_problem_type</code>(**`problem`**:`str`)\n\n",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseParams.get_problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'seq_tag'"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.get_problem_type('toy_seq_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"BaseParams.update_train_steps\" class=\"doc_header\"><code>BaseParams.update_train_steps</code><a href=\"__main__.py#L489\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>BaseParams.update_train_steps</code>(**`train_steps_per_epoch`**:`int`, **`epoch`**:`int`=*`None`*, **`warmup_ratio`**=*`0.1`*)\n\nIf the batch_size is dynamic, we have to loop through the tf.data.Dataset\nto get the accurate number of training steps. In this case, we need a function to\nupdate the train_steps which will be used to calculate learning rate schedule.\n\nWARNING: updating should be called before the model is compiled! \n\nArgs:\n    train_steps (int): new number of train_steps",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseParams.update_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the batch_size is dynamic, we have to loop through the tf.data.Dataset\n",
    "to get the accurate number of training steps. In this case, we need a function to\n",
    "update the train_steps which will be used to calculate learning rate schedule.\n",
    "\n",
    "WARNING: updating should be called before the model is compiled! \n",
    "\n",
    "Args:\n",
    "- train_steps (int): new number of train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0\n",
      "1500 150\n"
     ]
    }
   ],
   "source": [
    "print(params.train_steps, params.num_warmup_steps)\n",
    "params.update_train_steps(train_steps_per_epoch=100)\n",
    "print(params.train_steps, params.num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"BaseParams.set_data_sampling_strategy\" class=\"doc_header\"><code>BaseParams.set_data_sampling_strategy</code><a href=\"__main__.py#L521\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>BaseParams.set_data_sampling_strategy</code>(**`sampling_strategy`**=*`'data_balanced'`*, **`sampling_strategy_fn`**:`Callable`=*`None`*)\n\n",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseParams.set_data_sampling_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data sampling strategy for multi-task learning.\n",
    "\n",
    "'data_balanced' and 'problem_balanced' is implemented by default.\n",
    "data_balanced: sampling weight equals to number of rows of that problem chunk.\n",
    "problem_balanced: sampling weight equals to 1 for every problem chunk.\n",
    "\n",
    "Args:\n",
    "- sampling_strategy (str, optional): sampling strategy. Defaults to 'data_balanced'.\n",
    "- sampling_strategy_fn (Callable, optional): function to create weight dict. Defaults to None.\n",
    "\n",
    "Raises:\n",
    "- NotImplementedError: sampling_strategy_fn is not implemented yet\n",
    "- ValueError: invalid sampling_strategy provided\n",
    "\n",
    "Returns:\n",
    "- Dict[str, float]: sampling weight for each problem_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'toy_seq_tag': 0.5, 'toy_cls': 0.5}"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.set_data_sampling_strategy(sampling_strategy='problem_balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
