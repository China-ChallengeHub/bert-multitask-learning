{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Models\n",
    "\n",
    "Top models should following these rules:\n",
    "\n",
    "- losses and metrics should be defined within top layer using `self.add_loss` and `self.add_metric`.\n",
    "- init function takes `params` and `problem_name`. `input_embeddings` is optional\n",
    "- empty tensor should be correctly handled, otherwise, multi-task learning may not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import logging\n",
    "from functools import partial\n",
    "from typing import Dict, Tuple, Union\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import transformers\n",
    "from transformers.modeling_tf_utils import TFSharedEmbeddings\n",
    "from tensorflow_addons.layers.crf import CRF\n",
    "from tensorflow_addons.text.crf import crf_log_likelihood\n",
    "\n",
    "\n",
    "from bert_multitask_learning.params import BaseParams\n",
    "\n",
    "from bert_multitask_learning.utils import gather_indexes\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def empty_tensor_handling_loss(labels, logits, loss_fn):\n",
    "    if tf.equal(tf.size(labels), 0):\n",
    "        return 0.0\n",
    "    if tf.equal(tf.size(tf.shape(labels)), 0):\n",
    "        return 0.0\n",
    "    if tf.equal(tf.shape(labels)[0], 0):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return tf.reduce_mean(loss_fn(\n",
    "            labels, logits, from_logits=True))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def nan_loss_handling(loss):\n",
    "    if tf.math.is_nan(loss):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def create_dummy_if_empty(inp_tensor: tf.Tensor) -> tf.Tensor:\n",
    "    shape_tensor = tf.shape(inp_tensor)\n",
    "    if tf.equal(shape_tensor[0], 0):\n",
    "        data_type = inp_tensor.dtype\n",
    "        dummy_shape_first_dim = tf.convert_to_tensor([1], dtype=tf.int32)\n",
    "        dummy_shape = tf.concat(\n",
    "            [dummy_shape_first_dim, shape_tensor[1:]], axis=0)\n",
    "        dummy_tensor = tf.zeros(dummy_shape, dtype=data_type)\n",
    "        return dummy_tensor\n",
    "    else:\n",
    "        return inp_tensor\n",
    "\n",
    "\n",
    "class BaseTop(tf.keras.Model):\n",
    "    def __init__(self, params: BaseParams, problem_name: str) -> None:\n",
    "        super(BaseTop, self).__init__(name=problem_name)\n",
    "        self.params = params\n",
    "        self.problem_name = problem_name\n",
    "\n",
    "    def call(self, inputs: Tuple[Dict], mode: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SequenceLabel(tf.keras.Model):\n",
    "    def __init__(self, params: BaseParams, problem_name: str):\n",
    "        super(SequenceLabel, self).__init__(name=problem_name)\n",
    "        self.params = params\n",
    "        self.problem_name = problem_name\n",
    "        num_classes = self.params.num_classes[self.problem_name]\n",
    "        self.dense = tf.keras.layers.Dense(num_classes, activation=None)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(1-params.dropout_keep_prob)\n",
    "\n",
    "        if self.params.crf:\n",
    "            self.crf = CRF(num_classes)\n",
    "            self.metric_fn = tf.keras.metrics.Accuracy(\n",
    "                name='{}_acc'.format(self.problem_name)\n",
    "            )\n",
    "        else:\n",
    "            self.metric_fn = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "                name='{}_acc'.format(self.problem_name))\n",
    "\n",
    "    def return_crf_result(self, labels: tf.Tensor, logits: tf.Tensor, mode: str, input_mask: tf.Tensor):\n",
    "        input_mask.set_shape([None, None])\n",
    "        logits = create_dummy_if_empty(logits)\n",
    "        input_mask = create_dummy_if_empty(input_mask)\n",
    "        viterbi_decoded, potentials, sequence_length, chain_kernel = self.crf(\n",
    "            logits, input_mask)\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            loss = -crf_log_likelihood(potentials,\n",
    "                                       labels, sequence_length, chain_kernel)[0]\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            loss = nan_loss_handling(loss)\n",
    "            self.add_loss(loss)\n",
    "            acc = self.metric_fn(\n",
    "                labels, viterbi_decoded, sample_weight=input_mask)\n",
    "            self.add_metric(acc)\n",
    "\n",
    "        # make the crf prediction has the same shape as non-crf prediction\n",
    "        return tf.one_hot(viterbi_decoded, name='%s_predict' % self.problem_name, depth=self.params.num_classes[self.problem_name])\n",
    "\n",
    "    def call(self, inputs, mode):\n",
    "        training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        feature, hidden_feature = inputs\n",
    "        hidden_feature = hidden_feature['seq']\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            labels = feature['{}_label_ids'.format(self.problem_name)]\n",
    "            # sometimes the length of labels dose not equal to length of inputs\n",
    "            # that's caused by tf.data.experimental.bucket_by_sequence_length in multi problem scenario\n",
    "            pad_len = tf.shape(input=hidden_feature)[\n",
    "                1] - tf.shape(input=labels)[1]\n",
    "\n",
    "            # top, bottom, left, right\n",
    "            pad_tensor = [[0, 0], [0, pad_len]]\n",
    "            labels = tf.pad(tensor=labels, paddings=pad_tensor)\n",
    "\n",
    "        else:\n",
    "            labels = None\n",
    "        hidden_feature = self.dropout(hidden_feature, training)\n",
    "\n",
    "        if self.params.crf:\n",
    "            return self.return_crf_result(labels, hidden_feature, mode, feature['model_input_mask'])\n",
    "\n",
    "        logits = self.dense(hidden_feature)\n",
    "\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            loss = empty_tensor_handling_loss(\n",
    "                labels, logits,\n",
    "                tf.keras.losses.sparse_categorical_crossentropy)\n",
    "            self.add_loss(loss)\n",
    "            acc = self.metric_fn(\n",
    "                labels, logits, sample_weight=feature['model_input_mask'])\n",
    "            self.add_metric(acc)\n",
    "        return tf.nn.softmax(\n",
    "            logits, name='%s_predict' % self.problem_name)\n",
    "\n",
    "\n",
    "class Classification(tf.keras.layers.Layer):\n",
    "    def __init__(self, params: BaseParams, problem_name: str) -> None:\n",
    "        super(Classification, self).__init__(name=problem_name)\n",
    "        self.params = params\n",
    "        self.problem_name = problem_name\n",
    "        num_classes = self.params.num_classes[self.problem_name]\n",
    "        self.dense = tf.keras.layers.Dense(num_classes, activation=None)\n",
    "        self.metric_fn = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "            name='{}_acc'.format(self.problem_name))\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(1-params.dropout_keep_prob)\n",
    "\n",
    "    def call(self, inputs, mode):\n",
    "        training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        feature, hidden_feature = inputs\n",
    "        hidden_feature = hidden_feature['pooled']\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            labels = feature['{}_label_ids'.format(self.problem_name)]\n",
    "        else:\n",
    "            labels = None\n",
    "        hidden_feature = self.dropout(hidden_feature, training)\n",
    "        logits = self.dense(hidden_feature)\n",
    "\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            labels = tf.squeeze(labels)\n",
    "            # convert labels to one-hot to use label_smoothing\n",
    "            one_hot_labels = tf.one_hot(\n",
    "                labels, depth=self.params.num_classes[self.problem_name])\n",
    "            loss_fn = partial(tf.keras.losses.categorical_crossentropy,\n",
    "                              from_logits=True, label_smoothing=self.params.label_smoothing)\n",
    "\n",
    "            loss = empty_tensor_handling_loss(\n",
    "                one_hot_labels, logits,\n",
    "                loss_fn)\n",
    "            loss = nan_loss_handling(loss)\n",
    "            self.add_loss(loss)\n",
    "            acc = self.metric_fn(labels, logits)\n",
    "            self.add_metric(acc)\n",
    "        return tf.nn.softmax(\n",
    "            logits, name='%s_predict' % self.problem_name)\n",
    "\n",
    "\n",
    "class PreTrain(tf.keras.Model):\n",
    "    def __init__(self, params: BaseParams, problem_name: str, input_embeddings: tf.Tensor):\n",
    "        super(PreTrain, self).__init__(name=problem_name)\n",
    "        self.params = params\n",
    "        self.nsp = transformers.models.bert.modeling_tf_bert.TFBertNSPHead(\n",
    "            self.params.bert_config)\n",
    "\n",
    "        word_embedding_weight = input_embeddings.word_embeddings\n",
    "        self.vocab_size = word_embedding_weight.shape[0]\n",
    "        share_valid = (self.params.bert_config.hidden_size ==\n",
    "                       self.params.bert_config.embedding_size)\n",
    "        if not share_valid and self.params.share_embedding:\n",
    "            logging.warning(\n",
    "                'Share embedding is enabled but hidden_size != embedding_size')\n",
    "        self.share_embedding = self.params.share_embedding & share_valid\n",
    "        if self.share_embedding:\n",
    "            self.share_embedding_layer = TFSharedEmbeddings(\n",
    "                vocab_size=word_embedding_weight.shape[0], hidden_size=word_embedding_weight.shape[1])\n",
    "            self.share_embedding_layer.build([1])\n",
    "            self.share_embedding_layer.weight = word_embedding_weight\n",
    "        else:\n",
    "            self.share_embedding_layer = tf.keras.layers.Dense(self.vocab_size)\n",
    "\n",
    "    def call(self,\n",
    "             inputs: Tuple[Dict[str, Dict[str, tf.Tensor]], Dict[str, Dict[str, tf.Tensor]]],\n",
    "             mode: str) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        features, hidden_features = inputs\n",
    "\n",
    "        # compute logits\n",
    "        nsp_logits = self.nsp(hidden_features['pooled'])\n",
    "\n",
    "        # masking is done inside the model\n",
    "        seq_hidden_feature = hidden_features['seq']\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            positions = features['masked_lm_positions']\n",
    "\n",
    "            # gather_indexes will flatten the seq hidden_states, we need to reshape\n",
    "            # back to 3d tensor\n",
    "            input_tensor = gather_indexes(seq_hidden_feature, positions)\n",
    "            shape_tensor = tf.shape(positions)\n",
    "            shape_list = tf.concat(\n",
    "                [shape_tensor, [seq_hidden_feature.shape.as_list()[-1]]], axis=0)\n",
    "            input_tensor = tf.reshape(input_tensor, shape=shape_list)\n",
    "            # set_shape to determin rank\n",
    "            input_tensor.set_shape(\n",
    "                [None, None, seq_hidden_feature.shape.as_list()[-1]])\n",
    "        else:\n",
    "            input_tensor = seq_hidden_feature\n",
    "        if self.share_embedding:\n",
    "            mlm_logits = self.share_embedding_layer(\n",
    "                input_tensor, mode='linear')\n",
    "        else:\n",
    "            mlm_logits = self.share_embedding_layer(input_tensor)\n",
    "\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            nsp_labels = tf.squeeze(\n",
    "                features['next_sentence_label_ids'])\n",
    "            mlm_labels = features['masked_lm_ids']\n",
    "            mlm_labels.set_shape([None, None])\n",
    "            # compute loss\n",
    "            nsp_loss = empty_tensor_handling_loss(\n",
    "                nsp_labels, nsp_logits,\n",
    "                tf.keras.losses.sparse_categorical_crossentropy)\n",
    "            mlm_loss_layer = transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss()\n",
    "            # mlm_loss = tf.reduce_mean(\n",
    "            #     mlm_loss_layer.compute_loss(mlm_labels, mlm_logits))\n",
    "\n",
    "            # add a useless from_logits argument to match the function signature of keras losses.\n",
    "            def loss_fn_wrapper(labels, logits, from_logits=True):\n",
    "                return mlm_loss_layer.compute_loss(labels, logits)\n",
    "            mlm_loss = empty_tensor_handling_loss(\n",
    "                mlm_labels,\n",
    "                mlm_logits,\n",
    "                loss_fn_wrapper\n",
    "            )\n",
    "            loss = nsp_loss + mlm_loss\n",
    "            self.add_loss(loss)\n",
    "\n",
    "        return (tf.sigmoid(nsp_logits), tf.nn.softmax(mlm_logits))\n",
    "\n",
    "\n",
    "class Seq2Seq(tf.keras.Model):\n",
    "    def __init__(self, params: BaseParams, problem_name: str, input_embeddings: tf.keras.layers.Layer):\n",
    "        super(Seq2Seq, self).__init__(name=problem_name)\n",
    "        # self.params = params\n",
    "        # self.problem_name = problem_name\n",
    "        # # if self.params.init_weight_from_huggingface:\n",
    "        # #     self.decoder = load_transformer_model(\n",
    "        # #         self.params.transformer_decoder_model_name,\n",
    "        # #         self.params.transformer_decoder_model_loading)\n",
    "        # # else:\n",
    "        # #     self.decoder = load_transformer_model(\n",
    "        # #         self.params.bert_decoder_config, self.params.transformer_decoder_model_loading)\n",
    "\n",
    "        # # TODO: better implementation\n",
    "        # logging.warning(\n",
    "        #     'Seq2Seq model is not well supported yet. Bugs are expected.')\n",
    "        # config = self.params.bert_decoder_config\n",
    "        # # some hacky approach to share embeddings from encoder to decoder\n",
    "        # word_embedding_weight = input_embeddings.word_embeddings\n",
    "        # self.vocab_size = word_embedding_weight.shape[0]\n",
    "        # self.share_embedding_layer = TFSharedEmbeddings(\n",
    "        #     vocab_size=word_embedding_weight.shape[0], hidden_size=word_embedding_weight.shape[1])\n",
    "        # self.share_embedding_layer.build([1])\n",
    "        # self.share_embedding_layer.weight = word_embedding_weight\n",
    "        # # self.decoder = TFBartDecoder(\n",
    "        # #     config=config, embed_tokens=self.share_embedding_layer)\n",
    "        # self.decoder = TFBartDecoderForConditionalGeneration(\n",
    "        #     config=config, embedding_layer=self.share_embedding_layer)\n",
    "        # self.decoder.set_bos_id(self.params.bos_id)\n",
    "        # self.decoder.set_eos_id(self.params.eos_id)\n",
    "\n",
    "        # self.metric_fn = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        #     name='{}_acc'.format(self.problem_name))\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _seq2seq_label_shift_right(self, labels: tf.Tensor, eos_id: int) -> tf.Tensor:\n",
    "        batch_eos_ids = tf.fill([tf.shape(labels)[0], 1], eos_id)\n",
    "        batch_eos_ids = tf.cast(batch_eos_ids, dtype=tf.int64)\n",
    "        decoder_lable = labels[:, 1:]\n",
    "        decoder_lable = tf.concat([decoder_lable, batch_eos_ids], axis=1)\n",
    "        return decoder_lable\n",
    "\n",
    "    def call(self,\n",
    "             inputs: Tuple[Dict[str, Dict[str, tf.Tensor]], Dict[str, Dict[str, tf.Tensor]]],\n",
    "             mode: str):\n",
    "        features, hidden_features = inputs\n",
    "        encoder_mask = features['model_input_mask']\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            input_ids = None\n",
    "            decoder_padding_mask = None\n",
    "        else:\n",
    "            input_ids = features['%s_label_ids' % self.problem_name]\n",
    "            decoder_padding_mask = features['{}_mask'.format(\n",
    "                self.problem_name)]\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return self.decoder.generate(eos_token_id=self.params.eos_id, encoder_hidden_states=hidden_features['seq'])\n",
    "        else:\n",
    "            decoder_output = self.decoder(input_ids=input_ids,\n",
    "                                          encoder_hidden_states=hidden_features['seq'],\n",
    "                                          encoder_padding_mask=encoder_mask,\n",
    "                                          decoder_padding_mask=decoder_padding_mask,\n",
    "                                          decode_max_length=self.params.decode_max_seq_len,\n",
    "                                          mode=mode)\n",
    "            loss = decoder_output.loss\n",
    "            logits = decoder_output.logits\n",
    "            self.add_loss(loss)\n",
    "            decoder_label = self._seq2seq_label_shift_right(\n",
    "                features['%s_label_ids' % self.problem_name], eos_id=self.params.eos_id)\n",
    "            acc = self.metric_fn(decoder_label, logits)\n",
    "            self.add_metric(acc)\n",
    "            return logits\n",
    "\n",
    "\n",
    "class MultiLabelClassification(tf.keras.Model):\n",
    "    def __init__(self, params: BaseParams, problem_name: str) -> None:\n",
    "        super(MultiLabelClassification, self).__init__(name=problem_name)\n",
    "        self.params = params\n",
    "        self.problem_name = problem_name\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            self.params.num_classes[problem_name])\n",
    "        self.dropout = tf.keras.layers.Dropout(\n",
    "            1-self.params.dropout_keep_prob\n",
    "        )\n",
    "        # self.metric_fn = tfa.metrics.F1Score(\n",
    "        #     num_classes=self.params.num_classes[problem_name],\n",
    "        #     threshold=self.params.multi_cls_threshold,\n",
    "        #     average='macro',\n",
    "        #     name='{}_f1'.format(problem_name))\n",
    "\n",
    "    def call(self, inputs, mode):\n",
    "        training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        feature, hidden_feature = inputs\n",
    "        hidden_feature = hidden_feature['pooled']\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            labels = feature['{}_label_ids'.format(self.problem_name)]\n",
    "        else:\n",
    "            labels = None\n",
    "        hidden_feature = self.dropout(hidden_feature, training)\n",
    "        logits = self.dense(hidden_feature)\n",
    "\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            labels = tf.squeeze(labels)\n",
    "            labels = tf.cast(labels, tf.float32)\n",
    "            # use weighted loss\n",
    "            label_weights = self.params.multi_cls_positive_weight\n",
    "\n",
    "            def _loss_fn_wrapper(x, y, from_logits=True):\n",
    "                return tf.nn.weighted_cross_entropy_with_logits(x, y, pos_weight=label_weights, name='{}_loss'.format(self.problem_name))\n",
    "            loss = empty_tensor_handling_loss(\n",
    "                labels, logits, _loss_fn_wrapper)\n",
    "            loss = nan_loss_handling(loss)\n",
    "            self.add_loss(loss)\n",
    "            # labels = create_dummy_if_empty(labels)\n",
    "            # logits = create_dummy_if_empty(logits)\n",
    "            # f1 = self.metric_fn(labels, logits)\n",
    "            # self.add_metric(f1)\n",
    "\n",
    "        return tf.nn.sigmoid(\n",
    "            logits, name='%s_predict' % self.problem_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskLM(tf.keras.Model):\n",
    "    \"\"\"Multimodal MLM top layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: BaseParams, problem_name: str, input_embeddings: tf.keras.layers.Layer) -> None:\n",
    "        super(MaskLM, self).__init__(name=problem_name)\n",
    "        self.params = params\n",
    "        self.problem_name = problem_name\n",
    "\n",
    "        word_embedding_weight = input_embeddings.word_embeddings\n",
    "        self.vocab_size = word_embedding_weight.shape[0]\n",
    "        embedding_size = word_embedding_weight.shape[-1]\n",
    "        share_valid = (self.params.bert_config.hidden_size ==\n",
    "                       embedding_size)\n",
    "        if not share_valid and self.params.share_embedding:\n",
    "            logging.warning(\n",
    "                'Share embedding is enabled but hidden_size != embedding_size')\n",
    "        self.share_embedding = self.params.share_embedding & share_valid\n",
    "        if self.share_embedding:\n",
    "            self.share_embedding_layer = TFSharedEmbeddings(\n",
    "                vocab_size=word_embedding_weight.shape[0], hidden_size=word_embedding_weight.shape[1])\n",
    "            self.share_embedding_layer.build([1])\n",
    "            self.share_embedding_layer.weight = word_embedding_weight\n",
    "        else:\n",
    "            self.share_embedding_layer = tf.keras.layers.Dense(self.vocab_size)\n",
    "\n",
    "    def call(self, inputs, mode):\n",
    "        features, hidden_features = inputs\n",
    "\n",
    "        # masking is done inside the model\n",
    "        seq_hidden_feature = hidden_features['seq']\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            positions = features['masked_lm_positions']\n",
    "\n",
    "            # gather_indexes will flatten the seq hidden_states, we need to reshape\n",
    "            # back to 3d tensor\n",
    "            input_tensor = gather_indexes(seq_hidden_feature, positions)\n",
    "            shape_tensor = tf.shape(positions)\n",
    "            shape_list = tf.concat([shape_tensor, [seq_hidden_feature.shape.as_list()[-1]]], axis=0)\n",
    "            input_tensor = tf.reshape(input_tensor, shape=shape_list)\n",
    "            # set_shape to determin rank\n",
    "            input_tensor.set_shape(\n",
    "                [None, None, seq_hidden_feature.shape.as_list()[-1]])\n",
    "        else:\n",
    "            input_tensor = seq_hidden_feature\n",
    "        if self.share_embedding:\n",
    "            mlm_logits = self.share_embedding_layer(\n",
    "                input_tensor, mode='linear')\n",
    "        else:\n",
    "            mlm_logits = self.share_embedding_layer(input_tensor)\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            mlm_labels = features['masked_lm_ids']\n",
    "            mlm_labels.set_shape([None, None])\n",
    "            # compute loss\n",
    "            mlm_loss = empty_tensor_handling_loss(\n",
    "                mlm_labels,\n",
    "                mlm_logits,\n",
    "                tf.keras.losses.sparse_categorical_crossentropy\n",
    "            )\n",
    "            loss = nan_loss_handling(mlm_loss)\n",
    "            self.add_loss(loss)\n",
    "\n",
    "        return tf.nn.softmax(mlm_logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
