{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body Modeling\n",
    "\n",
    "Modeling code for body model, aka BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "from bert_multitask_learning.params import BaseParams\n",
    "\n",
    "from bert_multitask_learning.utils import (get_embedding_table_from_model,\n",
    "                    load_transformer_model, get_shape_list)\n",
    "\n",
    "LOGGER = tf.get_logger()\n",
    "\n",
    "\n",
    "class MultiModalBertModel(tf.keras.Model):\n",
    "    def __init__(self, params: BaseParams, use_one_hot_embeddings=False):\n",
    "        super(MultiModalBertModel, self).__init__()\n",
    "        self.params = params\n",
    "        if self.params.init_weight_from_huggingface:\n",
    "            self.bert_model = load_transformer_model(\n",
    "                self.params.transformer_model_name)\n",
    "        else:\n",
    "            self.bert_model = load_transformer_model(self.params.bert_config)\n",
    "            self.bert_model(tf.convert_to_tensor(\n",
    "                transformers.file_utils.DUMMY_INPUTS))\n",
    "        self.use_one_hot_embeddings = use_one_hot_embeddings\n",
    "\n",
    "        # multimodal input dense\n",
    "        embedding_dim = get_embedding_table_from_model(\n",
    "            self.bert_model).shape[-1]\n",
    "        self.modal_name_list = ['image', 'others']\n",
    "        self.multimodal_dense = {modal_name: tf.keras.layers.Dense(\n",
    "            embedding_dim) for modal_name in self.modal_name_list}\n",
    "\n",
    "        # multimodal modal type embedding\n",
    "        # this might raise no gradients warning if it's unimodal\n",
    "        # variable: [3, 768]\n",
    "        if self.params.enable_modal_type:\n",
    "            self.modal_type_embedding = tf.keras.layers.Embedding(input_dim=len(\n",
    "                self.modal_name_list)+1, output_dim=embedding_dim)\n",
    "\n",
    "        self.enable_modal_type = self.params.enable_modal_type\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        features_dict = inputs\n",
    "        input_ids = features_dict['input_ids']\n",
    "        input_mask = features_dict['input_mask']\n",
    "        token_type_ids = features_dict['segment_ids']\n",
    "        input_shape = get_shape_list(input_ids)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if input_mask is None:\n",
    "            input_mask = tf.ones(\n",
    "                shape=[batch_size, seq_length], dtype=tf.int32)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.zeros(\n",
    "                shape=[batch_size, seq_length], dtype=tf.int32)\n",
    "\n",
    "        config = self.bert_model.config\n",
    "\n",
    "        self.embedding_output = tf.gather(\n",
    "            get_embedding_table_from_model(self.bert_model), input_ids)\n",
    "\n",
    "        # we need to add [SEP] embeddings around modal input\n",
    "        # Since the last input_ids is always [SEP], we can use it directly\n",
    "        sep_embedding = tf.expand_dims(\n",
    "            self.embedding_output[:, -1, :], axis=1)\n",
    "\n",
    "        if self.enable_modal_type:\n",
    "            # for multimodal\n",
    "            modal_type_ids = tf.zeros_like(input_ids)\n",
    "        else:\n",
    "            modal_type_ids = None\n",
    "\n",
    "        for modal_name in self.modal_name_list:\n",
    "            input_name = '{}_input'.format(modal_name)\n",
    "            segment_id_name = '{}_segment_ids'.format(modal_name)\n",
    "            mask_name = '{}_mask'.format(modal_name)\n",
    "            if input_name not in features_dict:\n",
    "                continue\n",
    "\n",
    "            if not self.enable_modal_type:\n",
    "                LOGGER.warning('Seems there\\'s a multimodal inputs but params.enable_modal_type is '\n",
    "                               'not set to be True.')\n",
    "\n",
    "            # convert other modal embeddings to hidden_size\n",
    "            # [batch_size, seq_length, modal_dim] -> [batch_size, seq_length, hidden_size]\n",
    "            modal_input = self.multimodal_dense[modal_name](\n",
    "                features_dict[input_name])\n",
    "\n",
    "            # add sep embedding\n",
    "            modal_input = tf.concat(  # pylint: disable=unexpected-keyword-arg,no-value-for-parameter\n",
    "                [sep_embedding, modal_input, sep_embedding], axis=1)\n",
    "            # add same type id to left and right\n",
    "            modal_segment_ids = tf.concat(  # pylint: disable=unexpected-keyword-arg,no-value-for-parameter\n",
    "                [tf.expand_dims(features_dict[segment_id_name][:, 0], axis=1),\n",
    "                    features_dict[segment_id_name],\n",
    "                    tf.expand_dims(features_dict[segment_id_name][:, 0], axis=1)], axis=1)\n",
    "            # add mask\n",
    "            modal_mask = tf.concat(  # pylint: disable=unexpected-keyword-arg,no-value-for-parameter\n",
    "                [tf.expand_dims(features_dict[mask_name][:, 0], axis=1),\n",
    "                    features_dict[mask_name],\n",
    "                    tf.expand_dims(features_dict[mask_name][:, 0], axis=1)], axis=1)\n",
    "            # add modal type\n",
    "            if self.enable_modal_type:\n",
    "                this_modal_type_ids = tf.ones_like(\n",
    "                    modal_segment_ids) * self.params.modal_type_id[modal_name]\n",
    "\n",
    "            # concat to text correspondingly\n",
    "            self.embedding_output = tf.concat(  # pylint: disable=unexpected-keyword-arg,no-value-for-parameter\n",
    "                [self.embedding_output, modal_input], axis=1)\n",
    "            token_type_ids = tf.concat(  # pylint: disable=unexpected-keyword-arg,no-value-for-parameter\n",
    "                [token_type_ids, modal_segment_ids], axis=1)\n",
    "            input_mask = tf.concat(  # pylint: disable=unexpected-keyword-arg,no-value-for-parameter\n",
    "                [input_mask, modal_mask], axis=1)\n",
    "            if self.enable_modal_type:\n",
    "                modal_type_ids = tf.concat(\n",
    "                    [modal_type_ids, this_modal_type_ids], axis=1)\n",
    "\n",
    "        self.model_input_mask = input_mask\n",
    "        self.model_token_type_ids = token_type_ids\n",
    "        if self.enable_modal_type:\n",
    "            self.model_modal_type_ids = modal_type_ids\n",
    "\n",
    "        word_embedding = self.embedding_output\n",
    "        if self.enable_modal_type:\n",
    "            word_embedding = word_embedding + \\\n",
    "                self.modal_type_embedding(modal_type_ids)\n",
    "\n",
    "        outputs = self.bert_model(\n",
    "            {'input_ids': None,\n",
    "             'inputs_embeds': word_embedding,\n",
    "             'attention_mask': input_mask,\n",
    "             'token_type_ids': token_type_ids,\n",
    "             'position_ids': input_mask},\n",
    "            training=training,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        self.sequence_output = outputs.last_hidden_state\n",
    "        if 'pooler_output' in outputs:\n",
    "            self.pooled_output = outputs.pooler_output\n",
    "        else:\n",
    "            # no pooled output, use mean of token embedding\n",
    "            self.pooled_output = tf.reduce_mean(outputs.last_hidden_state, axis=1)\n",
    "        self.all_encoder_layers = tf.stack(outputs.hidden_states, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def get_pooled_output(self):\n",
    "        return self.pooled_output\n",
    "\n",
    "    def get_sequence_output(self):\n",
    "        \"\"\"Gets final hidden layer of encoder.\n",
    "\n",
    "        Returns:\n",
    "          float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
    "          to the final hidden of the transformer encoder.\n",
    "        \"\"\"\n",
    "        return self.sequence_output\n",
    "\n",
    "    def get_all_encoder_layers(self):\n",
    "        return self.all_encoder_layers\n",
    "\n",
    "    def get_embedding_output(self):\n",
    "        \"\"\"Gets output of the embedding lookup (i.e., input to the transformer).\n",
    "\n",
    "        Returns:\n",
    "          float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
    "          to the output of the embedding layer, after summing the word\n",
    "          embeddings with the positional embeddings and the token type embeddings,\n",
    "          then performing layer normalization. This is the input to the transformer.\n",
    "        \"\"\"\n",
    "        return self.embedding_output\n",
    "\n",
    "    def get_embedding_table(self):\n",
    "        return get_embedding_table_from_model(self.bert_model)\n",
    "\n",
    "    def get_input_mask(self):\n",
    "        return self.model_input_mask\n",
    "\n",
    "    def get_token_type_ids(self):\n",
    "        return self.model_token_type_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "from bert_multitask_learning.test_base import TestBase\n",
    "import bert_multitask_learning\n",
    "import shutil\n",
    "import numpy as np\n",
    "test_base = TestBase()\n",
    "test_base.params.assign_problem(\n",
    "    'weibo_fake_ner&weibo_fake_cls|weibo_fake_multi_cls|weibo_masklm')\n",
    "params = test_base.params\n",
    "train_dataset = bert_multitask_learning.train_eval_input_fn(\n",
    "    params=params, mode=bert_multitask_learning.TRAIN)\n",
    "eval_dataset = bert_multitask_learning.train_eval_input_fn(\n",
    "    params=params, mode=bert_multitask_learning.EVAL\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "one_batch_data = next(train_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MultiModalBertModel` is transformers model with multi-modal input support. One can use it as a normal keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_multitask_learning.infer_shape_and_type_from_dict(one_batch_data))\n",
    "model = MultiModalBertModel(params=params)\n",
    "_ = model(one_batch_data)\n",
    "assert model.get_pooled_output().shape[-1] == 312\n",
    "assert len(model.get_sequence_output().shape) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
